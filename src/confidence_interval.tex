\section{Доверительные интервалы}

Конечно же, точечные оценки, которые мы составляли ранее, почти наверное не совпадут с истинным значением параметра. Но нам ведь этого и не надо: достаточно того, чтобы они различались не очень сильно. Можно подойти к этой проблеме иначе: локализовать параметр в некотором интервале, куда он попадёт с некоторой высокой фиксированной вероятностью.

\begin{definition}
    \textit{Доверительным интервалом уровня доверия $\gamma$} для параметра $\theta$ называется пара статистик $(T_1(\mathbf X), T_2(\mathbf X))$ такая, что для любого $\theta \in \Theta$
    \[
    \pth (T_1(\mathbf X) < \theta < T_2(\mathbf X)) \ge \gamma.
    \]
    Если выполнено равенство $\pth (T_1(\mathbf X) < \theta < T_2(\mathbf X)) = \gamma$, то доверительный интервал называется \textit{точным}.
\end{definition}

Такой подход удобен и тем, что длина интервала будет показывать нашу уверенность в оценке, что совершенно не делает обычная точечная оценка. К тому же мы сами вольны выбирать, с каким уровнем доверия рассматривать доверительный интервал. Обычно на практике берут $\gamma = 0.9$, $0.95$ или $0.99$.

Не всегда получается легко построить такие интервалы, чтобы вероятность попадания в них была априори выше нужного числа. Но можно лишь потребовать, чтобы неравенство выполнялось в пределе, что в случае большой выборки не будет нас сильно ограничивать.

\begin{definition}
    Если
    \[
    \varliminf_{n \to \infty} \pth (T_1^{(n)}(\mathbf X) < \theta < T_2^{(n)}(\mathbf X)) \ge \gamma,
    \]
то $(T_1(\mathbf X), T_2(\mathbf X))$ называется \textit{асимптотическим доверительным интервалом уровня доверия $\gamma$}. Аналогично предыдущему определению, интервал называется $\textit{точным}$, если предел в точности равен $\gamma$.
\end{definition}

Хотя формально нет никаких условий на длину интервала, имеет смысл выбирать $T_1$, $T_2$ такими, чтобы длина интервала была как можно меньше, в частности, $T_2^{(n)}(\mathbf X) - T_1^{(n)}(\mathbf X)$ должно стремиться к 0 для всех $\theta$ (если это возможно).

\begin{example}\label{ci_cheb}
    Реализуем способ построения интервалов, который первый приходит в голову. Нам по сути нужно ограничить снизу вероятность нахождения неизвестного параметра внутри какой-то окрестности, или, что эквивалентно, ограничить сверху вероятность слишком сильного расхождения. У нас уже есть инструмент из курса теории вероятностей, который помогает в такой оценке --- неравенство Чебышёва.

    Рассмотрим выборку $X_1, \ldots, X_n \sim \bernd(p)$, где $p \in (0; 1)$ --- неизвестный параметр. Так как $\me[p]\overline{\mathbf X} = p$, то по неравенству Чебышёва
    \[
    \pth[p]\left(|\overline{\mathbf X} - p| \ge \epsilon\right) \le \frac{\va \overline{\mathbf X}}{\epsilon^2} = \frac{\va[p] X_1}{n\epsilon^2} = \frac{p(1-p)}{n\epsilon^2} \le \frac{1}{4n\epsilon^2}.
    \]

    Если нам нужно найти доверительный интервал уровня доверия $\gamma$, то полученное ограничение на вероятность ошибки должно равняться $1-\gamma$, то есть $\epsilon = \frac{1}{2\sqrt{n(1-\gamma)}}$. Таким образом,
    \[
    \pth[p]\left(\overline{\mathbf X} - \frac{1}{2\sqrt{n(1-\gamma)}} < p < \overline{\mathbf X} + \frac{1}{2\sqrt{n(1-\gamma)}}\right) \ge \gamma.
    \]

    Мы получили интервал длины порядка $1/\sqrt{n}$, и, как будет ясно далее, эта асимптотика неулучшаема.

    Впрочем, для более сложных распределений ситуация не столь благоприятная: выскакивающая $\va X_1$ скорее всего зависит от неизвестного параметра $\theta$, от которого избавиться будет не так просто, в отличие от данного примера. Это мотивирует нас искать более удачные подходы в нахождении ДИ.

\end{example}

\subsection{Методы построения интервалов}

\paragraph{\textit{Метод I. Центральная функция}}

\begin{definition}
    Функция $G(\mathbf{x}, \theta)$ называется $\textit{центральной функцией}$ (или \textit{центральной статистикой}), если
    \begin{enumerate}
        \item распределение $G(\mathbf X, \theta)$ не зависит от $\theta$ для всех $\theta \in \Theta$;
        \item\label{monot} при каждом $\mathbf{x} \in \R^n$ функция $g(\mathbf{x},\theta)$ непрерывна и строго убывает (возрастает) по $\theta$.
    \end{enumerate}
\end{definition}

Обозначим $p$-квантиль распределения $G(\mathbf X, \theta)$ через $x_p$ и возьмём $0 \le p_1 < p_2 \le 1$ такие, что $p_2 - p_1 =\gamma$. Определим $T_1(\mathbf{x})$ и $T_2(\mathbf{x})$ как решения относительно $\theta$ соответственно уравнений $G(\mathbf{x},\theta) = x_{p_1}$ и $G(\mathbf{x},\theta) = x_{p_2}$. Наличие и единственность решения следует из условия \ref{monot} определения выше. Тогда из монотонности по $\theta$ функции $G(\mathbf{x}, \theta)$ получаем, что
\[
\pth\bigl(T_1(\mathbf X) < \theta < T_2(\mathbf X)\bigr) = \pth\bigl(x_{p_1} < G(\mathbf X, \theta) < x_{p_2}\bigr) = p_2 - p_1 = \gamma.
\]
Удобно брать $p_2 = \frac{1+\gamma}{2}$ и $p_1 = \frac{1-\gamma}{2}$ (в таком случае интервал называется \textit{центральным}), особенно когда распределение $G(\mathbf X, \theta)$ симметрично относительно начала координат.

\begin{example}
    Пусть $X_1, \ldots, X_n \sim \Gamma(\alpha, \lambda)$, где $\alpha$ --- известная величина. Построим точный доверительный интервал уровня доверия $\gamma$ для параметра $\lambda$.
    
    Заметим, что если $X_i \sim \Gamma(\alpha, \lambda)$, то $\lambda X_i \sim \Gamma(\alpha, 1)$. Это значит, что $\lambda\sum X_i$ является центральной статистикой с распределением $\Gamma(n\alpha, 1)$. Поэтому 
    \[
    \pth[\lambda]\left(\frac{y_{(1-\gamma)/2}}{\sum X_i} < \lambda < \frac{y_{(1+\gamma)/2}}{\sum X_i}\right) = \gamma,
    \]
    где $y_p$ -- $p$-квантиль распределения $\Gamma(n\alpha, 1)$.
\end{example}

\begin{example}\label{ci_for_ud}
    Пусть теперь $X_1, \ldots, X_n \sim \ud(0, \theta)$. Построим доверительный интервал для параметра $\theta$.
    
    Рассмотрим функцию $G(\mathbf X, \theta) = \frac{X_{(n)}}{\theta}$. Она будет центральной функцией, поскольку
    \[
    \forall t\in[0, 1]\colon \pth(G(\mathbf X, \theta) \le t) = \pth(X_{(n)} \le t\theta) = \frac{(t\theta)^n}{\theta^n} = t^n,
    \]
    значит, её распределение не зависит от $\theta$, а выполнение условия \ref{monot} очевидно. Из этого представления легко найти квантиль распределения: $x_p = \sqrt[n]{p}$. Таким образом,
    \[
    \pth\left(\frac{X_{(n)}}{\sqrt[n]{\frac{1+\gamma}{2}}} < \theta < \frac{X_{(n)}}{\sqrt[n]{\frac{1-\gamma}{2}}}\right) = \pth\left(\sqrt[n]{\frac{1-\gamma}{2}} < \frac{X_{(n)}}{\theta} < \sqrt[n]{\frac{1+\gamma}{2}}\right) = \gamma.
    \]
    Немаловажно будет посмотреть на асимптотику длины полученного интервала. Примем для простоты $\alpha = \frac{1-\gamma}{2}$, $\beta = \frac{1 + \gamma}{2}$. Длина интервала может быть высчитана как 
    \[
    X_{(n)}(\alpha^{-1/n} - \beta^{-1/n}) \approx \theta \left(1 - \frac{\ln{\alpha}}{n} - 1 + \frac{\ln{\beta}}{n}\right) = \frac{\theta}{n} \ln{\frac{\beta}{\alpha}}
    \]
\end{example}

Не всегда очевидно, как найти хоть какую-нибудь центральную функцию, а вдруг её вообще нет? Однако в широком наборе случаев такая функция всё-таки существует, причём часто она имеет относительно приемлемый вид. Для её нахождения нам потребуется следующее вспомогательное

\begin{proposition}\label{f_to_uniform}
    Пусть $\xi$ --- случайная величина с непрерывной функцией распределения $F(x)$. Тогда $F(\xi) \sim \ud(0, 1)$.
\end{proposition}

\begin{proof}
    Для $t \in (0, 1)$ определим $F^{-1}(t)$ как $\sup\{x\colon F(x) = t\}$ (множество, по которому берётся супремум, не пусто в силу непрерывности). Тогда из неравенства $F(\xi) \le t$ следует $\xi \le F^{-1}(t)$. В обратную сторону импликация выполняется почти наверное: вероятность события
    \[
    \inf\{x\colon F(x) = t\} < \xi \le \sup\{x\colon F(x) = t\}
    \]
    равна нулю, так как функция распределения $\xi$ одинакова при левой и правой части сего неравенства. Значит,
    \[
    \pth(F(\xi) \le t) = \pth(\xi \le F^{-1}(t)) = F(F^{-1}(t)) = t,
    \]
    что есть функция распределения $\ud(0, 1)$. 
\end{proof}

Таким образом, если для любого $\theta$ функция распределения элементов выборки $F_{\theta}(x)$ непрерывна, то вне зависимости от значения $\theta$ статистики $F(X_i)$ распределены равномерно. Смастерим из них одну статистику. Складывать равномерные величины --- дело неблагодарное, поэтому предварительно возьмём от них функцию $\phi(t) = -\ln{t}$. Под действием $\phi$ распределение становится экспоненциальным. Действительно, если раньше плотность равнялась $\rho(t) = I(0 < t < 1)$, то теперь она равна
\[
\tilde \rho(x) = |(\phi^{-1}(x))'| \cdot \rho(\phi^{-1}(x)) = e^{-x}I(0 < e^{-x} < 1) = e^{-x}I(x > 0),
\]
то есть $-\ln{F_{\theta}(X_i)} \sim \expd(1) = \Gamma(1, 1)$. Отсюда $G(\mathbf X, \theta) = - \sum \ln{F_{\theta}(X_i)}$ как сумма независимых случайных величин распределена как $\Gamma(n, 1)$. Если к тому же нам известно, что при фиксированном $x$ функция $F_{\theta}(x)$ строго монотонна и непрерывна по $\theta$, то полученная функция будет центральной.

\begin{example}
    Пусть $X_1, \ldots, X_n \sim \paretod(\theta, 1)$, где $\theta > 0$. Построим точный доверительный интервал уровня доверия $\gamma$ для параметра $\theta$. Для распределения Парето функция распределения имеет вид $F_{\theta}(t) = 1 - t^{-\theta}$, $t \ge 1$. Для упрощения последующих выкладок заметим, что $1-F_{\theta}(X_i)$ в силу симметричности распределена также равномерно на $[0, 1]$, поэтому статистика
    \[
    G(\mathbf X, \theta) = - \sum_{i=1}^n \ln{(1-F_{\theta}(X_i))} = \theta \sum_{i=1}^n \ln{X_i}
    \]
    является центральной и распределена как $\Gamma(n, 1)$. Поэтому если принять $u_p$ за $p$-квантиль такого распределения, то
    \[
    \pth\left(\frac{u_{(1-\gamma)/2}}{\sum \ln{X_i}} < \theta < \frac{u_{(1+\gamma)/2}}{\sum \ln{X_i}}\right) = \pth\left(u_{(1-\gamma)/2} < G(\mathbf X, \theta) < u_{(1+\gamma)/2} \right) = \gamma.
    \]
\end{example}

\paragraph{\textit{Метод II. Использование оценки дисперсии}}

Допустим на нас с неба свалилась асимптотически нормальная оценка $\theta^*$, то есть $\sqrt{n}(\theta^*-\theta) \stackrel{d}{\to}\mathcal{N}(0, \sigma^2(\theta))$ при $n\to\infty$. Потребуем, чтобы асимптотическая дисперсия $\sigma^2(\theta)$ была положительна и непрерывна при всех $\theta \in \Theta$. Рассмотрим
\[
\frac{\sqrt{n}(\theta^*_n-\theta)}{\sigma(\theta^*_n)} = \frac{\sqrt{n}(\theta^*_n-\theta)}{\sigma(\theta)} \cdot \frac{\sigma(\theta)}{\sigma(\theta^*_n)}
\]
Первый множитель сходится к стандартно нормально распределённой случайной величине при $n \to \infty$. Разберёмся со вторым множителем. Так как $\theta^*_n$ асимптотически нормальна, то она состоятельна, то есть $\theta^*_n \stackrel{\pth}{\to} \theta$. Тогда из непрерывности асимптотической дисперсии $\sigma(\theta^*_n) \stackrel{\pth}{\to} \sigma(\theta)$, то есть $\frac{\sigma(\theta)}{\sigma(\theta^*_n)} \stackrel{\pth}{\to} 1$. Отсюда из леммы Слуцкого получаем, что всё произведение сходится к чему-то нормальному. Если обозначить за $z_p$ $p$-квантиль для $\mathcal{N}(0, 1)$, то из сходимости по распределению следует
\[
\pth\left(\theta^*_n -\frac{z_{(1+\gamma)/2} \sigma(\theta^*_n)}{\sqrt{n}} < \theta < \theta^*_n + \frac{z_{(1+\gamma)/2} \sigma(\theta^*_n)}{\sqrt{n}}\right) = \pth\left(\left|\sqrt{n}\cdot\frac{\theta^*_n - \theta}{\sigma(\theta^*_n)}\right| < z_{(1+\gamma)/2}\right) \to \gamma.
\]

Обратите внимание, что тут мы используем квантили с одинаковым индексом $x_{(1+\gamma)/2}$, но при этом знаки перед ними в левой и правой части неравенства разные. Правильным будет также записать интервал как
\[
\left(\theta^*_n -\frac{z_{(1+\gamma)/2} \sigma(\theta^*_n)}{\sqrt{n}}; \theta^*_n - \frac{z_{(1-\gamma)/2} \sigma(\theta^*_n)}{\sqrt{n}}\right),
\]
так как в силу симметричности распределения $z_{(1-\gamma)/2} = -z_{(1+\gamma)/2}$. Чаще всего для симметричных распределений мы будем расписывать интервалы через одинаковые квантили, потому что эстетически так красивее.

\begin{example}
    Рассмотрим модель сдвига $X_1, \ldots, X_n \sim \cauchyd(\theta, 1)$. Как мы знаем, медиана $\mu$ для распределения Коши является асимптотически нормальной оценкой параметра $\theta$:
    \[
    \sqrt{n}(\mu - \theta) \stackrel{d}{\to} \mathcal{N}\left(0, \frac{1}{4\rho_{\theta}^2(\theta)}\right) = \mathcal{N}\left(0, \frac{\pi^2}{4}\right).
    \]
    Таким образом, по методу 2 мы получаем точный асимптотический доверительный интервал:
    \[
    \pth\left(\mu - \frac{z_{(1+\gamma)/2}\pi}{2\sqrt{n}} < \theta < \mu + \frac{z_{(1+\gamma)/2}\pi}{2\sqrt{n}}\right) \to \gamma,
    \]
    где $x_p$ -- $p$-квантиль для стандартного нормального распределения.
\end{example}

\begin{example}
    Теперь пусть $X_i \sim \poisd(\lambda)$, где $\lambda$ -- неизвестный параметр, для которого нужно построить ДИ. По ЦПТ имеем
    \[
    \sqrt{n}\cdot\frac{\overline{\mathbf X} - \lambda}{\lambda} \stackrel{d_{\lambda}}{\longrightarrow} \mathcal{N}(0, 1),
    \]
    и после замены дисперсии $\lambda$ на её состоятельную оценку $\widehat{\lambda} = \overline{\mathbf X}$ получаем асимптотический интервал
    \[
    \left(\overline{\mathbf X} - z_{(1+\gamma)/2}\sqrt{\frac{\overline{\mathbf X}}{n}},\; \overline{X} + z_{(1+\gamma)/2}\sqrt{\frac{\overline{\mathbf X}}{n}}\right).
    \]
\end{example}

\paragraph{\textit{Метод III. Стабилизация дисперсии}}

Наконец, рассмотрим немного усовершенствованный способ, который можно применять в некоторых частных случаях, когда асимптотическая дисперсия является функцией от оцениваемого параметра:
\[
\sqrt{n} \cdot (\theta^*_n - \theta) \stackrel{d_{\theta}}{\to} \mathcal{N}(0, \sigma^2(\theta)).
\]

Для начала отметим главный недостаток предыдущего метода --- он дополнительно загрубляет доверительный интервал при подстановке оценки дисперсии, что непредсказуемо меняет вероятность накрытия интервалом истинного значения параметра. Хотелось бы избавиться от зависимости асимптотической дисперсии от параметра, заменив её на что-то константное. Благо у нас есть инструмент, который позволяет видоизменять дисперсию --- это дельта-метод. Идея заключается в том, чтобы перейти от исходной оценки к некоторой хорошей функции от неё, а именно первообразной от обратного стандартного отклонения:
\[
\psi(t) = \int \frac{dt}{\sigma(t)}.
\]

В таком случае, применяя дельта-метод, приходим к следующей сходимости:
\[
\sqrt{n} \cdot (\psi(\theta^*_n) - \psi(\theta)) \stackrel{d_{\theta}}{\to} \mathcal{N}(0, \sigma^2(\theta) \cdot \psi'(\theta)^2) = \mathcal{N}\left(0, \sigma^2(\theta) \cdot \left(\frac{1}{\sigma(\theta)}\right)^2\right) = \mathcal{N}(0, 1).
\]

Отсюда получаем асимптотический ДИ для $\psi(\theta)$, который посредством взятия обратной функции можно превратить в ДИ для $\theta$:
\[
\gamma = \lim_{n \to \infty} \pth\left(\psi(\theta^*_n) - \frac{z_{(1+\gamma)/2}}{\sqrt{n}} < \psi(\theta) < \psi(\theta^*_n) + \frac{z_{(1+\gamma)/2}}{\sqrt{n}}\right)  = 
\]
\[
= \lim_{n \to \infty} \pth\left(\psi^{-1}\left[\psi(\theta^*_n) - \frac{z_{(1+\gamma)/2}}{\sqrt{n}}\right] < \theta < \psi^{-1}\left[\psi(\theta^*_n) + \frac{z_{(1+\gamma)/2}}{\sqrt{n}}\right]\right).
\]

В заключение скажем, что последний переход корректен в силу строгого возрастания функции $\psi$, ведь её производная положительна.

\begin{wrapfigure}{r}{0.55\textwidth}
    \includegraphics[width=0.5\textwidth]{pic/stabilization/stabilization.pdf}
\end{wrapfigure}

\begin{example}
	Рассмотрим выборку $X_1, \ldots, X_n \sim \expd(\theta)$. Из примера \ref{delta_method_for_exp} нам известная асимптотически нормальная оценка
	\[
		\sqrt{n}\left(\frac{1}{\overline{\mathbf X}} - \theta\right) \stackrel{d_{\theta}}{\to} \mathcal{N}(0, \theta^2),
	\]
	с помощью которой легко построить асимптотический ДИ по методу II:
	\[
	\lim_{n \to \infty} \pth\left(\frac{1}{\overline{\mathbf X}} - \frac{z_{(1+\gamma)/2}}{\sqrt{n}\overline{\mathbf X}} < \theta < \frac{1}{\overline{\mathbf X}} + \frac{z_{(1+\gamma)/2}}{\sqrt{n}\overline{\mathbf X}}\right) = \gamma.
	\]
	
	Усовершенствуем её, применив стабилизацию дисперсии. В данном случае 
	\[
	\psi(t) = \int \frac{dt}{\sigma(t)} = \int \frac{dt}{t} = \ln(t).
	\]
	
	Отсюда получаем заветный интервал
	\[
	\lim_{n \to \infty} \pth\left(\exp\left[-\ln{\overline{\mathbf X}} - \frac{z_{(1+\gamma)/2}}{\sqrt{n}}\right] < \theta < \exp\left[-\ln{\overline{\mathbf X}} + \frac{z_{(1+\gamma)/2}}{\sqrt{n}}\right]\right) = \gamma.
	\] 
\end{example}

Отметим важную особенность, свойственную стабилизованным оценками. Несмотря на сходимость к нормальному распределению, для небольших размеров выборки оценки могут слабо походить на что-то нормальное: зачастую их распределение асимметрично, и приближение нормальным законом будет несостоятельным. При стабилизации дисперсии же распределение выравнивается и становится более куполообразным. Для иллюстрации этого эффекта изобразим гистограммы обычной и стабилизованной оценки из предыдущего примера для $\theta = 5$. На первой картинке левый хвост распределения гораздо легче правого, в то время как на второй картинке всё куда приятнее.



\subsection{Интервалы для нормального распределения}\label{norm_intervals}

Ввиду распространённости нормального распределения среди реальных данных особенно полезно уметь строить доверительные интервалы в нормальной модели, когда элементы выборки имеют распределение $\mathcal{N}(a, \sigma^2)$. В принципе, методов выше хватает для построения асимптотических доверительных интервалов, однако в этом разделе мы попробуем построить их точные аналоги, а также познакомимся с некоторыми важными распределениями, которые непосредственно связаны с нормальным и будут встречаться нам ещё не раз.

\paragraph{\textit{Модель сдвига}}

Будем считать, что дисперсия элементов выборки известна и равна $\sigma^2$, и нашей задачей будет построить доверительный интервал для неизвестного параметра $a$, который отвечает за сдвиг распределения. Данная задача легко решается методом центральной функции, который был рассмотрен выше. Действительно, если $X_i \sim \mathcal{N}(a, \sigma^2)$, то 
\[
G(\mathbf X, a) = \sqrt{n} \cdot \frac{\overline{\mathbf X} - a}{\sigma} \sim \mathcal{N}(0, 1),
\]
поэтому в качестве границ интервала можно взять статистики $\overline{\mathbf X} \pm \frac{\sigma z_{(1+\gamma)/2}}{\sqrt{n}}$, где здесь и далее $z_p$ --- $p$-квантиль стандартного нормального распределения:
\[
\pth[a]\left(\overline{\mathbf X} - \frac{\sigma z_{(1+\gamma)/2}}{\sqrt{n}} < a < \overline{\mathbf X} + \frac{\sigma z_{(1+\gamma)/2}}{\sqrt{n}}\right) = \pth[a]\left(-z_{(1+\gamma)/2} < \sqrt{n} \cdot \frac{\overline{\mathbf X} - a}{\sigma} < z_{(1+\gamma)/2}\right) = \gamma.
\]

\paragraph{\textit{Модель масштаба}}

Теперь перейдём к обратному случаю, когда сдвиг $a$ известен заранее, а дисперсию $\sigma^2$ предстоит оценить. Тут также не представляет труда найти центральную функцию, на сей раз это будет
\[
G(\mathbf X, \sigma^2) = \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - a)^2 = \sum_{i=1}^n \left(\frac{X_i - a}{\sigma}\right)^2.
\]
\setlength\intextsep{15pt}
\begin{wrapfigure}[6]{r}{0.35\textwidth}
    \includegraphics[width=0.35\textwidth]{pic/chi2_density/chi2_density.pdf}
\end{wrapfigure}

Заметим, что $(X_i - a) / \sigma \mathcal{N}(0, 1)$, а также слагаемые в сумме выше независимы в совокупности. Значит, распределение $G(\mathbf X, \sigma^2)$ фиксировано и не зависит от $\sigma^2$. Оно имеет специальное название.

\begin{definition}
    Пусть $\xi_1, \ldots, \xi_k$ --- независимые одинаково распределённые случайные величины, и $\xi_i \sim \mathcal{N}(0, 1)$. Тогда распределение $\eta = \xi_1^2 + \ldots + \xi_k^2$ называют \textit{распределением хи-квадрат ($\chi^2_k$) с $k$ степенями свободы}.
\end{definition}

Не будем подробно останавливаться на свойствах этого распределения, но стоит отметить, что распределение $\chi^2_k$ является частным случаем гамма-распределения. И вправду: непосредственно проверяется, что если $\xi \sim \mathcal{N}(0, 1)$, то $\xi^2 \sim \Gamma(1/2, 1/2)$, поэтому по свойству гамма-распределения $\chi^2_k = \Gamma(k/2, 1/2)$. Плотности распределения $\chi^2_k$ для некоторых $k$ даны на рисунке.

Итак, из вышесказанного следует, что $G(\mathbf X, \sigma^2) \sim \chi^2_{n}$. За $\chi^2_{k,p}$ возьмём $p$-квантиль распределения $\chi^2_{k}$. Тогда получаем следующий интервал:
\begin{gather*}
    \pth[\sigma^2]\left(\frac{1}{\chi^2_{n,(1+\gamma)/2}}\sum_{i=1}^n (X_i - a)^2 < \sigma^2 < \frac{1}{\chi^2_{n,(1-\gamma)/2}}\sum_{i=1}^n (X_i - a)^2\right) = \\
    = \pth[\sigma^2]\left(\chi^2_{n,(1-\gamma)/2} < G(\mathbf X, \sigma^2) < \chi^2_{n,(1+\gamma)/2}\right) = \gamma.
\end{gather*}

\begin{remark}
    Подумайте, почему центральная функция из модели сдвига не пригодна для построения доверительного интервала в данном случае.
\end{remark}

\paragraph{\textit{Модель сдвига-масштаба}}
Перейдём к наименее тривиальному примеру, когда неизвестны оба параметра распределения. Здесь нам здорово поможет результат из примера \ref{ind_of_stat_for_norm}, где было показано с помощью теоремы Басу, что $\overline{\mathbf X} \ind s^2$. Но также ключевую роль в этой задаче будет играть следующая
\begin{theorem}[label=distr_of_var]{Фишер}{}
    Пусть $s^2$ --- выборочная дисперсия в нормальной модели сдвига-масштаба $X_1, \ldots, X_n \sim \mathcal{N}(a, \sigma^2)$. Тогда
    \[
    \frac{ns^2}{\sigma^2} \sim \chi^2_{n-1}.
    \]
\end{theorem}

\begin{proof}
    Для дальнейшего удобства нормируем наблюдения, а именно представим $X_i = a + \sigma Y_i$, где $Y_i \sim \mathcal{N}(0, 1)$. Перепишем выборочную дисперсию:
    \begin{gather*}
        \frac{ns^2}{\sigma^2} = \frac{1}{\sigma^2}\sum \left(X_i - \overline{\mathbf X}\right)^2 = \frac{1}{\sigma^2}\sum \left(a+\sigma Y_i - \overline{a + \sigma \mathbf Y}\right)^2 = \sum \left(Y_i - \overline{\mathbf Y}\right)^2 =\\
    =\sum Y_i^2 - \frac{1}{n}\left(\sum Y_i\right)^2 = \sum Y_i^2 - \left(\sum \frac{Y_i}{\sqrt{n}}\right)^2.
    \end{gather*}
    
    \textit{Способ I.} Заметим, что $\eta = \left(\sum \frac{Y_i}{\sqrt{n}}\right)^2 \sim \chi^2_1$, так как $\sum \frac{Y_i}{\sqrt{n}} \sim \mathcal{N}(0, 1)$, и в то же время
    $$\zeta = ns^2/\sigma^2 + \eta = \sum Y_i^2 \sim \chi^2_n.$$
    Мы знаем, что $ns^2/\sigma^2 \ind \eta$, так как левая и правая части есть функции от выборочных дисперсии и среднего соответственно. Из свойства хар. функций: $\phi_{\eta}(t)\cdot \phi_{ns^2/\sigma^2}(t) = \phi_{\zeta}(t)$, поэтому
    $$\phi_{ns^2/\sigma^2}(t) = \frac{\phi_{\zeta}(t)}{\phi_{\eta}(t)}.$$
    Но так как $\chi^2_n = \Gamma\left(\frac{n}{2}, \frac{1}{2}\right)$, то в равенство выше можно подставить хар. функцию для гамма-распределения:
        \[
        \phi_{ns^2/\sigma^2}(t) = \frac{\left(1 - 2it\right)^{-\frac{n}{2}}}{\left(1 - 2it\right)^{-\frac{1}{2}}} = \left(1 - 2it\right)^{-\frac{n-1}{2}},
        \]
        что есть хар. функция для $\Gamma\left(\frac{n-1}{2}, \frac{1}{2}\right) = \chi^2_{n-1}$. Значит, по теореме о единственности $ns^2/\sigma^2$ имеет в точности распределение $\chi^2_{n-1}$.
    
    \textit{Способ II.} Рассмотрим такую ортогональную матрицу $A$ (то есть $AA^T=1$), что первая её строчка равна $(\frac{1}{\sqrt{n}}, \ldots, \frac{1}{\sqrt{n}})$. Очевидно, этот вектор можно дополнить до ортонормированного базиса, а стало быть такая $A$ существует. Тогда $\mathbf Z = A\mathbf Y$ является гауссовым вектором с нулевым матожиданием и матрицей ковариаций $AEA^T=E$, то есть $Z_i$ --- независимые и стандартно нормально распределены. При этом в силу ортогональности длина вектора не меняется, то есть $\sum Z_i^2 = \sum Y_i^2$. Таким образом, выборочная дисперсия выше перепишется как
    \[
    \frac{ns^2}{\sigma^2} = \sum_{i=1}^n Y_i^2 - \left(\sum_{i=1}^n \frac{Y_i}{\sqrt{n}}\right)^2 = \sum_{i=1}^n Z_i^2 - Z_1^2 = \sum_{i=2}^{n} Z_i^2 \sim \chi^2_{n-1}.
    \]
\end{proof}

Отсюда можно легко выписать доверительный интервал для $\sigma^2$:
\[
\pth[a, \sigma^2]\left(\frac{ns^2}{\chi^2_{n-1,(1+\gamma)/2}} < \sigma^2 < \frac{ns^2}{\chi^2_{n-1,(1-\gamma)/2}}\right) = \pth[a, \sigma^2]\left(\chi^2_{n-1,(1-\gamma)/2} < \frac{ns^2}{\sigma^2} < \chi^2_{n-1,(1+\gamma)/2}\right) = \gamma.
\]

Теперь построим интервал для $a$. Вспомним, с чего мы начинали:
\[
\sqrt{n} \cdot \frac{\overline{\mathbf X} - a}{\sigma} \sim \mathcal{N}(0, 1).
\]
Таким образом, если мы поделим эту функцию на корень из $ns^2/\sigma^2$, мы избавимся от неизвестной $\sigma^2$, оставив лишь неизвестный параметр $a$, причём в силу независимости выборочных дисперсий и среднего распределение полученной величины будет фиксированным.

\begin{definition}
Если случайные величины $\xi$ и $\eta$ независимы, причём $\xi \sim \mathcal{N}(0, 1)$, а $\eta \sim \chi^2_{m}$, то говорят, что случайная величина
\[
\zeta = \frac{\xi}{\sqrt{\eta / m}}
\]
имеет \textit{распределение Стьюдента с $m$ степенями свободы}. Обозначается как $\zeta \sim T_m$.
\end{definition}

\begin{wrapfigure}{l}{0.4\textwidth}
    \includegraphics[width=0.4\textwidth]{pic/t_density/t_density.pdf}
\end{wrapfigure}

Деление на $m$ обусловлено следующим свойством: $T_m \stackrel{d}{\rightarrow} \mathcal{N}(0, 1)$ при $m \to \infty$. Действительно, если $\eta_m = \xi^2_1 + \ldots + \xi^2_m \sim \chi^2_{m}$, где $\xi_i \sim \mathcal{N}(0, 1)$, то по ЗБЧ $\eta_m / m \stackrel{\pth[]}{\rightarrow} 1$, откуда несложно вывести требуемое по лемме Слуцкого. Однако важно помнить, что распределение Стьюдента имеет более <<тяжёлые>> хвосты, которые при малых $n$ дают существенные различия в квантилях нормального распределения и Стьюдента. При $m=1$ это распределение и вовсе будет распределением Коши, у которого чрезвычайно тяжёлые хвосты, поэтому $T_m$ можно считать гладким переходом между этими двумя крайностями.

Возвращаясь к нашей задаче, получаем, что
\[
\sqrt{n-1}\cdot \frac{\overline{\mathbf X} - a}{s} \sim T_{n-1},
\]
откуда находим доверительный интервал для $a$:
\begin{gather*}
    \pth[a, \sigma^2]\left(\overline{\mathbf X} - \frac{sT_{n-1, (1+\gamma)/2}}{\sqrt{n-1}} < a < \overline{\mathbf X} + \frac{sT_{n-1, (1+\gamma)/2}}{\sqrt{n-1}}\right) =\\
    = \pth[a, \sigma^2]\left(-T_{n-1, (1+\gamma)/2} < \sqrt{n-1}\cdot \frac{\overline{\mathbf X} - a}{s} < T_{n-1, (1+\gamma)/2}\right) = \gamma.
\end{gather*}

\subsection*{Задачи}

\begin{problem}
	По выборке $X_1, \ldots, X_n \sim \mathcal{N}(\theta, \theta^2)$ постройте точный доверительный интервал уровня доверия $\gamma$ для параметра $\theta \in \R$.
\end{problem}

\begin{problem}
	По выборке $X_1, \ldots, X_n \sim \chi^2_m$ постройте асимпт. доверительный интервал уровня доверия $\gamma$ для параметра $m > 0$.
\end{problem}

\begin{problem}
    По выборке $X_1, \ldots, X_n \sim \bernd(p)$ постройте асимптотический доверительный интервал уровня доверия $\gamma$ и сравните его с интервалом из примера \ref{ci_cheb}.
\end{problem}

\begin{problem}
	В примере \ref{ci_for_ud} найденный интервал, конечно, хороший, но ещё далёк от идеала. С помощью статистики $X_{(n)}$ постройте точный доверительный интервал \textit{наименьшей длины} уровня доверия $\gamma$ для параметра $\theta$.
\end{problem}

\begin{problem}
	С помощью стабилизации дисперсии постройте доверительный интервал для $p$ уровня доверия $\gamma$ по выборке из распределения $\bernd(p)$.
\end{problem}

\begin{problem}
    Найдите точную \textit{доверительную область} уровня доверия $\gamma$ для вектора $(a, \sigma^2)$ в модели сдвига-масштаба для нормального распределения $\mathcal{N}(\mu, \sigma^2)$, то есть такое борелевское множество $B(X_1, \ldots, X_n)$ в $\R^2$, что
    \[
    \forall a \in \R, \sigma^2 \in \R_{+} \colon \pth[a,\sigma^2]((a, \sigma^2) \in B) = \gamma,
    \]
    и найдите асимптотику её площади.
\end{problem}

\begin{problem}
    Рассмотрим одноэлементную выборку из распределения $\mathcal{N}(a, \sigma^2)$ (оба параметра неизвестны). Приведите пример нетривиального доверительного интервала для параметра $\sigma^2$ уровня доверия $\gamma$. 
\end{problem}



