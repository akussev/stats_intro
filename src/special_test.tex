\section{Goodness of fit критерии для сложных гипотез}\label{special_test}

Теперь разберёмся, как проверять принадлежность распределения некоторому семейству. Например, может возникнуть необходимость проверить нормальность распределения, то есть когда основная гипотеза имеет вид $$H_0\colon \pth[] \in\{\mathcal{N}(a, \sigma^2)\colon a\in\R, \sigma^2 > 0\}.$$
Далее нам часто будут встречаться критерии, в которых подразумевается нормальность данных, и обычно такие критерии мощнее тех, что работают в общем случае.

Вообще говоря, эта задача не такая простая: во-первых, крайне желательно, чтобы статистика критерия не зависела от истинных значений параметров при верности гипотезы, так как в таком случае проще считать p-value, а во-вторых, критерий должен иметь высокую мощность на широком классе распределений. Понятное дело, мы не можем идеально отслеживать отклонение от принадлежности рассматриваемому семейству. Чаще всего каждый критерий смотрит на какую-то одну характеристику распределения, отклонение от которой важно проверить в данной ситуации. Приведём пример такого критерия.

Что мы любим в нормальном распределении? Во-первых, оно симметрично относительно своего матожидания, а во-вторых, оно имеет довольно лёгкие хвосты. К счастью, есть характеристики, которые в некоторой степени отражают наличие этих двух свойств. Они основаны на так называемых \textit{центральных моментах}:
\[
\mu_k(\xi) = \me[](\xi - \me[]\xi)^k.
\]
Как функционал от распределения, центральный момент обладает <<выборочным>> аналогом, который получается методом подстановки:
\[
m_k(\mathbf X) = \frac{1}{n} \sum_{i=1}^n \left(X_i - \overline{\mathbf X}\right)^k.
\]
\begin{definition}
    \textit{Коэффициентом асимметрии} называется величина $\mu_3 / \mu_2^{3/2}$. \textit{Коэффициентом эксцесса} называется величина $\mu_4 / \mu_2^{2} - 3$. Как следствие, числа $\alpha_3 = m_3 / m_2^{3/2}$ и $\alpha_4 = m_4 / m_2^2 - 3$ называют \textit{выборочными коэффициентами асимметрии и эксцесса} соответственно.
\end{definition}
Из-за того, что мы используем центрированные моменты, а также делим на степень дисперсии, распределение этих коэффициентов не зависит от параметров сдвига и масштаба, а вычитание тройки в коэффициенте эксцесса позволяет нам сказать, что оба коэффициента для нормального распределения равны нулю. Посредством дельта-метода можно найти асимптотические дисперсии $\alpha_3$ и $\alpha_4$, которые равны 6 и 24 соответственно. Уже сейчас можно на их основе построить критерий Вальда (см. раздел \ref{wald_test}), но можно контролировать и симметричность, и вес хвостов, взяв комбинированный тест со статистикой
\[
JB(\mathbf X) = \left(\frac{\sqrt{n}\alpha_3(X)}{\sqrt{6}}\right)^2 + \left(\frac{\sqrt{n}\alpha_4(X)}{\sqrt{24}}\right)^2 = \frac{n}{6}\left(\alpha_3^2 + \frac{1}{4}\alpha_4^2\right),
\]

\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-5pt}
    \includegraphics[width=0.4\textwidth]{pic/jb_statistic/new_test-9.pdf}
\end{wrapfigure}

который равен сумме квадратов нормированных коэффициентов. Известно, что $JB \stackrel{d}{\rightarrow} \chi^2_2$. Таким образом, построен асимптотический \textit{критерий Харке-Бера}

\[
R = \{\mathbf{x}\colon JB(\mathbf{x}) \ge \chi^2_{2, 1-\alpha}\}.
\]

Имеется функция \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.jarque_bera.html}{\texttt{scipy.stats.jarque\_bera}}, которая вычисляет значение статистики и p-value. Однако приближение этим предельным распределением является достаточно точным лишь при $n > 2000$, что связано с плохой сходимостью распределения $\alpha_4$ к нормальному и зависимостью коэффициентов (см. рис.). Поэтому зачастую используют более мощный \textit{$K^2$-критерий}, который к коэффициентам асимметрии и эксцесса применяет дополнительные нормализующие преобразования, что позволяет достаточно хорошо приближать предельное распределение уже при малых $n$ (более подробно описано в \cite[3.2.2.16]{kobzar} или в \cite{k2_test}). Чтобы не считать статистику критерия самостоятельно, можно воспользоваться реализацией $K^2$-критерия в функции \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html}{\texttt{scipy.stats.normaltest}}.

\begin{example}
    Посмотрим, как $K^2$-критерий справляется с альтернативами, которые по-разному отличаются от нормального распределения.
    \begin{minted}{python}
size = 100
samples = {
    'Laplace': sps.laplace.rvs(size=size), 
    'Uniform': sps.uniform(loc=-1, scale=2).rvs(size=size), 
    'Skewed': sps.norm.rvs(size=size) + sps.expon.rvs(size=size), 
    'With two hills': sps.laplace(loc=1.5).rvs(size=size) * \
        np.random.choice([1, -1], size=size)
}
print("P-value for distribution:")
for name, sample in samples.items():
    print(f"- {name}: {sps.normaltest(sample).pvalue}")
    \end{minted}
    \begin{lstlisting}
P-value for distribution:
- Laplace: 0.01217731862212881
- Uniform: 8.63304887582185e-07
- Skewed: 0.0016579199197891447
- With two hills: 0.19779708143850236
    \end{lstlisting}

Первые два распределения симметричны, но имеют хвосты, отличные от нормального. Третье распределение есть свёртка нормального распределения с экспоненциальным, что в результате даёт <<скошенное>> распределение. В нашем случае все три альтернативы успешно отвергаются на уровне значимости 0.05. Последний пример интереснее: в нём выборка из сдвинутого распределения Лапласа берётся со случайным знаком, из-за чего плотность распределения образует <<два холма>>, что даже отдалённо не похоже на нормальное распределение. Однако сдвиг подобран так, чтобы коэффициент эксцесса был примерно нулевой, что в купе с симметричностью не даёт критерию отвергнуть гипотезу о нормальности. Впрочем, критерий будет маломощен на любом сколь угодно плохом распределении, у которого $\alpha_3, \alpha_4 \approx 0$ (см. задачу \ref{jarque_bera_nonconsist}).
\end{example}

\subsection{QQ-plot и критерий Шапиро-Уилка}\label{qq_plot}

Множество потенциальных кандидатов на роль семейства распределений, из которого пришла выборка, можно значительно сузить, если провести некоторый визуальный анализ данных. Например, если выборка одномерная, то полезно для начала построить гистограмму, чтобы определить носитель распределения и его характерные особенности, или Box-plot, чтобы обнаружить выбросы. Другим неформальным инструментом описательной статистики является так называемый QQ-plot, который позволяет обнаружить явно выраженные отклонения от заданного семейства распределений.

Предположим, что задано некоторое распределение с функцией распределения $F_0(x)$ и плотностью $\rho(x)$, непрерывной на носителе. Зададим семейство распределений, принадлежность к которому мы хотим проверить, посредством параметров сдвига и масштаба:
$$\mathcal{P}_0 = \left\{\pth[]\colon F_{\pth[]}(x) = F(x; a, \sigma) := F_0\left(\frac{x-a}{\sigma}\right), \;a\in\R, \sigma > 0\right\},$$
или, что эквивалентно, $F_0^{-1}\left[F_{\pth[]}(x)\right] = \frac{x-a}{\sigma}$, то есть при верности основной гипотезы $F_0^{-1} \circ F_{\pth[]}$ является какой-то линейной функцией. С помощью метода подстановки можно заменить истинную функцию распределения $F_{\pth[]}$ на её выборочный аналог $\widehat{F}$ и оценить, насколько сильно полученная функция похожа на линейную. Чтобы не считать эту композицию напрямую, достаточно найти её значения в точках разрыва $X_{(i)}$, а именно в точках $(X_{(i)}, F_0^{-1}(i/n))$ для $i \in \{1, \ldots, n\}$, однако обычно берут точки $(X_{(i)}, F_0^{-1}((i - 0.5)/n))$, дабы не было проблем с прообразом $F_0$ при $i=n$. Отсюда и название QQ-plot: он показывает соотношение между теоретическими квантилями, которые откладываются по оси абсцисс, и выборочными квантилями, которые откладываются по оси ординат.

\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=0.9\textwidth]{pic/qq_plot_for_norm/qq_plot_for_norm1.pdf} 
\caption{Выборка из $\mathcal{N}(3, 25)$}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=0.9\textwidth]{pic/qq_plot_for_norm/qq_plot_for_norm2.pdf}
\caption{QQ-plot для $F_0 = \Phi$}
\end{subfigure}
\end{figure}

Несложно показать, что с ростом $n$ и при верности гипотезы эти точки действительно будут описывать нужную прямую. Если $\mathbf X$ --- выборка из распределения $F(x; a, \sigma)$, то $Y_i = (X_i - a) / \sigma$ имеет распределение $F_0(x)$, и тогда по теореме \ref{quantile} о выборочном квантиле
\[
Y_{(i)} - F_0^{-1}\left(\frac{i}{n}\right) \stackrel{\pth[]}{\longrightarrow} 0
\]
при $n \to \infty$ и $i/n \to p \in (0, 1)$, поэтому
\[
X_{(i)} = a + \sigma Y_{(i)} \approx F_0^{-1}\left(\frac{i}{n}\right) \approx a + \sigma F_0^{-1}\left(\frac{i-0.5}{n}\right).
\]

Однако некоторые выводы об истинном распределении можно сделать и в случае альтернативы, то есть QQ-plot даже в случае плохой аппроксимации прямой может подсказать, какую очередную $F_0$ следует взять. Посмотрим на рис. \ref{qq_plots}, как выглядят QQ-plot с $F_0 = \Phi$ (такие графики ещё называют Normal QQ-plot). На каждый график нанесена прямая, которая наиболее точно подгоняется под данные точки (как это делается, см. в главе \ref{regression}). Можно заметить, что для тяжёлых хвостов график начинает закручиваться против часовой стрелки (как, например, для $T_3$ на рис. \ref{qq_plot_t}), а для лёгких --- по часовой (проще всего взять распределение с ограниченным носителем, например, бета с рис. \ref{qq_plot_beta}). Из-за такой особенности распределения с положительным коэффициентом асимметрии образуют график, выпуклый вниз, а с отрицательным --- вверх.

\begin{figure}[H]
    \centering
    \caption{Normal QQ-plot для различных распределений}\label{qq_plots}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=0.45\textwidth]{pic/qq_plot_compare/t1.pdf}
        \includegraphics[width=0.47\textwidth]{pic/qq_plot_compare/t2.pdf}
        \caption{Выборка из $T_3$}
        \label{qq_plot_t}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=0.45\textwidth]{pic/qq_plot_compare/beta1.pdf}
        \includegraphics[width=0.45\textwidth]{pic/qq_plot_compare/beta2.pdf}
        \caption{Выборка из $B(2, 2)$}
        \label{qq_plot_beta}
    \end{subfigure}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=0.45\textwidth]{pic/qq_plot_compare/chi21.pdf}
        \includegraphics[width=0.45\textwidth]{pic/qq_plot_compare/chi22.pdf}
        \caption{Выборка из $\chi^2_5$}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=0.45\textwidth]{pic/qq_plot_compare/expon1.pdf}
        \includegraphics[width=0.45\textwidth]{pic/qq_plot_compare/expon2.pdf}
        \caption{Выборка из $-\expd(1)$}
    \end{subfigure}
\end{figure}

Следует помнить, что QQ-plot не является критерием: он не может доказать, что основная гипотеза верна, равно как и сделать заключение о неверности гипотезы --- всё делается <<на глаз>>. Если на графике наблюдается значимое отклонение от прямой, мы можем не проверять формально гипотезу, однако гарантий, что при проверке она будет отвержена, никаких нет. Впрочем, можно формализовать понятие <<отклонения от прямой>>, введя характеристику
\begin{equation}\label{sw_statistic}
    W = \frac{\left(\sum_{i=1}^n a_i X_{(i)}\right)^2}{\sum_{i=1}^n (X_i - \overline{\mathbf X})^2},
\end{equation}
причём
$$\mathbf{a} = \frac{\mathbf{m}^TV^{-1}}{\sqrt{\mathbf{m}^T V^{-2}\mathbf{m}}},\;\;\;m_i = \me[] Y_{(i)},\;\;\;V_{ij} = \cov(Y_{(i)}, Y_{(j)}),
$$
где $(Y_1, \ldots, Y_n)$ --- выборка из распределения $F_0$.

Оказывается, при верности основной гипотезы $H_0\colon \pth[] \in \mathcal{P}_0$ распределение $W$ зависит разве что от $n$. Критерий, основанный на статистике $W$, называют \textit{критерием Шапиро-Уилка}. Ввиду нетривиальности определения сей статистики $W$ обычно не задумываются ни о её виде, ни о вычислении вектора $\mathbf{a}$, а просто пользуются готовой реализацией, например, \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html}{\texttt{scipy.stats.shapiro}}. На практике такой критерий оказывается наиболее мощным для проверки гипотезы нормальности, и если отклонение от нормальности в данной ситуации существенно, то используют именно этот критерий. Подробный вывод формулы можно встретить в примере \ref{proof_of_sw}.

\subsection{Подстановка неизвестного параметра}

Напоследок рассмотрим ещё один способ проверки принадлежности семейству распределений с параметрами сдвига и масштаба, которая заключается в использовании критериев согласия из предыдущей главы. Идея следующая: вместо какой-то фиксированной функции распределения давайте использовать ту, которая получается подстановкой на место неизвестных параметров их состоятельных оценок, то есть среди всех функций распределения из основной гипотезы возьмём самую правдоподобную. 

Посмотрим на примере модификации критерия Колмогорова. Напомним, что он имеет вид
\[
R = \{\sqrt{n}D_n \ge k_{1-\alpha}\}, \;\;\; D_n = \sup_{x\in\R} |\widehat{F}_n(x) - F(x)|.
\]
Поставим на проверку гипотезу о принадлежности истинного распределения модели сдвига-масштаба
\[
H_0\colon F(x) = F_0(x;a,\sigma) = F_0\left(\frac{x-a}{\sigma}\right).
\]
В качестве функции $F$, которую мы подставим в статистику $D_n$, возьмём $F(x;\widehat{a},\widehat{\sigma})$, где $\widehat{a},\widehat{\sigma}$ --- некоторые состоятельные оценки параметров $a$ и $\sigma$. Например, пусть $F_0 = \Phi$ (то есть проверяется гипотеза о нормальности), и в качестве оценок параметров возьмём выборочные среднее и дисперсию. Тогда полученная модифицированная статистика
\[
D_n' = \sup_{x\in\R} |\widehat{F}_n(x) - F_0(x;\overline{\mathbf X};s(\mathbf X))|,
\]
домноженная на $\sqrt{n}$, будет иметь какое-то предельное распределение. Критерий, основанный на статистике $\sqrt{n}D_n'$, называется \textit{критерием Лиллиефорса}.

\begin{wrapfigure}{l}{0.5\textwidth}
    \includegraphics[width=0.5\textwidth]{pic/ks_test_pdfs/ks_test_pdfs.pdf}
    \caption{Плотности предельных распределений}
    \label{ks_test_pdfs}
\end{wrapfigure}

Подвох заключается в том, что, вообще говоря, это распределение отличается от распределения Колмогорова, которое было предельным в обычном критерии Колмогорова и не зависело от истинного $F$. В общем случае статистики модифицированных критериев согласия, во-первых, могут зависеть от $F_0$ (то есть для каждого конкретного семейства предельное распределение, а стало быть и критические значения, будет своими), и во-вторых, могут зависеть от вида оценок $\widehat{a}, \widehat{\sigma}$.

На рис. \ref{ks_test_pdfs} изображены плотности предельных распределений статистик всяких критериев: Колмогорова, Лиллиефорса и модифицированного критерия Колмогорова с $F_0(x) = 1 - e^{-x}$ (проверяющего экспоненциальность). Как можно видеть, модифицированные критерии, несмотря на увеличение основной гипотезы, оказываются куда более требовательными, и для них критические значения будут меньше, однако при $n < 30$ следует пользоваться более точными значениями (их можно смоделировать самостоятельно или воспользоваться таблицами, например, \href{https://ru.wikipedia.org/wiki/Критерий_Лиллиефорса}{этой}). Модифицированный критерий Колмогорова для нормальной и экспоненциальной моделей реализован в функции \href{https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.lilliefors.html}{\texttt{statsmodels.stats.diagnostic.lilliefors}}.

Помимо критерия Колмогорова можно модифицировать и другие критерии согласия, например, критерий Андерсона-Дарлинга, статистика которого будет иметь вид
\[
n\omega'^2_n = \int_{\R} \frac{\left(\widehat{F}_n(x) - F_0(x;\widehat{a}, \widehat{\sigma})\right)^2}{F_0(x;\widehat{a}, \widehat{\sigma})(1-F_0(x;\widehat{a}, \widehat{\sigma}))} \, dF_0(x;\widehat{a}, \widehat{\sigma}).
\]

Как и ранее, предельное распределение сей статистики отличается от того, что было ранее, и может существенно зависеть от $F_0$. Данный критерий реализован в функции \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html#scipy.stats.anderson}{\texttt{scipy.stats.anderson}} для множества семейств распределений, правда, вместо p-value он возвращает критические значения для различных уровней значимости.

\subsection{Критерий отношения правдоподобий}

Попробуем обобщить подход из леммы Неймана-Пирсона, когда отвержение гипотезы основывается на чрезмерно большом правдоподобии альтернативы по сравнению с правдоподобием нулевой гипотезы. В случае сложных гипотез
$$H_0\colon \theta \in \Theta_0 \vs H_1\colon \theta \in \Theta_1$$
предлагается брать супремум функции правдоподобия $f_{\theta}(\mathbf x)$ на соответствующем множестве параметров: это своего рода лучшее, что может предложить каждая из гипотез:
\[
	\widetilde\lambda(\mathbf x) = \frac{\sup\limits_{\theta \in \Theta_1} f_{\theta}(\mathbf x)}{\sup\limits_{\theta \in \Theta_0} f_{\theta}(\mathbf x)}, \;\;\; \mathbf x \in \mathcal{X}.
\]

Однако зачастую берут статистику 
\begin{equation}\label{lrt_stat}
	\lambda(\mathbf x) = \frac{\sup\limits_{\theta \in \Theta} f_{\theta}(\mathbf x)}{\sup\limits_{\theta \in \Theta_0} f_{\theta}(\mathbf x)}, \;\;\; \mathbf x \in \mathcal{X},
\end{equation}
в которой максимизация в числителе производится по всему множеству параметров $\Theta$. Обычно это более удобно для общей альтернативы $\Theta_1 = \Theta \setminus \Theta_0$, при том что разницы в значениях $\lambda$ и $\widetilde\lambda$ практически нет. Таким образом, большое значение этих статистик свидетельствует о недостаточной правдоподобности нулевой гипотезы по сравнению с альтернативой, отчего разумным представляется взять критерий вида $R_{c} = \{\mathbf x \in \mathcal{X}\colon \lambda(\mathbf x) > c\}$. Если вдруг нам удастся выбрать $c$ так, что $\pth[\theta](R_c) \le \alpha$ для всех $\theta \in \Theta_0$, то мы получим критерий уровня значимости $\alpha$, который называется \textit{критерием отношения правдоподобий} (англ. \textsf{likelihood ratio test}) или сокращённо КОП. Уже на данном этапе он весьма полезен, так как даёт лёгкий способ строить критерии. Однако далеко не всегда удаётся найти распределение статистики критерия и уж тем более убедиться, что оно одинаковое при нулевой гипотезе. Тем удивительнее, что в некоторых моделях асимптотическое поведение статистики одно и то же, причём имеет относительно приемлемый вид.

\begin{theorem}{Уилкс}{}
	Пусть $\Theta \subset \R^d$ --- многообразие размерности $k$, в котором выделено подмногообразие $\Theta_0$ размерности $l$. Тогда в условиях регулярности при всех $\theta \in \Theta_0$ имеется сходимость
	$$
	2\ln{\lambda(\mathbf X)} \stackrel{d_{\theta}}{\to} \chi^2_{k-l}.
	$$
\end{theorem}

Количество степеней свободы у предельного распределения можно интерпретировать как коразмерность подповерхности $\Theta_0$. Таким образом, при условиях теоремы имеет место асимптотический критерий уровня значимости $\alpha$, который можно записать в виде
\[
R_{\alpha} = \{\mathbf x \in \mathcal{X}\colon 2\ln{\lambda(\mathbf x)} \ge \chi^2_{k-l,1-\alpha}\}.
\]

\begin{example}
	Рассмотрим нормальную модель сдвига-масштаба $X_1, \ldots, X_n \sim \mathcal N(\mu, \sigma^2)$, где оба параметра неизвестны. Поставим на проверку гипотезу $H_0\colon \mu = \mu_0$ против общей альтернативы, используя критерий отношения правдоподобий. Для этого нам нужно найти две ОМП: в общем случае и при фиксированном среднем $\mu_0$. Первый случай разбирался ранее в примере \ref{mle_example}, где ОМП имели вид
	\[
	\widehat{\mu} = \overline{\mathbf X}, \;\;\; \widehat{\sigma^2} = s^2(\mathbf X).
	\]
	Во втором случае несложно убедится, что ОМП для дисперсии равна
	\[
	\widetilde{\sigma^2} = \frac{1}{n}\sum_{i=1}^n (X_i - \mu_0)^2.
	\]
	
	Значит, отношение правдоподобий равно
	\[
	\lambda(\mathbf X) = \frac{f_{\widehat{\mu}, \widehat{\sigma^2}}(\mathbf X)}{f_{\mu_0, \widetilde{\sigma^2}}(\mathbf X)} =
	\frac{\left(\dfrac{1}{\sqrt{2\pi \widehat{\sigma^2}}}\right)^{n} \exp{\displaystyle\left[-\dfrac{1}{2\widehat{\sigma^2}} \sum\limits_{i=1}^n (X_i - \widehat\mu)^2\right]}}{\left(\dfrac{1}{\sqrt{2\pi \widetilde{\sigma^2}}}\right)^{n} \exp{\displaystyle\left[-\dfrac{1}{2\widetilde{\sigma^2}} \sum\limits_{i=1}^n (X_i - \mu_0)^2\right]}} = 
	\left(\frac{\widetilde{\sigma^2}}{\widehat{\sigma^2}}\right)^{n/2},
	\]
	откуда статистика критерия равна
	\[
	2\ln\lambda(\mathbf X) = n\ln\frac{\widetilde{\sigma^2}}{\widehat{\sigma^2}} = n\ln{\left(\frac{\sum_{i=1}^n (X_i - \mu_0)^2}{\sum_{i=1}^n (X_i - \overline{\mathbf X})^2}\right)}.
	\]
	
	Сам критерий уровня значимости $\alpha$ будет иметь вид $\{\mathbf x\colon 2\ln\lambda(\mathbf X) \ge \chi^2_{1,1-\alpha}\}$.
\end{example}

В более сложных моделях, где максимизация правдоподобия в явном виде затруднительна, можно использовать численные методы оптимизации, такие как градиентный спуск. Потренироваться можно в задаче \ref{minimizing_cauchy}.

\begin{wrapfigure}{r}{0.45\textwidth}
    \includegraphics[width=0.45\textwidth]{pic/lrt_for_cat_data/lrt_for_cat_data.pdf}
\end{wrapfigure}

Наконец рассмотрим следующий частный случай применения КОП, имеющий множество приложений, которые будут снова и снова встречаться далее. Рассмотрим выборку $\mathbf X = (X_1, \ldots, X_n)$ категориальных признаков, как мы уже делали при рассматрении критерия $\chi^2$ в разделе \ref{pearson_chi2_test}: для простоты скажем, что $X_i \in \{1, \ldots, k\}$, причём $p_i = \mathsf{P}(X_1 = i)$. Параметры модели лежат в $(k-1)$-мерном симплексе
\[
\mathcal{S}_{k-1} = \left\{(x_1, \ldots, x_k)\colon \sum_{i=1}^k x_i = 1,\;\; x_i \ge 0\right\},
\]
а сама модель параметризуется $k-1$ числами $p_1, \ldots, p_{k-1}$ ($p_k$ восстанавливается однозначно). Поставим на проверку гипотезу о том, что истинное значение вектора $\mathbf p = (p_1, \ldots, p_k)$ лежит на некоторой подповерхности в $\mathcal{S}_{k-1}$, которую для простоты можно описать $l < k - 1$ параметрами $(\alpha_1, \ldots, \alpha_l)$, лежащими в открытом $U \subset \R^l$. Формально,
\[
H_0\colon \mathbf{p} \in \pi(U) = \left\{(p_1, \ldots, p_k) \;|\;\exists (\alpha_1, \ldots, \alpha_l) \in U \colon p_i = \pi_i(\alpha_1, \ldots, \alpha_l), \;\; i = 1, \ldots, k\right\},
\]
где $\pi\colon U \to S_{k-1}$ --- инъективная функция, задающая параметризацию посредством $\alpha_1, \ldots, \alpha_l$. Такая гипотеза может символизировать собой выражение одних вероятностей через другие или функциональные ограничения, которым подчиняются вероятности $p_i$. Для её проверки воспользуемся КОП, который требует нахождения ОМП на множестве $\Theta_0 = \pi(U)$ и $\Theta = \mathcal{S}_{k-1}$. Первая оценка, очевидно, зависит от природы функции $\pi$ и в каждом случае вычисляется отдельно, обозначим её за $\widehat{\alpha}(\mathbf X)$, то есть $\sup_{\mathbf p \in \Theta_0} f_{\mathbf p}(\mathbf X) = f_{\pi(\widehat{\alpha})}(\mathbf X)$. Теперь найдём вторую оценку. Рассмотрим реализацию выборки $\mathbf x = (x_1, \ldots, x_n)$, в которой $\nu_i$ раз выпало значение $i$, таким образом, $\nu_1 + \ldots + \nu_k = n$. Тогда правдоподобие в точке $\mathbf x$ равно
\[
f_{\mathbf p}(\mathbf x) = p_1^{\nu_1} \cdot \cdots \cdot p_k^{\nu_k}.
\]

Для получения ОМП максимизируем логарифмическую функцию правдоподобия со следующими условиями:
\[
\left\{
\begin{aligned}
	&\nu_1 \ln{p_1} + \ldots + \nu_k \ln{p_k} \longrightarrow \max_{\mathbf p}\\
	&p_1 + \ldots + p_k = 1.
\end{aligned}\right.
\]

Решая задачу условной оптимизации, получаем ожидаемое решение $p_i = \frac{\nu_i}{n}$. Подставляя к формулу \eqref{lrt_stat}, находим статистику критерия:
\[
T(\mathbf x) = 2 \sum_{i=1}^k \nu_i \ln{\frac{\nu_i}{n \pi(\widehat{\alpha})}}.
\]

Хотелось бы применить теорему Уилкса, чтобы сделать вывод о асимптотическом поведении статистики, однако для этого нужно потребовать некоторые условия регулярности, о которых мы заблаговременно умолчали в формулировке теоремы. Применительно к нашей задаче они будут иметь следующий вид:
\begin{enumerate}
	\item Функции $\pi_i$ дважды непрерывно дифференцируемы (модель достаточно гладкая);
	\item Для всех $\alpha \in U$ и всех $i$ справедлива оценка $\pi_i(\alpha) > c > 0$ (в условиях регулярности часто требуется компактность множества параметров, при этом $p_i$ нельзя быть нулевыми, отсюда и такое требование);
	\item Матрица $\left(\frac{\partial \pi_i}{\partial \alpha_j}\right)$ полного ранга $l$ (тогда $\pi(U)$ будет гладкой поверхностью).
\end{enumerate} 
Стоит отметить, что второе условие формально не выполняется в большинстве случаев, но на практике оно не столь обременительно: можно считать, что $c$ достаточно мало и на деле значения ниже него не реализуются. Поэтому либо про это условие не вспоминают вовсе, либо заявляют в самом начале, а потом ни разу не упоминают (мы пойдём по второму пути).
\begin{theorem}{}{}
	В условиях регулярности выше при любом $\alpha \in U$ статистика $T(\mathbf X)$ стремится по распределению к $\chi^2_{k-1-l}$.
\end{theorem}

\subsection{Параметрический критерий $\chi^2$}\label{complext_chi2_test}

Полученный выше критерий уже вполне рабочий и позволяет решать множество задач, однако в литературе чаще всего встречается его аналог, который является обобщением критерия $\chi^2$ Пирсона. Чтобы понять, как его получить, заметим, что статистика критерия выше записывается через дивергенцию Кульбака-Лейблера:
\[
T(\mathbf x) = 2 \sum_{i=1}^k \nu_i \ln{\frac{\nu_i}{n \pi_i(\widehat{\alpha})}} = 2n \sum_{i=1}^k \frac{\nu_i}{n} \ln{\frac{\nu_i}{n \pi_i(\widehat{\alpha})}} = 2n \cdot D(\pth[] \parallel \widehat{\mathsf{P}}),
\]
где $\pth[] = (\nu_1 / n, \ldots, \nu_k / n)$ --- эмпирическое распределение, а $\widehat{\mathsf{P}} = (\pi_1(\widehat{\alpha}), \ldots, \pi_k(\widehat{\alpha}))$ --- наиболее правдоподобное распределение с точки зрения $H_0$, а величина $\nu_i$, напомним, равна количеству выпадений $i$ в выборке. При решении задачи \ref{distance_equivalence} читатель уже убедился, что KL-дивергенция и $\chi^2$-расстояние, равное
\[
\chi^2(\mathsf{P} \parallel \mathsf{Q}) = \sum_{i=1}^k \frac{(p_i - q_i)^2}{q_i},
\]
асимптотически эквивалентны (с точностью до константы 2), то есть при $\mathsf{P} \to \mathsf{Q}$ отношение $KL$ и $\chi^2$ от этих распределений стремится к 2. Применяя те же рассуждения к нашей статистике, получаем предельное распределение для статистики $\chi^2$ при верности $H_0$:
\[
\chi^2(\mathbf X) = n \cdot \chi^2(\mathsf{P} \parallel \mathsf{Q}) = n \sum_{i=1}^k \frac{(\nu_i / n - \pi_i(\widehat{\alpha}))^2}{\pi_i(\widehat{\alpha})} = \sum_{i=1}^k \frac{(\nu_i - n\pi_i(\widehat{\alpha}))^2}{n\pi_i(\widehat{\alpha})} \stackrel{d}{\to} \chi^2_{k-1-l}.
\]

Полученная статистика крайне похожа на аналогичную статистику хи-квадрат \eqref{chi2_stat}, только тут вместо конкретных значений $p_i^0$ мы подставляем их оценку максимального правдоподобия. Таким образом, обычная статистика $\chi^2$ является частным случаем текущей, когда основная гипотеза говорит о принадлежности подмногообразию размерности 0, то есть точке.

Подытожим всё вышесказанное в одной теореме, непосредственное доказательство которой для любознательных читателей предлагается в книге \cite[\S~30.3]{cramer}.
\begin{theorem}[label=complex_chi2_theorem]{}{}
	Рассмотрим категориальную модель, в которой параметром является вектор $\mathbf p = (p_1, \ldots, p_k)$ с $p_i > 0$ и $\sum p_i = 1$, причём $X_1 \in \{1, \ldots, k\}$, $\pth[\mathbf p](X_1 = i) = p_i$. Множество допустимых $\mathbf p$ обозначим за $\mathcal{S}_{k-1}$, а количество $i$ в выборке за $\nu_i$.
	
	Пусть задано натуральное $l < k-1$, открытое множество $U \subset \R^l$ и функции $\pi_i\colon U \to \mathcal{S}_{k-1}$, $i = 1, \ldots, k$, которые удовлетворяют следующим условиям:
	\begin{itemize}
		\item Найдётся $c > 0$ такое, что для всех $\alpha \in U$ и $i = 1, \ldots, k$ верно $\pi_i(\alpha) > c$;
		\item Функции $\pi_i$ дважды непрерывно дифференцируемы;
		\item Матрица $\left(\frac{\partial \pi_i}{\partial \alpha_j}\right)$, $i=1,\ldots, k$, $j=1,\ldots, l$, имеет ранг $l$.
	\end{itemize}
	Тогда для всех $\alpha \in U$ статистика
	\begin{equation}\label{complex_chi2}
		\chi^2(\mathbf X) = \sum_{i=1}^k \frac{(\nu_i - n\pi_i(\widehat{\alpha}))^2}{n\pi_i(\widehat{\alpha}(\mathbf X))},
	\end{equation}
	где $\widehat{\alpha}(\mathbf X)$ --- ОМП для параметра $\alpha \in U$, стремится к $\chi^2_{k-1-l}$ при $n \to \infty$. Таким образом, критерий
	\[
	R_{\alpha} = \left\{\mathbf x\colon \sum_{i=1}^k \frac{(\nu_i - n\pi_i(\widehat{\alpha}))^2}{n\pi_i(\widehat{\alpha}(\mathbf x))} > \chi^2_{k-1-l, \, 1 - \gamma}\right\}
	\]
	является асимптотическим критерием уровня значимости $\gamma$ для проверки гипотезы $H_0 \colon \mathbf p \in \pi(U)$ против общей альтернативы.
\end{theorem}

Полученный критерий называют \textit{параметрическим или обобщённым критерием $\chi^2$}. Звучит всё довольно сложно, поэтому предлагается сразу рассмотреть следующий хрестоматийный

\begin{example}[пример 1 из \S~57 \cite{borovkov}]
	Как известно, существует 4 группы крови, 0, A, B и AB. Они определяются наличием генов 0, A и B, причём первый из них является рецессивным. Значит, если вероятности получить от одного из родителей ген 0, A и B равны $p$, $q$ и $r = 1 - p - q$ соответственно, то вероятности появления групп крови у ребёнка можно вычислить, как указано в таблице ниже. Проверим гипотезу о том, что такой механизм наследования имеет место. 
	
	\begin{center}
		\begin{tabular}{|m{0.1\textwidth}|m{0.3\textwidth}|m{0.15\textwidth}|}
	\hline
	     Группа крови & Комбинации, приводящие к группе & Вероятность \\
	     \hline
	     0 & 00 & $r^2$ \\
	     \hline
	     A & 0A, AA & $2pr + p^2$ \\
	     \hline
	     B & 0B, BB & $2qr + q^2$ \\
	     \hline
	     AB & AB & $2pq$ \\
	     \hline
	\end{tabular}
	\end{center}
	
	Пусть нам дана выборка с частотами $\nu_1 = 121$, $\nu_2 = 120$, $\nu_3 = 79$ и $\nu_4 = 33$, которые равны количеству людей с группами 0, A, B и AB соответственно. При верности нулевой гипотезы вероятности появления конкретной группы определяется таблицей выше, а значит, функция правдоподобия имеет следующий вид:
	\[
	f_{p, q, r}(\nu) = (r^2)^{\nu_1} \cdot (2pr+p^2)^{\nu_2} \cdot (2qr+p^2)^{\nu_3} \cdot (2pq)^{\nu_4}.
	\]
	
	Для подсчёта ОМП параметров $(p, q, r)$ потребуется помощь численных методов. С помощью простого градиентного спуска мы можем максимизировать логарифмическую функцию правдоподобия и получить оценки $\widehat{p} \approx 0.246$, $\widehat{q} \approx 0.173$, $\widehat{r} \approx 0.58$. Оценки самих вероятностей, $\pi(\alpha)$ в старых обозначениях, равны
	\[
	\widehat{p}_1 \approx 0.337, \;\;\; \widehat{p}_2 \approx 0.347,\;\;\; \widehat{p}_3 \approx 0.231, \;\;\; \widehat{p}_4 \approx 0.085.
	\]
	
	Подставим полученные значения в статистику \eqref{complex_chi2}, что даст нам $\chi^2(\nu) \approx 0.437$.
	
	Разберёмся со степенями свободы предельного распределения статистики. Изначально было $k=4$ групп, при этом гипотеза заключалась в том, что модель параметризуется двумя числами --- $p$ и $q$ ($r$ восстанавливается однозначно), то есть $\Theta_0$ имеет размерность $l = 2$. Стало быть, при верности нулевой гипотезы статистика критерия распределена примерно как $\chi^2_{k-1-l} = \chi^2_1$. Для уровня значимости $\alpha = 0.05$ пороговое значение равно $\chi^2_{1,1-\alpha} \approx 3.841$, а соответствующее p-value равно $\approx 0.5084$, то есть оснований для отвержения гипотезы о виде наследования группы крови нет.
\end{example}

\subsection*{Задачи}

\begin{problem}\label{jarque_bera_nonconsist}
    Подберите \textbf{(а)} дискретное; \textbf{(б)} сингулярное распределение, на котором критерий Харке-Бера будет несостоятельным.
\end{problem}

\begin{problem}\label{minimizing_cauchy}
	Рассмотрим модель сдвига-масштаба с распределением Коши:
	$$\rho_{\theta_1, \theta_2}(x) = \frac{1}{\pi\theta_2\left[1+\left(\frac{x - \theta_1}{\theta_2}\right)^2\right]}.$$
	
	С помощью численных методов (например, функции \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html}{\texttt{scipy.optimize.minimize}}) научитесь считать статистику КОП для проверки гипотезы $H_0\colon \theta_1 = 0$. Зафиксировав параметр масштаба и перебрав разные размеры выборки, сгенерируйте несколько выборок при верности гипотезы $H_0$ и проверьте распределение статистики критерия и pvalue. При разных параметрах сдвига и размерах выборки изучите критерий на предмет мощности.
\end{problem}



