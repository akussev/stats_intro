\setcounter{problem}{0}
\section{Достаточные статистики}

\begin{definition}
    Статистика $S(X)$ называется \textit{достаточной} для семейства распределений $\mathcal P_{\theta}$, если условное распределение $\mathsf P_{\theta}(X \in B|S(X) = t)$ одинаково для всех $\theta \in \Theta$.
\end{definition}

\textit{Интуиция.} Неформально "розыгрыш"\, значения вектора выборки можно разделить на два этапа: сначала выбираем значение для достаточной статистики $S(X)$, а после этого -- само значение $X$ в соответствии с условным распределением $\mathsf P_{\theta}(X \in B|S(X) = t)$. Так как оно не зависит от параметра $\theta$, то вся информация о нём хранится в первом этапе, то есть то, какое именно значение $X$ доставляет равенство $S(X) = t$, нас не интересует.

\begin{theorem*}[критерий факторизации]
Пусть $\mathcal P_{\theta}$ -- доминируемое семейство распределений с обобщённой плотностью $\rho_{\theta}$. Тогда $S(X)$ -- достаточная статистика тогда и только тогда, когда обобщённая плотность допускает представление
\[
\rho_{\theta}(\mathbf{x}) = g_\theta(S(\mathbf{x})) h(\mathbf{x}),
\]
где для всех $\theta$ функции $g_\theta$ и $h$ -- борелевские и неотрицательные (сравните с выводом задачи \ref{reduction}).
\end{theorem*}

Заметим, что распределения из экспоненциального семейства $\mathcal P_{\theta}$ автоматически имеют достаточные статистики $T(X)$, ведь 
\begin{equation}\label{exp}
    \rho_{\theta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(a_0(\theta)+\sum_{i=1}^{k} a_i(\theta) T_i(\mathbf{x})\right),
\end{equation}
и из критерия факторизации получаем требуемое.

\begin{example*}
    Рассмотрим пример достаточной статистики не для экспоненциального семейства. Введём модель с равномерным распределением $U(0, \theta)$, где $\theta$ -- неизвестный параметр. Это семейство распределений имеет плотность $\rho_{\theta}(x) = \frac{1}{\theta} I(0 < x < \theta)$, а значит, совместная плотность имеет вид
    \[
    \rho_{\theta}(X) = \frac{1}{\theta^n}I(0 < X_1, \ldots X_n < \theta) = I(0 < X_{(1)})\cdot\frac{I(X_{(n)} < \theta)}{\theta^n} = h(X) \cdot g(T(X), \theta),
    \]
    где $h(X) = I(0 < X_{(1)})$, $g(t, \theta) = \frac{I(t < \theta)}{\theta^n}$, $T(X) = X_{(n)}$. Таким образом, $X_{(n)}$ является достаточной статистикой по критерию факторизации.
\end{example*}

\begin{definition}
    Статистика $S(X)$ называется \textit{полной} для семейства распределений $\mathcal P_{\theta}$, если для любой борелевской функции $f(x)$ выполнено
    \begin{gather*}
        \forall {\theta \in \Theta}\colon \mathsf E_{\theta}f(S(X)) = 0 \Longrightarrow\\
        \forall {\theta \in \Theta}\colon f(S(X)) = 0\,\, (\mathsf P_{\theta}\text{-п. н.})
    \end{gather*}
\end{definition}

Определение по сути говорит, что статистика $S(X)$ выражает параметр единственным образом, то есть вы не можете двумя разными способами несмещённо оценить функцию от параметра, так как иначе матожидание их разности даёт нуль (пример применения подобного рассуждения есть в задаче \ref{not_dost_pol}). Весьма полезной для нахождения полных достаточных статистик оказывается следующая

\begin{theorem*}
Пусть $\theta \in \Theta \subset \R^k$ и для семейства $\mathcal P_{\theta}$ выполняется (\ref{exp}). Пусть кроме того множество значений $(a_1(\theta), \ldots, a_k(\theta))$ для $\theta \in \Theta$ содержит внутреннюю точку. Тогда $T(X)$ является полной достаточной статистикой.
\end{theorem*}

Ключевой особенностью полных достаточных статистик является тот факт, что они из оценки любой степени паршивости могут сделать конфетку:

\begin{definition}
    Оценка $\widehat{\theta}$ называется \textit{оптимальной}, если она является наилучшей в классе несмещённых оценок в среднеквадратичном подходе.
\end{definition}

\begin{theorem*}[Леман-Шеффе]    
    Если $S(X)$ -- полная достаточная статистика для $\mathcal P_{\theta}$ и $\me \widehat{\theta}(X) = \tau(\theta)$, то $\theta^*(X) = \me (\widehat{\theta}(X) | S(X))$ -- оптимальная оценка для $\tau(\theta)$.
\end{theorem*}

Как следствие, функция от полной достаточной статистики заведомо является оптимальной оценкой своего матожидания, так как в силу её $S$-измеримости её УМО -- она же сама.

\begin{problem}\label{dost_pois}
    Найдите оптимальные оценки для параметров распределений \textbf{(а)} $Bern(p)$; \textbf{(б)} $Pois(\lambda)$.
\end{problem}

\begin{solution}
    В обоих пунктах из представления распределений в виде экспоненциального семейства получаем, что $\overline{X}$ будет являться достаточной статистикой. Полнота следует из достаточного условия полноты. Значит, по теореме Лемана-Шеффе данная оценка будет оптимальной.
\end{solution}

\begin{problem}
    По выборке размера $n \ge 2$ из распределения $Exp(\lambda)$ найдите оптимальные оценки для \textbf{(а)} $\lambda$; \textbf{(б)} $\tau(\lambda) = \lambda^{1/2}$. %Зачем нужно условие $n \ge 2$?
\end{problem}

\begin{solution}
    \textbf{(а)} Так как $\mathsf E_{\theta} \overline{X} = \frac{1}{\lambda}$, то логично предположить, что $1 / \overline{X}$ даст нам что-то подходящее. Проверим эту догадку. Так как $X_i \sim Exp(\lambda) \sim \Gamma(1, \lambda)$, то $\sum X_i \sim \Gamma(n, \lambda)$ (в силу независимости $X_i$). Это в свою очередь означает, что
    \[
    \mathsf E_{\theta} \frac1{\sum X_i} = \int_0^{+\infty} \frac{1}{x} \frac{\lambda^n x^{n-1}}{\Gamma(n)}e^{-\lambda x}\,dx = \frac{\Gamma(n-1)\lambda}{\Gamma(n)} \underbrace{\int_0^{+\infty} \frac{\lambda^{n-1} x^{n-2}}{\Gamma(n-1)}e^{-\lambda x}\,dx}_{\text{интеграл плотности $\Gamma(n-1,\lambda)$}} = \frac{\Gamma(n-1)\lambda}{\Gamma(n)} = \frac{\lambda}{n-1}.
    \]
    Таким образом, из теоремы Лемана-Шеффе получаем, что $\frac{n-1}{\sum X_i}$ является требуемой оптимальной оценкой.
    
    \textbf{(б)} Решение аналогично:
    \begin{gather*}
    \mathsf E_{\theta} \frac1{\sqrt{\sum X_i}} = \int_0^{+\infty} \frac{1}{\sqrt x} \frac{\lambda^n x^{n-1}}{\Gamma(n)}e^{-\lambda x}\,dx = \frac{\Gamma(n-1/2)\sqrt{\lambda}}{\Gamma(n)} \underbrace{\int_0^{+\infty}  \frac{\lambda^{n-1/2} x^{n-3/2}}{\Gamma(n-1/2)}e^{-\lambda x}\,dx}_{\text{интеграл плотности $\Gamma(n-1/2,\lambda)$}} = \frac{\Gamma(n-1/2)\sqrt{\lambda}}{\Gamma(n)}.
    \end{gather*}
    В данном случае оптимальной оценкой уже будет являться нечто более сложное: $\hat{\theta} = \frac{\Gamma(n)}{\Gamma(n-1/2)\sqrt{\sum X_i}}$
    
    Возникает логичный вопрос: а зачем условие $n \ge 2$? Что будет, если рассмотреть случай $n=1$? Есть два объяснения разной степени обоснованности. Первое: наша оценка в пункте \textbf{(а)} тупо перестаёт работать. Такую аргументацию даже могут принять, но куда более удачным является второе объяснение -- оптимальной оценки в данном случае просто нет. Действительно, пусть существует $T(X_1)$ такая, что $\me T(X_1) = \lambda$. В данной модели выполняются условия регулярности (можете проверить), причём так как $T(X_1)$ -- оптимальна, то у неё существует второй момент, поэтому можно применить свойство \ref{regdiff} регулярности:
    \begin{gather*}
    1 = \frac{\partial}{\partial \lambda} \lambda = \frac{\partial}{\partial \lambda} \me T(X_1) = \me\left( T(X_1) \frac{\partial}{\partial \lambda} \ln{\rho_{\lambda}(X_1)}\right) = \me\left(T(X_1) \left(\frac{1}{\lambda} - X_1\right)\right) =\\
    = \frac{1}{\lambda} \me T(X_1) - \me (X_1 \cdot T(X_1)) = 1 - \me (X_1 \cdot T(X_1)).
    \end{gather*}
    Таким образом, $\me (X_1 \cdot T(X_1)) = 0$. Но так как $T(X_1)$ -- оценка для $\lambda$, то её значения неотрицательны. Если интеграл неотрицательной функции равен нулю, то она почти наверное равна нулю, чего быть не может -- противоречие.
\end{solution}

\begin{problem}
    По выборке из распределения $\mathcal N(a, \sigma^2)$ постройте оптимальную оценку для вектора параметров $\theta = (a, \sigma^2)$.
\end{problem}

\begin{solution}
    Распишем более подробно функцию правдоподобия для нормального распределения:
    \begin{gather*}
    \rho_{\theta}(\mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum \frac{(x_i - a)^2}{2\sigma^2}\right) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac1{2\sigma^2}\sum x_i^2 + \frac{a}{\sigma^2}\sum x_i - \frac{na^2}{2\sigma^2}\right),
    \end{gather*}
    что является функцией от $\left(\sum x_i^2, \sum x_i\right)$ и вектора параметров $\theta = (a, \sigma^2)$. Значит, по критерию факторизации статистика $T(X)=\left(\sum X_i^2, \sum X_i\right)$ является достаточной, а следовательно
    \[
    \widehat{a} = \overline{X},\,\,\,\widehat{\sigma^2} = \frac{n}{n-1}s^2 = \frac{n}{n-1}(\overline{X^2} - \overline{X}^2)
    \]
    будут являться оптимальными оценками.
\end{solution}

\begin{problem}
    Докажите, что если $\widehat{\theta}(X)$ и $\theta^*(X)$ -- две оптимальные оценки, то они равны $\pth$-п.н. для любого $\theta$.
\end{problem}

\begin{solution}
    По определению эти оценки несмещённые, а значит, по линейности матожидания оценка $(\widehat{\theta} + \theta^*)/2$ также не смещена. В силу оптимальности $4\me \widehat{\theta}^2 \le \me (\widehat{\theta} + \theta^*)^2$ и \\$4\me \left(\theta^*\right)^2 \le \me (\widehat{\theta} + \theta^*)^2$, что при сложении даёт $2\me \widehat{\theta}^2 + 2\me \left(\theta^*\right)^2 \le 4\me 
    \widehat{\theta}\theta^*$. При выделении полного квадрата получаем $\me (\widehat{\theta} - \theta^*)^2 \le 0$, что, конечно, означает, что для каждого $\theta$ выражение под знаком матожидания почти наверное равно 0.
\end{solution}

\begin{problem}
    Пусть $X_1, \ldots, X_n$ -- н.о.р.с.в. из распределения $Pois(\lambda)$. Найдите $\me[\lambda] (X_1^2|X_1 + \ldots + X_n)$.
\end{problem}

\begin{solution}
    Из задачи \ref{dost_pois} мы знаем, что правая часть УМО -- полная достаточная статистика, а значит, по теореме Лемана-Шеффе УМО будет являться оптимальной оценкой для матожидания $X_1^2$, то есть для $\me[\lambda] X_1^2 = \va[\lambda] X_1 + (\me[\lambda] X_1)^2 = \lambda^2 + \lambda$. Таким образом, нам надо каким-то образом найти такую $\phi$, что $\me[\lambda] \phi\left(\sum X_i\right) = \lambda^2 + \lambda$. Что ж, логично начать с
    \begin{gather*}
        \me[\lambda] \left(\sum X_i\right)^2 = \va[\lambda] \sum X_i + \left(\me[\lambda] \sum X_i\right)^2 = n\va[\lambda] X_1 + (n \me[\lambda] X_1)^2 = n\lambda + n^2\lambda^2.
    \end{gather*}
    Мы почти у цели. Осталось слегка поменять коэффициент у $\lambda$:
    \begin{gather*}
    \me[\lambda] \left[(n-1)\sum X_i + \left(\sum X_i\right)^2 \right] = (n - 1) n\lambda + n\lambda + n^2\lambda^2 = n^2(\lambda + \lambda^2) \Longrightarrow\\
    \me[\lambda] (X_1^2|X_1 + \ldots + X_n) = \frac{n - 1}{n^2} \sum X_i + \frac{1}{n^2} \left(\sum X_i\right)^2.
    \end{gather*}
\end{solution}

\begin{problem}
    (теорема Басу) Пусть $S(X)$ -- полная достаточная статистика, $A(X)$ -- статистика, распределение которой одинаково при всех $\theta \in \Theta$ (\textit{англ.} \texttt{ancillary statistic}). Докажите, что $A(X)$ и $S(X)$ независимы.
\end{problem}

\begin{solution}
    Ничего более умного, чем по определению показать независимость событий из $\sigma$-алгебр, порождённых $S(X)$ и $A(X)$, тут придумать нельзя, поэтому так и поступим.
    Пусть $T \in \sigma(A)$, то есть $\exists {B \in \mathcal B(\R)} A^{-1}(B) = T$. Рассмотрим $I_B \circ A(X) = I_T(X)$. Её распределение также независит от $\theta$, так как определяется распределением $A(X)$. Это значит, что $\me I_T(X)$ является некоторой константой, независящей от $\theta$. То есть $I_T(X)$ является несмещённой оценкой константы $\me I_T(X)$. Возникает вопрос: а какая есть у этой константы оптимальная оценка, то есть с минимальной дисперсией? Так она же сама и является! Её дисперсия попросту равна нулю, куда уж меньше? Следовательно, по теореме Лемана-Шеффе
    \[
    \me (I_T(X) | S(X)) = \me I_T(X).
    \]
    По определению УМО это означает, что для любого $C \in \sigma(S)$:
    \[
    \int_C I_T(X)\,d\mathsf{P}_{\theta} = \int_C \me I_T(X)\,d\mathsf{P}_{\theta}.
    \]
    Но первый интеграл равен $\int I_{T\cap C}(X)\,d\mathsf{P}_{\theta} = \mathsf{P}_{\theta}(T \cap C)$, а второй -- $\me I_T(X) \int I_C(X)\,d\mathsf{P}_{\theta} = \me I_T(X)\cdot \mathsf{P}_{\theta}(C) = \mathsf{P}_{\theta}(T)\cdot\mathsf{P}_{\theta}(C)$, что и требовалось.
\end{solution}

\begin{problem}\label{indep}
    Докажите, что статистики $\overline X$ и $s^2$, построенные по выборке из нормального распределения, независимы.
\end{problem}

\begin{solution}
    Рассмотрим модель сдвига $\mathcal{N}(a, \sigma^2)$, где $a$ -- параметр, а $\sigma^2$ -- \textit{известная} величина. Распишем плотность, как в задаче 3:
    \begin{gather*}
    \rho_{\theta}(\mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum \frac{(x_i - a)^2}{2\sigma^2}\right) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac1{2\sigma^2}\sum x_i^2 + \frac{a}{\sigma^2}\sum x_i - \frac{na^2}{2\sigma^2}\right) = \\
    = \underbrace{\frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac1{2\sigma^2}\sum x_i^2\right)}_{h(\mathbf{x})} \cdot \underbrace{\exp\left(\frac{a}{\sigma^2}\sum x_i - \frac{na^2}{2\sigma^2}\right)}_{g(T(\mathbf{x}), a)},
    \end{gather*}
    где $T(X)=\sum X_i$. Применяем критерий факторизации и понимаем, что $\overline{X}$ -- достаточная статистика. Она же будет полной, так как модель принадлежит экспоненциальному семейству, и функция перед $T(X)$ в экспоненте, а именно $a/\sigma^2$, подходит под достаточное условие полноты. Осталось только показать, что распределение $s^2$ не зависит от $a$, и дело в шляпе -- применима теорема Басу. Но это несложно показать, избавившись от параметра в формуле выборочной дисперсии: так как $a$ -- матожидание $X_i$, а $\sigma^2$ -- её дисперсия, то $X_i$ можно представить в виде $a + \sigma\xi_i$, где $\xi_i$ имеет стандартное нормальное распределение. Но тогда
    \[
    s^2 = \sum \left(X_i - \overline{X}\right)^2 = \sum \left(a + \sigma\xi_i - \overline{a + \sigma\xi}\right)^2 = \sigma^2\sum \left(\xi_i - \overline{\xi}\right)^2.
    \]
    Последнее выражение -- функция от выборки из независимых величин, которые распределены одинаково вне зависимости от $a$, поэтому её распределение также не зависит от $a$.
\end{solution}

\begin{problem}
    Пусть $X_1, X_2, X_3$ -- н.о.р.с.в. из распределения $Exp(\lambda)$. Докажите, что $X_1 + X_2 + X_3 \ind \frac{X_1}{X_1+X_2+X_3}$.
\end{problem}

\begin{solution}
    Идея не шибко отличается от предыдущей задачи: полнота и достаточность $X_1 + X_2 + X_3$ проверяется аналогично. Так как $X_i \sim Exp(\lambda)$, то $X_i = \xi_i / \lambda$, где $\xi_i \sim Exp(1)$. Значит, для любого $c\in\R$
    \[
    \mathsf P_{\lambda}\left(\frac{X_1}{X_1+X_2+X_3} \le c\right) = \mathsf P_{\lambda}\left(\frac{\xi_1}{\xi_1+\xi_2+\xi_3} \le c\right) = \mathsf P_{\lambda}\left(\xi_1 \le c(\xi_1+\xi_2+\xi_3)\right) = F_{(1-c)\xi_1-c\xi_2-c\xi_3}(0),
    \]
    что определяется свёрткой независимых случайных величин $\xi_i$, а стало быть определяется целиком и полностью $c$, и от $\lambda$ не зависит.
\end{solution}

\begin{problem}\label{not_dost_pol}
    Убедитесь, что для семейства распределений $\mathcal N(\theta, \theta^2)$ не существует полной достаточной статистики.
\end{problem}

\begin{solution}
    \textit{Идея.} Мы уже знаем, что $\left(\sum X_i^2, \sum X_i\right)$ является достаточной статистикой, но теперь параметр лишь один, и есть подозрения, что сейчас эта оценка несёт в себе слишком много информации, что наводит на мысли о неполноте. Надо показать, что, во-первых, она не будет полной, а во-вторых, и это самое главное, любая другая достаточная статистика будет априори \textit{богаче} данной, и из этого мы выведем, что она тем паче не будет полной.
    
    Неполнота $\left(\sum X_i^2, \sum X_i\right)$ получается из выражения параметра $\theta$ двумя способами:
    \begin{gather*}
    \begin{aligned}
        \me \left(\sum X_i\right)^2 = \va \sum X_i + \left(\me \sum X_i\right)^2 = (n+n^2)\theta^2\\
        \me \sum X_i^2 = n \me X_i^2 = n(\va X_i + (\me X_i)^2) = 2n\theta^2
    \end{aligned}
    \Longrightarrow \me \left[2\left(\sum X_i\right)^2 - (n+1)\sum X_i^2\right] = 0,
    \end{gather*}
    при этом выражение под знаком матожидания не равно нулю почти наверное. Обратите внимание, что мы не используем здесь признак полноты из начала параграфа, так как это лишь достаточное условие!
    
    Пусть нашлась достаточная статистика $T(X)$, то есть по критерию факторизации $\rho_{\theta}(X)=g(T(X), \theta)\cdot h(X)$. Также мы знаем, что
    \[
    \rho_{\theta}(X) = \frac{1}{(2\pi\theta^2)^{n/2}}\exp\left(-\frac1{2\theta^2}\sum X_i^2 + \frac{1}{\theta}\sum X_i - \frac{n}{2}\right),
    \]
    Очень хочется показать, что $\sum X_i^2$ и $\sum X_i$ на самом деле выражаются через $T(X)$. Постараемся подобрать $\theta$ так, чтобы и избавиться от ненужной $h(X)$, и убрать, например, $\sum X_i$, оставив наедине $T(X)$ и $\sum X_i^2$:
    \begin{gather*}
    \frac{\rho_{1}(X)\rho_{1/3}(X)}{\rho_{1/2}(X)\rho_{1/2}(X)} = \frac{g(T(X), 1)g(T(X), 1/3)}{g(T(X), 1/2)g(T(X), 1/2)} = C\cdot \exp \left((-1/2-9/2+2+2)\sum X_i^2\right) \Longrightarrow\\
    \sum X_i^2 = -\ln\left(\frac{g(T(X), 1)g(T(X), 1/3)}{C g(T(X), 1/2)g(T(X), 1/2)}\right)
    \end{gather*}
    В правой части -- функция от $T(X)$, что мы и хотели. Аналогично можно показать, что $\sum X_i$ -- функция от $T(X)$, то есть $\left(\sum X_i^2, \sum X_i\right) = \phi(T(X))$, где $\phi$ -- некая борелевская функция.
    
    {\footnotesize Это и показывает тот факт, что $T(X)$ богаче $\left(\sum X_i^2, \sum X_i\right)$, ведь если статистика есть борелевская функция от другой статистики, то $\sigma$-алгебра первой есть подмножество второй. Иными словами, $\left(\sum X_i^2, \sum X_i\right)$ является \textit{минимальной достаточной статистикой}. Занятно, что минимальная достаточная $\sigma$-алгебра существует \textit{всегда}. Правда нам всё же удобнее работать со статистиками, а с отысканием минимальных достаточных статистик всё не так просто, и этому можно посвятить отдельный параграф (например, \cite[\S~23]{borovkov}).}
    
    Почему же из этого следует, что $T(X)$ точно не полная? Предположим, что это не так, и $T(X)$ всё-таки полная. Но тогда несложно по определению проверить, что полной окажется $\left(\sum X_i^2, \sum X_i\right)$. Действительно, если
    \begin{gather*}
    \me f\left(\sum X_i^2, \sum X_i\right) = 0 \Longrightarrow \me f(\phi(T(X))) = 0 \Longrightarrow \text{из полноты $T(X)$} \\
    f(\phi(T(X))) = f\left(\sum X_i^2, \sum X_i\right) = 0\,\,(\mathsf P_{\theta}-\text{п. н.}) \Longrightarrow \left(\sum X_i^2, \sum X_i\right)\text{ -- полная},
    \end{gather*}
    откуда и получаем заветное противоречие.
\end{solution}

