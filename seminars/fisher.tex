\setcounter{problem}{0}
\section{Сравнение оценок и эффективные оценки}
\subsection{Сравнение оценок в равномерном подходе}

Итак, мы научились с горем пополам строить оценки с различными свойствами. Как же их сравнивать? Один из способов сравнения оценок -- введение некоторой функции, которая будет показывать, насколько сильно оценка отличается от истинного значения параметра.

\begin{definition}
Борелевская неотрицательная функция двух переменных $g(x, y)$ называется \textit{функцией потерь}.
\end{definition}

\begin{example*}
\begin{enumerate}
    \item $g(x, y) = |x - y|$
    \item $g(x, y) = (x - y)^2$ -- \textit{квадратичная функция потерь}.
\end{enumerate}
\end{example*}

\begin{definition}
    Пусть $\theta^*$ -- оценка параметра $\theta$, $g(x, y)$ -- функция потерь. Функция $R(\theta^*, \theta) = \me g(\theta^*, \theta)$ называется \textit{функцией риска} оценки $\theta^*$.
    Говорят, что оценка $\theta^*$ \textit{лучше} оценки $\hat \theta$ \textit{в равномерном подходе с функцией потерь g}, если для любого $\theta \in \Theta$ $R(\theta^*, \theta) \le R(\hat\theta, \theta)$, причём существует такое $\theta$, что неравенство является строгим. Равномерный подход с квадратичной функцией потерь называют \textit{среднеквадратичным}.
\end{definition}

\begin{problem}
    Пусть $X_1, \ldots, X_n$ выборка из равномерного распределения на отрезке $U[0, \theta]$. Сравните следующие оценки параметра $\theta$ в среднеквадратичном подходе: $2\overline{X}$, $(n + 1)X_{(1)}$, $\frac{n+1}{n}X_{(n)}$.
\end{problem}

\begin{solution}
    Данные оценки являются несмещёнными, а значит, их функция риска будет являться дисперсией.
    
    \textbf{(а)} $R(2\overline{X}, \theta) = \va 2\overline{X} = \frac{4}{n^2}\va \sum X_i = \frac{4}{n}\va X_i = \frac{4}{n}\cdot \frac{\theta^2}{12} = \frac{\theta^2}{3n}$.
    
    \textbf{(б)} Вспомним, что для $t \in [0, \theta]$ выполнено $$\pth(X_{(1)} \le t) = 1 - \pth(X_{(1)} > t) = 1 - \prod \pth(X_i > t) = 1 - (1 - \pth(X_1 \le t))^n = 1 - \left(1 - \frac{t}{\theta}\right)^n,$$
    а стало быть $\rho_{X_{(1)}}(t) = \frac{n}{\theta}\left(1 - \frac{t}{\theta}\right)^{n - 1}I_{[0, \theta]}(t)$. Отсюда можно в лоб посчитать дисперсию
    \begin{gather*}
    R((n+1)X_{(1)}, \theta) = \va (n+1)X_{(1)} = \me (n+1)^2X_{(1)}^2 - (\me (n+1)X_{(1)})^2 =\\
    = -\theta^2 + (n+1)^2\int_0^{\theta} t^2\frac{n}{\theta}\left(1 - \frac{t}{\theta}\right)^{n - 1}\,dt = -\theta^2 + n(n+1)^2 \theta^2 \int_0^1 s^2(1-s)^{n-1} ds =\\
    = -\theta^2 + n(n+1)^2 \theta^2 B(n, 3) = -\theta^2 + n(n+1)^2 \theta^2 \frac{2!(n-1)!}{(n+2)!} = \frac{\theta^2n}{n+2}.
    \end{gather*}
    Как видим, дисперсия не стремится к нулю при увеличении размера выборки, поэтому данная оценка весьма плохая в среднеквадратичном подходе.
    
    \textbf{(в)} Тут распределение ищется по-проще: для $t\in[0, \theta]$
    \[
    \pth(X_{(n)} \le t) = \prod \pth(X_{i} \le t) = \frac{t^n}{\theta^n},
    \]
    поэтому $\rho_{X_{(n)}}(t) = \frac{nt^{n-1}}{\theta^n} I_{[0, \theta]}(t)$. Значит,
    \begin{gather*}
    R\left(\frac{n+1}{n}X_{(n)}, \theta\right) = \va \frac{n+1}{n}X_{(n)} = \me \left(\frac{n+1}{n}X_{(n)}\right)^2 - \left(\me \frac{n+1}{n}X_{(1)}\right)^2 =\\
    = -\theta^2 + \frac{(n+1)^2}{n^2}\int_0^{\theta} t^2\frac{nt^{n-1}}{\theta^n}\,dt = -\theta^2 + \left.\frac{(n+1)^2}{n} \cdot \frac{t^{n+2}}{\theta^n(n+2)}\right|_0^{\theta} =\\
    = -\theta^2 + \frac{(n+1)^2\theta^2}{n(n+2)} = \frac{\theta^2}{n(n+2)}.
    \end{gather*}
    Полученная дисперсия убывает даже быстрее, чем, казалось бы, самая логичная и простая оценка $2\overline{X}$, что наводит на определённые мысли.
\end{solution}
\subsection{Информация Фишера и эффективные оценки}
\epigraph{"--* Просто верить? И всё?\\
    "--* Просто верить. Этого вполне достаточно.}{Дети против волшебников
    }

Далее мы по умолчанию предполагаем в вероятностно-статистической модели выполнены \textit{условия регулярности}:
\begin{enumerate}[label=\alph*)]
\item носитель распределения (т.е. $\{x \colon \rho_{\theta}(x) > 0\}$) не зависит от $\theta$;\\
\item $\Theta$ -- открытое связное множество в $\R^k$;\\
\item\label{regdiff} для любого $\theta \in \Theta$ и для любой статистики $S(X)$ с условием $\me S(X)^2 < \infty$ выполнено
\[
\frac{\partial}{\partial\theta} \me S(X) = \me \left(S(X)\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(X)}\right);
\]
\item $0 < I_X(\theta) < \infty$ для любого $\theta \in \Theta$.
\end{enumerate}

Третье условие может ввести в ужас, хотя на самом деле мы всего лишь хотим дифференцировать по параметру $\theta$:
\begin{gather*}
\frac{\partial}{\partial\theta} \me S(X) = \frac{\partial}{\partial\theta} \int S(\mathbf x) \rho_{\theta}(\mathbf x)\,d\mathbf{x} = \int \frac{\partial}{\partial\theta}\left(S(\mathbf x) \rho_{\theta}(\mathbf x)\right)\,d\mathbf{x} = \int S(\mathbf x) \rho'_{\theta}(\mathbf x)\,d\mathbf{x} = \\
= \int S(\mathbf x) \frac{\rho'_{\theta}(\mathbf x)}{\rho_{\theta}(\mathbf x)} \rho_{\theta}(\mathbf x)\,d\mathbf{x} = \int \left(S(\mathbf x) \frac{\partial}{\partial \theta}\ln{\rho_{\theta}(\mathbf x)}\right) \rho_{\theta}(\mathbf x)\,d\mathbf{x} = \me \left(S(X)\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(X)}\right),
\end{gather*}
а проверять, можно ли так делать, мы умеем с матана 4 семестра. Для разнообразия можете проверить, что некоторые стандартные семейства удовлетворяют условиям регулярности, но мы будем в них искренне и бесповоротно верить.

Одной из важнейших характеристик выборок является следующая величина:

\begin{definition}
    \textit{Информацией Фишера} выборки $X$ называется величина $$I_X(\theta) = \va \left(\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(X)}\right).$$ В случае многомерного параметра $I_X(\theta)$ определяется как матрица ковариаций градиента $\ln{p_{\theta}(X)}$ и называется \textit{информационной матрицей}.
\end{definition}

\begin{problem}\label{zerome}
    \textbf{(а)} Убедитесь, что $I_X(\theta) = \me \left(\frac{\partial}{\partial \theta} \ln{\rho_{\theta}(X)}\right)^2$\\
    \textbf{(б)} Докажите аддитивность информации Фишера, а именно
    \[
    I_{(X,Y)}(\theta) = I_X(\theta) + I_Y(\theta)
    \]
    для любых двух независимых выборок $X$ и $Y$. Как следствие, $I_X(\theta) = ni(\theta)$, где $i(\theta)$ -- информация, содержащаяся в одном элементе выборки.
\end{problem}

\begin{solution}
    \textbf{(а)} Достаточно показать, что $\me \frac{\partial}{\partial \theta} \ln{\rho_{\theta}(X)} = 0$. Это можно доказать, воспользовавшись пунктом \ref{regdiff} из условий регулярности и взяв статистику $S(X)\equiv 1$:
    \[
    0 = \frac{\partial}{\partial \theta} (1) = \frac{\partial}{\partial \theta} \me 1 = \me \left(1\cdot\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(X)}\right).
    \]
    \textbf{(б)} Воспользуемся линейностью дисперсии для независимых случайных величин:
    \begin{gather*}
    I_{(X,Y)}(\theta) = \va \left(\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(X, Y)}\right) = \va \left(\frac{\partial}{\partial\theta}\ln{(\rho_{\theta}(X)\cdot \rho_{\theta}(Y))}\right) = \va \left(\frac{\partial}{\partial\theta}(\ln{\rho_{\theta}(X)} +\ln{\rho_{\theta}(Y)})\right) = \\
    = \va \left(\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(X)} +\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(Y)}\right) = \va \left(\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(X)}\right) +\va \left(\frac{\partial}{\partial\theta}\ln{\rho_{\theta}(Y)}\right) = I_X(\theta) + I_Y(\theta).
    \end{gather*}
\end{solution}

Следующая оценка показывает, что есть предел мечтаний о дисперсии нашей оценки:

\begin{theorem*}[неравенство Рао-Крамера]
    Для любой несмещённой оценки $\theta^*$ для $\tau(\theta)$ c $\me {\theta^*}^2 < \infty$ и для любого $\theta \in \Theta$ справедливо неравенство
    \[
    \va (\theta^*(X)) \ge \frac{(\tau'(\theta))^2}{I_X(\theta)}.
    \]
    В многомерном случае неравенство Рао-Крамера принимает вид
    \[
    \mathsf{cov}(\theta^*(X), \theta^*(X)) \ge \frac{\partial \tau}{\partial \theta} I^{-1}_X(\theta) \left(\frac{\partial \tau}{\partial \theta}\right)^T.
    \]
    Неравенство $A \ge B$ двух симметричных матриц $A$ и $B$ понимают как неотрицательную определённость матрицы $A - B$.
\end{theorem*}

Возникает вопрос: а как понять, что наша оценка имеет наименьшую дисперсию, которую позволяет нам это неравенство? На него есть весьма простой ответ:

\begin{definition}
    Оценка $\theta^*$ называется \textit{эффективной}, если для неё выполнено равенство в неравенстве Рао-Крамера.
\end{definition}

\begin{theorem*}[критерий эффективности]
    Оценка $\theta^*$ эффективна тогда и только тогда, когда для любого $\theta \in \Theta$
    \[
    \theta^*(X) - \tau(\theta) = \tau'(\theta) I^{-1}_X(\theta) (\ln{\rho_{\theta}(X)})'_{\theta}.
    \]
\end{theorem*}

Но не спешите радоваться: отнюдь не любое семейство распределений позволяет иметь эффективную оценку. Впрочем, есть критерий, дающий понять, для какого класса семейств она имеется, и он весьма широк:

\begin{definition}
    Семейство $\{\mathsf P_{\theta}\}$, где $\theta = (\theta_1, \ldots, \theta_k)$, принадлежит \textit{экспоненциальному классу распределений}, если обобщённая плотность распределения $\pth$ имеет вид
    \[
    \rho_{\theta}(x) = g(x) \exp\left(a_0(\theta)+\sum_{i=1}^{k} a_i(\theta) u_i(x)\right).
    \]
\end{definition}

\begin{theorem*}
    Пусть $\theta \subset \R$. Тогда эффективная оценка существует только для экспоненциальных семейств. Более того, в этом случае $\theta^*(X) = \overline{u(X)}$ является эффективной оценкой для $-a_0'(\theta) / a_1'(\theta)$, а любая другая эффективная оценка будет линейной функцией от $\theta^*(X)$.
\end{theorem*}

\begin{problem}
    Убедитесь, что \textbf{(а)} $\mathcal N(a, \sigma^2)$, \textbf{(б)} $Exp(\lambda)$, \textbf{(в)} $\Gamma(\alpha, \lambda)$, \textbf{(г)} $Beta(\alpha, \beta)$, \textbf{(д)} $Bern(p)$, \textbf{(е)} $Pois(\lambda)$ принадлежат экспоненциальному классу распределений. Найдите соответствующие значения $u_i(x)$.
\end{problem}

\begin{solution}
    \textbf{(а)} $\rho_{(a, \sigma^2)}(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x - a)^2}{2\sigma^2}\right) = \exp\left(\left(\frac{1}{2}\ln{\frac{1}{2\pi\sigma^2}} - \frac{a^2}{2\sigma^2}\right) - \frac{1}{2\sigma^2}x^2 + \frac{a}{\sigma^2}x\right)$. $u_1(x)=x$, $u_2(x)=x^2$.
    
    \textbf{(б)} $\rho_{\lambda}(x) = \lambda e^{-\lambda x} I(x > 0) = I(x>0)\cdot \exp(\ln{\lambda} - \lambda x)$. $u_1(x)=x$.
    
    \textbf{(в)} $\rho_{(\alpha, \lambda)}(x) = \frac{\lambda^{\alpha}x^{\alpha-1}}{\Gamma(\alpha)}e^{-\lambda x}I(x > 0) = I(x > 0) \cdot \exp\left((\alpha\ln\lambda - \ln{\Gamma(\alpha)) - \lambda x + (\alpha - 1)\ln{x}}\right)$. $u_1(x)=x$, $u_2(x)=\ln{x}$.
    
    \textbf{(г)} $\rho_{(\alpha, \beta)}(x) = \frac{x^{\alpha - 1}(1-x)^{\beta - 1}}{B(\alpha, \beta)}I(0 < x < 1) = I(0 < x < 1)\cdot  \exp(-\ln{B(\alpha, \beta)} + (\alpha - 1)\ln{x} +\phantom{1}$\\$ + (\beta - 1)\ln{(1 - x)} )$. $u_1(x)=\ln{x}$, $u_2(x)=\ln{(1-x)}$.
    
    \textbf{(д)} $\rho_{p}(x) = p^x(1-p)^{1-x} = \exp(x\ln{p} + (1 - x)\ln{(1-p)}) = \exp(\ln{(1 - p)} + x\ln{\frac{p}{1-p}})$. $u_1(x)=x$.
    
    \textbf{(е)} $\rho_{\lambda}(x) = \frac{\lambda^x e^{-\lambda}}{x!} = \frac{1}{x!} \exp(-\lambda + x \ln{\lambda})$. $u_1(x) = x$.
\end{solution}

\begin{problem}
    Пусть $X_1, \ldots, X_n$ -- выборка из $Pois(\lambda)$. Для каких функций $\tau (\lambda)$ существует эффективная оценка? Найдите $i(\lambda)$ -- информацию одного наблюдения выборки.
\end{problem}

\begin{solution}
    Воспользуемся последней теоремой и предыдущей задачей. В совокупности они утвержают, что эффективной оценкой является линейная функция от $\lambda^* = \overline{u_1(X)} = \overline{X}$, причём она оценивает $\tau(\lambda) = -a_0'(\lambda) / a_1'(\lambda)$, где $a_0(\lambda) = -\lambda$, $a_1(\lambda) = \ln{\lambda}$, то есть $\tau(\lambda) = \lambda$. Из критерия эффективности
    \[
    i(\lambda) = \frac{\tau'(\lambda)\cdot (\ln{\rho_{\lambda}(x)})'_{\lambda}}{\lambda^* - \tau(\lambda)} = \frac{1 \cdot (-\lambda + x\ln{\lambda} - \ln{x!})'_{\lambda}}{x - \lambda} = \frac{x / \lambda - 1}{x - \lambda} = \frac{1}{\lambda}.
    \]
    Впрочем, её несложно найти и напрямую, найдя дисперсию вклада выборки.
\end{solution}

\begin{problem}\label{fisher_norm}
    Пусть $X_1, \ldots, X_n$ -- выборка из нормального распределения с параметрами $(a, \sigma^2)$. Найдите эффективную оценку \textbf{(а)} параметра $a$, если $\sigma^2$ известно; \textbf{(б)} параметра $\sigma^2$, если $a$ известно; \textbf{(в)} параметра $(a, a^2+\sigma^2)$. \textbf{(г)} Существует ли эффективная оценка для параметра $(a, \sigma^2)$? Вычислите информацию Фишера одного наблюдения во всех случаях.
\end{problem}

\begin{solution}
    \textbf{(а)} Имеем $\rho_{a}(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x - a)^2}{2\sigma^2}\right) = \exp\left(- \frac{1}{2\sigma^2}x^2\right)\cdot \exp\left(\left(\frac12\ln{\frac{1}{2\pi\sigma^2}} - \frac{a^2}{2\sigma^2}\right) + \frac{a}{\sigma^2}x\right)$. Стало быть, наше модель принадлежит экспоненциальному семейству, а значит, по теореме $\overline{u_1(x)}=\overline{X}$ является эффективной оценкой для $-a_0'(a) / a_1'(a) = -\left(\frac12\ln{\frac{1}{2\pi\sigma^2}} - \frac{a^2}{2\sigma^2}\right)'_a / \left(\frac{a}{\sigma^2}\right)'_a = a$. Для разнообразия посчитаем информацию одного наблюдения по определению:
    \[
    i(a) = \va[a] \left(\frac{\partial}{\partial a} \ln{\rho_{a}(x)}\right) = \va[a] \left(\frac{x - a}{\sigma^2}\right) = \va \frac{x}{\sigma^2} = \frac{1}{\sigma^4}\va[a] x = \frac{1}{\sigma^2}.
    \]
    
    \textbf{(б)} Модель всё ещё лежит в экспоненциальном семействе:
    \[
    \rho_{\sigma^2}(x) = \exp\left(-\frac12\ln{2\pi\sigma^2} - \frac{1}{2\sigma^2}(x - a)^2\right).
    \]
    По теореме имеется эффективная оценка $\widehat{\sigma^2} = \overline{(X-a)^2}$ для $\tau(\sigma^2) = \left(\frac{1}{2}\ln{2\pi\sigma^2}\right)'_{\sigma^2} / \left(\frac{-1}{2\sigma^2}\right)'_{\sigma^2} = \frac{2\pi}{4\pi\sigma^2}\cdot 2\sigma^4 = \sigma^2.$ Из критерия эффективности для одноэлементной выборки
    \[
    i(\sigma^2) = \frac{\tau'(\sigma^2)\cdot (\ln{\rho_{\sigma^2}(x)})'_{\sigma^2}}{\widehat{\sigma^2} - \tau(\sigma^2)} = \frac{1 \cdot \left(\frac{-1}{2\sigma^2}+\frac{(x-a)^2}{2\sigma^4}\right)}{(x-a)^2 - \sigma^2} = \frac{1}{2\sigma^4}.
    \]
    
    \textbf{(в)} Будем пользоваться критерием эффективности. Тут-то и начинается многомерное веселье:
    \[
    \frac{\partial}{\partial a} \ln{\rho(X)} = \sum \frac{X_i - a}{\sigma^2}\,\,\,\frac{\partial}{\partial \sigma^2} \ln{\rho(X)} = -\frac{n}{2\sigma^2} + \sum \frac{(X_i-a)^2}{2\sigma^4}.
    \]
    Нахождение информационной матрицы упрощается тем, что на её диагонали стоят $\mathsf{cov}\left(\frac{\partial}{\partial a} \ln{\rho(X)}, \frac{\partial}{\partial a} \ln{\rho(X)}\right) = I_X(a) = \frac{n}{\sigma^2}$ и $\mathsf{cov}\left(\frac{\partial}{\partial \sigma^2} \ln{\rho(X)}, \frac{\partial}{\partial \sigma^2} \ln{\rho(X)}\right) = I_X(\sigma^2) = \frac{n}{2\sigma^4}$, которые мы посчитали ранее. С остальными элементами матрицы нам не очень повезло:
    \begin{gather*}
        \mathsf{cov}\left(\frac{\partial}{\partial a} \ln{\rho(X)}, \frac{\partial}{\partial \sigma^2} \ln{\rho(X)}\right) = \mathsf{cov}\left(\sum \frac{X_i - a}{\sigma^2}, -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\left(\sum X_i^2 - 2a\sum X_i + na^2\right)\right) \eqcirc
    \end{gather*}
    Заметим, что ковариация с константой равна 0, а значит, вынося множители за знак ковариации:
    \begin{gather*}
        \eqcirc \frac{1}{2\sigma^6}\mathsf{cov}\left(\sum X_i, \sum X_i^2 - 2a\sum X_i\right) = \frac{1}{2\sigma^6}\mathsf{cov}\left(\sum X_i, \sum X_i^2\right) - \frac{a}{\sigma^6}\mathsf{cov}\left(\sum X_i, \sum X_i\right) \eqcirc
    \end{gather*}
    Вспоминаем, что элементы выборки независимы, а в сумме выше выживают лишь ковариации по одинаковым индексам:
    \[
    \eqcirc
    \frac{1}{2\sigma^6}\sum \mathsf{cov}\left(X_i, X_i^2\right) - \sum\frac{a}{\sigma^6}\mathsf{cov}\left(X_i, X_i\right) = \frac{n}{2\sigma^2}(\me X_1^3 - \me X_1 \cdot \me X_1^2 - 2a\va X_1) \eqcirc
    \]
    3-ий момент либо считаем ручками, либо смотрим в Википедию:
    \[
    \eqcirc \frac{n}{2\sigma^2}(a^3+3a\sigma^2 - a(\sigma^2 + a^2) - 2a\sigma^2) = 0.
    \]
    Находим матрицу Якоби для $\tau(a, \sigma^2) = (a, a^2 + \sigma^2)$, обращаем информационную матрицу $I_X(\theta)$ (благо это нетрудно) и считаем ответ:
    \begin{gather*}
    \theta^* = \tau'(\theta) I^{-1}_X(\theta) (\ln{\rho_{\theta}(X)})'_{\theta} = \begin{pmatrix}a\\a^2+\sigma^2
    \end{pmatrix} + 
    \begin{pmatrix}
    1 & 0\\
    2a & 1
    \end{pmatrix}
    \begin{pmatrix}
    \frac{\sigma^2}{n} & 0\\
    0 & \frac{2\sigma^4}{n}
    \end{pmatrix}
    \begin{pmatrix}
    \sum \frac{X_i - a}{\sigma^2}\\
    -\frac{n}{2\sigma^2} + \sum \frac{(X_i-a)^2}{2\sigma^4}
    \end{pmatrix} = \\
    = \begin{pmatrix}a\\a^2+\sigma^2
    \end{pmatrix} + 
    \begin{pmatrix}
    \frac{\sigma^2}{n} & 0\\
    \frac{2a\sigma^2}{n} & \frac{2\sigma^4}{n}
    \end{pmatrix}
    \begin{pmatrix}
    \frac{n}{\sigma^2}(\overline{X} - a)\\
    -\frac{n}{2\sigma^2} + \frac{n}{2\sigma^4}\left(\overline{X^2} - 2a\overline{X} + a^2\right)
    \end{pmatrix} =\\
    = \begin{pmatrix}a\\a^2+\sigma^2
    \end{pmatrix} + 
    \begin{pmatrix}
    \overline{X} - a\\
    \overline{X^2} - \sigma^2 - a^2
    \end{pmatrix} = \begin{pmatrix}
    \overline{X}\\
    \overline{X^2}.
    \end{pmatrix}
    \end{gather*}
    
    \textbf{(г)} Предположим, что такая оценка $\theta^*$ существует. Тогда по критерию эффективности
    \[
    \theta^* = \begin{pmatrix}a\\\sigma^2\end{pmatrix}+
    \begin{pmatrix}
    \frac{\sigma^2}{n} & 0\\
    0 & \frac{2\sigma^4}{n}
    \end{pmatrix}
    \begin{pmatrix}
    \frac{n}{\sigma^2}(\overline{X} - a)\\
    -\frac{n}{2\sigma^2} + \frac{n}{2\sigma^4}\left(\overline{X^2} - 2a\overline{X} + a^2\right)
    \end{pmatrix} = \begin{pmatrix}
    \overline{X}\\
    \overline{X^2} - 2a\overline{X} + a^2
    \end{pmatrix}.
    \]
    Полученная статистика не является статистикой, так как имеется явная зависимость от параметра. Значит, эффективной оценки для данной модели не существует.
\end{solution}

Следующая задача носит довольно технический характер, но она проясняет понятие информации Фишера, а также подводит нас к новой теме.

\begin{problem}\label{reduction}
    Пусть $S(X)$ -- статистика, обобщённая плотность которой равна $g_{\theta}(s)$. Определим информацию Фишера $I_S(\theta) = \va\left( \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right)$ и потребуем выполнения условия регулярности $\frac{\partial}{\partial \theta} \me T(X) = \me \left(T(X)\frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right)$ для любой $S(X)$-измеримой статистики $T(X)$. Докажите, что
    
    \textbf{(а)} $\me \left(\frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}|S(X)\right) = \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}$;
    
    \textbf{(б)} $I_S(\theta) = \mathsf{cov}(\frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}, \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))})$;
    
    \textbf{(в)} $I_S(\theta) \le I_X(\theta)$.
\end{problem}

\begin{solution}
\textbf{(а)} Проверяется по определению. $\frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}$ будет $S(X)$-измеримой как функция от $S(X)$. По условиям регулярности для любого $C \in \sigma(S)$:
\[
\me \left(I_C(X)\frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right) = \frac{\partial}{\partial \theta} \me I_C(X) = \me \left(I_C(X)\frac{\partial}{\partial \theta }\ln{\rho_{\theta}(X)}\right).
\]

\textbf{(б)} По определению
\begin{gather*}
\mathsf{cov}\left(\frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}, \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right) = \me\left( \frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\cdot \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right) -\\
\phantom{1} - \me\left( \frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\right)\cdot \me\left( \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right) = \me\left( \frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\cdot \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right) \eqcirc
\end{gather*}
так как по задаче \ref{zerome} $\me\left( \frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\right)=0$. Тогда по пункту \textbf{(а)} и формуле полного матожидания
\begin{gather*}
\eqcirc \me\left[ \me\left(\left.\frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\cdot \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right|S(X)\right)\right] = \me\left[\frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\cdot \me\left(\left.\frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\right|S(X)\right)\right]=\\
= \me\left( \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right)^2 = I_S(X).
\end{gather*}
То, что дисперсия и второй момент у $\frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}$ совпадают, доказывается аналогично задаче \ref{zerome}.

\textbf{(в)} Применяем предыдущий пункт и неравенство Коши-Буняковского:
\begin{gather*}
    I_S(X) = \me\left( \frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\cdot \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right) \le \sqrt{\me\left( \frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}\right)^2\cdot \me\left(\frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}\right)^2} = \\
    = \sqrt{I_X(\theta)\cdot I_S(\theta)} \Longrightarrow I_S(\theta) \le I_X(\theta).
\end{gather*}
\end{solution}

Мы получили очень любопытный результат: если мы \textit{редуцируем} данные и рассматриваем не всю выборку $X$, а лишь статистику от неё $S(X)$, то информация Фишера либо остаётся той же, либо уменьшается, что соответствует нашим ожиданиям. Это в очередной раз подтверждает, что $I_X(\theta)$ является показательной мерой того, насколько много данных содержится в выборке.

Но возникает вопрос: а когда достигается равенство в пункте \textbf{(в)}? Так как при доказательстве мы использовали неравенство Коши-Буняковского, то равенство будет достигаться, если $\frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)}$ и $\frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}$ будут линейно зависимыми почти наверное. Но из пункта \textbf{(а)} мы знаем, что одно есть УМО от другого, поэтому они обязаны почти наверное совпадать, то есть для каждого $\theta \in \Theta$ 
\[
\frac{\partial}{\partial \theta}\ln{\rho_{\theta}(X)} \stackrel{\pth-\text{п. н.}}{=} \frac{\partial}{\partial \theta }\ln{g_{\theta}(S(X))}.
\]
Стало быть, выражения под знаками $\frac{\partial}{\partial \theta}$ отличаются на константу, не зависящую от $\theta$ (но может быть от $X$), то есть
\begin{equation}\label{factor}
    \rho_{\theta}(X) = g_{\theta}(S(X))\cdot h(X).
\end{equation}
Итог: только статистики $S(X)$, которые удовлетворяют равенству (\ref{factor}), сохраняют информацию при редуцировании данных. Из-за подобного свойства при "сжатии"\, эти статистики представляют особый интерес с практической точки зрения. Рассмотрим их поподробнее.

