\setcounter{problem}{0}
\section{Основные методы нахождения оценок}
\subsection{Метод моментов}
Допустим, что распределение элементов выборки зависит от $k$ неизвестных параметров $\theta_1, \ldots, \theta_k$, где вектор $\theta = (\theta_1, \ldots, \theta_k)$ принадлежит некоторой области $\Theta$ в $\R^k$. Для построения оценки по методу моментов возьмём такие борелевские $g_1, \ldots, g_k\colon \R \to \R$, что $\forall {i\in\{1,\ldots, k\}}$ определено $\me[]g_i(X_1) = m_i(\theta)$. Предположим, что у уравнения $m(\theta) = \overline{g(X)}$ имеется единственное решение, где $m = (m_1, \ldots, m_k)$, $g = (g_1, \ldots, g_k)$. Так как из закона больших чисел мы знаем, что $\overline{g_i(X)}$ примерно равно $\me[]g_i(X_1)$, то логично положить решение уравнения выше за оценку параметра. А именно:
\begin{theorem*}[сильная состоятельность оценки по методу моментов]
Пусть $m\colon \Theta \to m(\Theta)$ - биекция, и функцию $m^{-1}$ можно доопределить до функции, заданной на всем $\R^k$, и непрерывной в каждой точке множества $m(\Theta)$. Также, $\me[]g_i(X_1) < \infty$ $\forall{i\in\{1,\ldots, k\}}$, $\forall{\theta \in \Theta}$. Тогда оценка по методу моментов является сильно состоятельной оценкой параметра $\theta$.
\end{theorem*}

Часто в задачах не получается доопределить $m^{-1}$ на всё $\R^k$, но это и не надо, если $g(X) \in m(\Theta)$.

Для упрощения вычислений часто в качестве $g_i(t)$ берут $t^i$ (такие фукнции называют \textit{пробными}), и тогда соответствующая $m_i(\theta)$ называется \textit{моментом $i$-ого порядка}, откуда собственно и пошло название метода.

Схожее утверждение можно дать про асимптотическую нормальность:
\begin{theorem*}[асимптотическая нормальность оценки по методу моментов]
Если в условиях предыдущей теоремы функция $m_{-1}$, доопределенная на $\R^k$, дифференцируема на $m(\Theta)$, и $\me[]g_i(X_1)^2 < \infty$ $\forall{i\in\{1,\ldots, k\}}$, $\forall{\theta \in \Theta}$, то оценка, полученная по методу моментов, асимптотически нормальна.
\end{theorem*}

Как можно видеть, оценки по методу моментов интуитивно понятны и легки в построении. Однако обычно асимптотическая дисперсия оценок, полученных по методу моментов, довольно велика, в то время как оценки, построенные другими методами, оказываются более выигрышными (например, как в задаче \ref{quantile}). К тому же не факт, что они будут несмещёнными.

\begin{problem}
    Найдите оценку по методу моментов для параметров распределений \textbf{(а)} $Pois(\lambda)$, \textbf{(б)} $Geom(p)$, \textbf{(в)} $Beta(\alpha, \beta)$, \textbf{(г)} $U(a, b)$.
\end{problem}
 
\begin{solution}
    \textbf{(а)} Так как $m(\lambda) = \me[\lambda] X = \lambda$ -- тождественная, то в качестве оценки параметра можно взять $\overline{X}$.
    
    \textbf{(б)} В данном случае $m(p) = \me[p] X = \frac{1-p}{p} = \frac{1}{p} - 1$, тогда $m^{-1}(t) = \frac{1}{t+1}$. Таким образом, оценка по методу моментов $\widehat{p} = \frac{1}{\overline{X}+1}$.
    
    \textbf{(в)} Тут уже надо оценивать двумерный параметр $\theta = (\alpha, \beta)$, поэтому найдём первый и второй моменты: $m_1(\alpha, \beta) = \me X_1 = \frac{\alpha}{\alpha + \beta} = x$, $m_2(\alpha, \beta) = \me X_1^2 = \va X_1 + (\me X_1)^2 = \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} + \frac{\alpha^2}{(\alpha + \beta)^2} = \frac{\alpha^3+\alpha^2\beta+\alpha^2+\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} = \frac{\alpha(\alpha+1)}{(\alpha + \beta)(\alpha + \beta + 1)} = y$. Тогда
    \begin{gather*}
    \left.
    \begin{aligned}
    \frac{x}{y} = \frac{\alpha + \beta + 1}{\alpha + 1} = 1 + \frac{\beta}{\alpha + 1},\;\;\;\frac{\beta}{\alpha+1} = \frac{x}{y} - 1 ,\;\;\;\frac{\alpha}{\beta} + \frac{1}{\beta}= \frac{y}{x - y}\\
    \frac{1}{x} = \frac{\alpha + \beta}{\alpha} = 1 + \frac{\beta}{\alpha},\;\;\; \frac{\alpha}{\beta} = \frac{x}{1 - x}
    \end{aligned}
    \right\} \Longrightarrow \frac{1}{\beta} = \frac{y}{x-y} - \frac{x}{1-x}\Longrightarrow\\
    \beta = \frac{(x-y)(1-x)}{y-x^2} ,\;\;\;\alpha = \frac{x(x-y)}{y-x^2}.
    \end{gather*}
    С учётом того, что $\overline{X^2} - \overline{X}^2=s^2$, оценку по методу моментов можно записать как
    \[
    \widehat{\alpha} = \frac{\overline{X}(\overline{X} - \overline{X^2})}{s^2},\;\;\;\widehat{\beta} = \frac{(1 - \overline{X})(\overline{X} - \overline{X^2})}{s^2}.
    \]
    
    \textbf{(г)} $m_1(a, b) = \me X_1 = \frac{a+b}{2}=x$, $m_2(a, b) = \me X_1^2 = \va X_1 + (\me X_1)^2 = \frac{(b-a)^2}{12} + \frac{(a+b)^2}{4} = \frac{a^2+b^2+ab}{3}=y$. Откуда $a+b = 2x$, $ab = a^2 + 2ab + b^2 - 3y = 4x^2 - 3y$, что есть коэффициенты квадратного уравения с корнями $a$ и $b$. С учётом того, что $a \le b$, получаем, что $a = x - \sqrt{3y - 3x^2}$, $b = x + \sqrt{3y - 3x^2}$. Итого имеется следующая оценка по методу моментов:
    \[
    \widehat{a} = \overline{X} - \sqrt{3s^2},\;\;\;\widehat{b} = \overline{X} + \sqrt{3s^2}.
    \]
\end{solution}

\subsection{Метод максимального правдоподобия}

\begin{definition}
Пусть $\mathcal P_{\theta}$ -- доминируемое семейство распределений с плотностью $\rho_{\theta}(x)$. \textit{Функцией правдоподобия} выборки $X_1, \ldots, X_n$ называется плотность их совместного распределения
\[
f_{\theta}(X_1, \ldots, X_n) = \rho_{\theta}(X_1)\ldots \rho_{\theta}(X_n).
\]
Величина
\[
L_{\theta}(X_1, \ldots, X_n) = \ln{f_{\theta}(X_1, \ldots, X_n)}
\]
называется \textit{логарифмической функцией правдоподобия}.

\textit{Оценкой максимального правдоподобия} параметра $\theta$ называется статистика 
\[
\widehat{\theta}(X) = \argmax_{\theta \in \Theta} f_{\theta}(X_1, \ldots, X_n)
\]
\end{definition}

Заметим, что точки максимума функции правдоподобия и её логарифмического брата-близнеца совпадают в силу монотонности логарифма, поэтому максимизировать можно любую из этих функций. Практически всегда мы будем просто находить нули производных и максимально забивать на доказательство того, что они будут являться точками максимума. Ну, или считайте, что доказательство даётся читателю в качестве упражнения.

\begin{problem}
    Найдите оценки по методу максимального правдоподобия для параметров следующих распределений: \textbf{(а)} $Geom(p)$, \textbf{(б)} $U(0, a)$, \textbf{(в)} $\mathcal N (a, \sigma^2)$, \textbf{(г)} оценку параметра $\lambda$ распределения $\Gamma(\alpha, \lambda)$, считая $\alpha$ известным, \textbf{(д)} $Pareto(k, a)$.
\end{problem}

\begin{solution}
    \textbf{(а)}
    \begin{gather*}
        f_{p}(X_1, \ldots, X_n) = \prod (1-p)^{X_i}p,\;\;\;L_{p}(X_1, \ldots, X_n) = n\ln{p} + \sum X_i\ln{(1-p)},\\
        \frac{\partial}{\partial p}L_{p}(X_1, \ldots, X_n) = \frac{n}{p} - \sum \frac{X_i}{1 - p} = 0,\;\;\; \frac{n(1-p)-p\sum X_i}{p(1-p)}=0,\\
        p\left(n + \sum X_i\right) = n \Longrightarrow \widehat{p} = \frac{n}{n + \sum X_i} = \frac{1}{1 + \overline{X}}.
    \end{gather*}
    
    \textbf{(б)} Один из немногих случаев, когда тупо взять и продифференцировать не выйдет. Функция правдоподобия выглядит так
    \[
    f_{a}(X_1, \ldots, X_n) = \frac{1}{a^n}I(0 \le X_1,\ldots, X_n \le a).
    \]
    Там, где $f_{a}$ не равна нулю, она равна некоторой константе $\frac{1}{a^n}$, которую надо максимизировать, то есть надо минимизировать $a$. Но сделать её меньше $X_{(n)}$ не получится, так как иначе не выполнится условие под индикатором. Следовательно, $\widehat{a} = X_{(n)}$ будет искомой ОМП.
    
    \textbf{(в)} 
    \begin{gather*}
        f_{\theta}(X_1,\ldots, X_n) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum{\frac{(X_i-a)^2}{2\sigma^2}}\right),\;\; L_{\theta}(X_1, \ldots, X_n) = -\frac{n}{2}\ln{2\pi\sigma^2} - \sum{\frac{(X_i-a)^2}{2\sigma^2}}\\
        \left\{
        \begin{aligned}
        &\frac{\partial}{\partial a} = \sum{\frac{X_i-a}{\sigma^2}} = 0,\\
        &\frac{\partial}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \sum{\frac{(X_i-a)^2}{2\sigma^4}} = 0.
        \end{aligned}\right.\;\;\;
        \left\{
        \begin{aligned}
        &\widehat{a} = \overline{X},\\
        &\frac{1}{\sigma^2}\underbrace{\sum (X_i - \overline{X})^2}_{\phantom{0}=ns^2} - n = 0.
        \end{aligned}\right.\\
        \left\{
        \begin{aligned}
        &\widehat{a} = \overline{X},\\
        &\widehat{\sigma^2} = s^2.
        \end{aligned}\right.
    \end{gather*}
    
    \textbf{(г)}
    \[
    f_{\lambda}(X_1,\ldots, X_n) = \frac{\lambda^{n\alpha} \left(\prod X_i\right)^{\alpha-1}}{\Gamma(\alpha)^n}e^{-\lambda\sum X_i}I(X_1,\ldots,X_n>0).
    \]
    Для $X_1,\ldots,X_n>0$ имеем
    \begin{gather*}
        L_{\lambda}(X_1,\ldots, X_n) = n\alpha\ln{\lambda} - \lambda \sum X_i + \ln{\left(\left(\prod X_i\right)^{\alpha-1}\right)} - n\ln{\Gamma(\alpha)},\\
        \frac{\partial}{\partial \lambda} L_{\lambda}(X_1,\ldots, X_n) = \frac{n\alpha}{\lambda} - \sum X_i = 0 \Longrightarrow \widehat{\lambda} = \frac{\alpha}{\overline{X}}.
    \end{gather*}
    
    \textbf{(д)}
    \[
    f_{\lambda}(X_1,\ldots, X_n) = \frac{k^{n} a^{nk}}{\left(\prod X_i\right)^{k+1}}I(X_1,\ldots,X_n\ge a).
    \]
    Для фиксированного $k$ максимум $f_{\theta}$ достигает при $\widehat{a} = X_{(1)}$ (аналогично пункту \textbf{(б)}). Тогда если принять $a$ равным первой порядковой статистике, получаем
    \begin{gather*}
        L_{\theta}(X_1,\ldots, X_n) = n \ln{k} + nk\ln{X_{(1)}} - (k+1)\sum \ln{X_i}\\
        \frac{\partial}{\partial \theta} L_{\theta}(X_1,\ldots, X_n) = \frac{n}{k} + n\ln{X_{(1)}} - \sum \ln{X_i} = 0,\;\;\; \frac{1}{k} = \overline{\ln{X}} - \ln{X_{(1)}}\Longrightarrow\\
        \widehat{k} = \frac{1}{\overline{\ln{X}} - \ln{X_{(1)}}}.
    \end{gather*}
\end{solution}

\begin{problem}
    Найдите оценку максимального правдоподобия для параметра сдвига в модели распределения Коши,
    \[
    \rho_{\theta}(x) = \frac{1}{\pi(1+(x-\theta)^2)},
    \]
    если выборка состоит из \textbf{(а)} одного наблюдения, \textbf{(б)} двух наблюдений (т.е. $n = 1, 2$).
\end{problem}

\begin{solution}
    \textbf{(а)} Тут всё весьма просто:
    \begin{gather*}
        f_{\theta}(x) = \frac{1}{\pi(1+(x-\theta)^2)}\\
        f'_{\theta}(x) = \frac{1}{\pi(1+(x-\theta)^2)^2}\cdot 2(x - \theta) = 0 \Longrightarrow \widehat{\theta} = x.
    \end{gather*}
    
    \textbf{(б)} И тут становится понятно, почему распределения Коши не было в предыдущей задаче...
    \begin{gather*}
        f_{\theta}(x_1, x_2) = \frac{1}{\pi^2(1+(x_1-\theta)^2)(1+(x_2-\theta)^2)}\\
        L_{\theta}(x_1, x_2) = - \ln{(1+(x_1-\theta)^2)} - \ln{(1+(x_2-\theta)^2)} - 2\ln{\pi}\\
        \frac{\partial}{\partial \theta} L_{\theta}(x_1, x_2) = \frac{2(x_1-\theta)}{1+(x_1-\theta)^2} + \frac{2(x_2-\theta)}{1+(x_2-\theta)^2} = 0\\
        x_1 - \theta + (x_1 - \theta)(x_2 - \theta)^2 + x_2 - \theta + (x_2 - \theta)(x_1 - \theta)^2 = 0\\
        \left((x_1 - \theta)(x_2 - \theta) + 1\right)(x_1-\theta+x_2-\theta) = 0\Longrightarrow\\
        \left[
        \begin{aligned}
        &\widehat{\theta} = \overline{X}\\
        &\widehat{\theta}^2 - \widehat{\theta}(X_1 + X_2) + X_1X_2 + 1=0
        \end{aligned}
        \right.;\;\;\;
        \left[
        \begin{aligned}
        &\widehat{\theta} = \overline{X}\\
        &\widehat{\theta} = \overline{X} \pm \frac{\sqrt{(X_1-X_2)^2-4}}{2}
        \end{aligned}
        \right.
    \end{gather*}
    При $|X_1 - X_2| \le 2$ второго решения нет, а значит, имеется ОМП $\widehat{\theta} = \overline{X}$. Иначе определено ещё два решения, причём несложно проверить, что именно они будут доставлять максимум, а $\overline{X}$ -- локальный минимум. То есть в случае $|X_1 - X_2| > 2$ оценками максимального правдоподобия будут $\widehat{\theta} = \overline{X} \pm \frac{\sqrt{(X_1-X_2)^2-4}}{2}$.
\end{solution}

\begin{problem}
    Напомним, что плотность гауссовского вектора размерности $k$ равна
    \[
    \rho(x_1, \ldots, x_k) = (2\pi)^{-k/2}(\det{\Sigma})^{-1/2}\exp{\left(-\frac{1}{2}(\mathbf{x}-a)^T\Sigma^{-1}(\mathbf{x}-a)\right)}.
    \]
    Пусть $X_1, \ldots, X_n \sim \mathcal N (a,\Sigma)$ -- независимые гауссовские векторы. Найдите оценку максимального правдоподобия для вектора средних $a$ и ковариационной матрицы $\Sigma$, где $a \in \R^{k}$, $\Sigma \in \mathbb{S}^k_{++}$.
\end{problem}

\begin{solution} Найдём логарифмическую функцию правдоподобия:
\begin{gather*}
    f_{\theta}(X_1, \ldots, X_n) = (2\pi)^{-nk/2}(\det{\Sigma})^{-n/2}\exp{\left(\sum_{i=1}^n-\frac{1}{2}(X_i-a)^T\Sigma^{-1}(X_i-a)\right)},\\
    \ln{f_{\theta}(X_1, \ldots, X_n)} = -\frac{nk}{2}\ln{2\pi} - \frac{n}{2}\ln{\det{\Sigma}} - \frac{1}{2}\sum_{i=1}^n (X_i-a)^T\Sigma^{-1}(X_i-a).
\end{gather*}
С производной по $a$ всё плюс-минус ясно, хотя для дальнейшего понимания выпишем её через дифференциал:
\begin{gather*}
d_a \ln{f_{\theta}(X_1, \ldots, X_n)} = - \frac{1}{2} d_a\left( \sum_{i=1}^n (X_i-a)^T\Sigma^{-1}(X_i-a)\right) =\\
= - \frac{1}{2}\sum_{i=1}^n\left(d_a(X_i-a)^T\Sigma^{-1}(X_i-a) + (X_i-a)^T\Sigma^{-1}d_a(X_i-a)\right) =\\
= \frac{1}{2}\sum_{i=1}^n\left(d_a a^T\Sigma^{-1}(X_i-a) + (X_i-a)^T\Sigma^{-1}d_a a\right) = \frac{1}{2}\sum_{i=1}^n\left(\langle d_a a, \Sigma^{-1}(X_i-a) \rangle+ \langle \Sigma^{-T}(X_i-a), d_a a \rangle \right) = \\
=\left< \sum_{i=1}^n \Sigma^{-1}(X_i-a), d_a a \right> = 0 \Longrightarrow \sum_{i=1}^n \Sigma^{-1}(X_i-a) = 0 \Longrightarrow \widehat{a} = \overline{X}.
\end{gather*}
\end{solution}
Тут даже представляется возможным найти второй дифференциал, тем самым можно показать, что при фиксированной $\Sigma$ полученная оценка $\widehat{a}$ доставляет максимум функции правдоподобия, но мы на это как обычно забьём.

Куда интереснее найти дифференциал по $\Sigma$. Для этого сделаем следующий трюк: сумму в логарифмической функции правдоподобия представим как след от одноэлементной матрицы:
\[
\ln{f_{\theta}(X_1, \ldots, X_n)} = -\frac{nk}{2}\ln{2\pi} - \frac{n}{2}\ln{\det{\Sigma}} - \frac{1}{2}\tr{\sum_{i=1}^n (X_i-a)^T\Sigma^{-1}(X_i-a)}.
\]
Это окажется весьма удобным, так как по свойству следа функцию теперь можно записать так:
\begin{gather*}
\ln{f_{\theta}(X_1, \ldots, X_n)} = -\frac{nk}{2}\ln{2\pi} - \frac{n}{2}\ln{\det{\Sigma}} - \frac{1}{2}\sum_{i=1}^n \tr{(X_i-a)(X_i-a)^T\Sigma^{-1}} = \\
= -\frac{nk}{2}\ln{2\pi} - \frac{n}{2}\ln{\det{\Sigma}} - \frac{1}{2}\sum_{i=1}^n \langle (X_i-a)(X_i-a)^T, \Sigma^{-1}\rangle.
\end{gather*}
Осталось также вспомнить (или загуглить) формулу для дифференциала определителя:
\[
d (\det{\Sigma}) = \det{\Sigma} \cdot \langle\Sigma^{-T}, d \Sigma\rangle,
\]
где $\Sigma^{-T} = (\Sigma^T)^{-1}$ (что в нашем случае просто $\Sigma^{-1}$). 

{\footnotesize Эту формулу на самом деле несложно вывести, если вспомнить, что частная производная определителя по элементу матрицы -- это соответствующее алгебраическое дополнение, а у нас как раз есть формула из алгема, связывающее матрицы из алгебраических дополнений и обратную. Но вернёмся к нашим баранам.}

Чтобы не вспоминать дифференциал для обратной матрицы, введём замену $\Xi = \Sigma^{-1}$ и будем дифференцировать по ней:
\begin{gather*}
    d_{\Xi} \ln{f_{\theta}(X_1, \ldots, X_n)} = \frac{n}{2}d_{\Xi} (\ln{\det{\Xi}}) - \frac{1}{2} d_{\Xi} \left( \sum_{i=1}^n \langle (X_i-a)(X_i-a)^T, \Xi\rangle\right) =\\
    = \frac{n}{2\det{\Xi}} d_{\Xi}(\det{\Xi}) - \frac{1}{2} \sum_{i=1}^n \left< (X_i-a)(X_i-a)^T, d_{\Xi} \Xi\right> = \\
    = \left< \frac{n}{2}\Xi^{-1} - \frac{1}{2} \sum_{i=1}^n (X_i-a)(X_i-a)^T, d_{\Xi} \Xi\right> = 0 \Longrightarrow \Xi^{-1} = \frac{1}{n}\sum_{i=1}^n (X_i-a)(X_i-a)^T.
\end{gather*}

С учётом того, что оценку для $a$ мы нашли ранее, получаем итоговый ответ:
\[
\widehat{a} = \overline{X},\;\;\;\widehat{\Sigma} = \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X})(X_i-\overline{X})^T.
\]


\subsection{Метод выборочных квантилей}

Один из способов нахождения оценок является рассмотрение порядковых статистик из выборки. Идея в следующем: для непрерывных функций распределения $F_{\theta}(X_1) \sim U[0, 1]$ (строго это доказано в задаче \ref{uniform} из листка по доверительным интервалам). Поэтому если мы возьмём $p$-ую часть от выборки в порядке возрастания, то последний её элемент при действии $F_{\theta}$ будет \textit{примерно} равен $p$, а значит, сам он примерно равен $F^{-1}_{\theta}(p)$. Формализуем вышесказанное.

\begin{definition}
\textit{$p$-квантилью} распределения $\pth[]$ называется $z_p = \inf\{x\colon F_{\pth[]}(X) \ge p\}$, где $p \in (0; 1)$.
\end{definition}

\begin{definition}
    Пусть $X_1,\ldots X_n$ -- выборка. Статистика
    \[
    z_{n,p} = \left\{
    \begin{aligned}
    X_{(np)+1},\,\,\,np \notin \Z,\\
    X_{(np)},\,\,\, np \in \Z
    \end{aligned}
    \right.
    \]
    называется \textit{выборочным квантилем}.
\end{definition}

\begin{theorem*}[о выборочной квантиле]
    Пусть $X_1,\ldots X_n$ -- выборка из распределения $\pth[]$ с плотностью $\rho(x)$. Пусть $z_p$ -- $p$-квантиль распределения $\pth[]$, причем $\rho(x)$ непрерывно дифференцируема в окрестности $z_p$ и $\rho(z_p) > 0$. Тогда
    \[
    \sqrt{n}(z_{n, p} - z_p) \stackrel{d}{\to} \mathcal{N}\left(0, \frac{p(1-p)}{\rho^2(z_p)}\right).
    \]
\end{theorem*}

Особенно часто выделяют случай, когда $p=1/2$. Этот случай настолько исключительный, что для него придумали другое определение выборочного квантиля:
\begin{definition}
    \textit{Выборочной медианой} для выборки $X_1,\ldots X_n$ называется
    \[
    \widehat{\mu} = \left\{
    \begin{aligned}
    X_{(k+1)},\,\,\,n = 2k+1,\\
    \frac{X_{(k)}+X_{(k+1)}}{2},\,\,\, n = 2k
    \end{aligned}
    \right.
    \]
\end{definition}

Для неё также справедлива теорема выше, только обычно $z_{n, 1/2}$ заменяют на $\widehat{\mu}$.

\begin{problem}\label{quantile}
    Постройте асимптотически нормальную оценку для параметра масштаба в модели распределения Коши:
    \[
    \rho_{\theta}(x) = \frac{\theta}{\pi(\theta^2+x^2)}.
    \]
\end{problem}

\begin{solution}
    Функция распределения будет равняться
    \[
    F_{\theta}(x) = \int_{-\infty}^x \frac{\theta}{\pi(\theta^2+t^2)}\,dt = \left[s = \frac{t}{\theta}\right] = \int_{-\infty}^{\frac{x}{\theta}} \frac{1}{\pi(1+s^2)}\,ds = \left.\frac{1}{\pi}\arctg{s}\right|_{-\infty}^{\frac{x}{\theta}} = \frac{1}{\pi}\arctg{\frac{x}{\theta}} + \frac{1}{2}.
    \]
    Следовательно, $\frac{3}{4}$-квантилью для данного семейства распределений будет $z_{3/4} = \theta$. Тогда по теореме о выборочном квантиле $z_{n, 3/4}$ будет асимптотически нормальной оценкой параметра $\theta$:
    \[
    \sqrt{n}(z_{n, 3/4} - \theta) \stackrel{d}{\to} \mathcal{N}\left(0, \frac{3/4(1 - 3/4)}{\rho^2(\theta)}\right) = \mathcal{N}\left(0, \frac{3\pi^2\theta^2}{4}\right).
    \]
    
    В качестве поучительного примера решим задачу методом моментов. С пробными функциями он работать не будет, так как у распределения Коши нет матожидания. Поэтому рассмотрим $g(t) = \frac{1}{1+t^2}$. Для неё
    \begin{gather*}
        m(\theta) = \me g(X_1) = \int_{\R} \frac{\theta}{\pi(\theta^2+t^2)(1+t^2)}\,dt = \frac{\theta}{\pi(1-\theta^2)} \int_{\R} \left(\frac{1}{\theta^2+t^2} - \frac{1}{1+t^2}\right)\,dt = \\
        = \frac{\theta}{\pi(1-\theta^2)} \left(\frac{\pi}{\theta} - \pi\right) = \frac{1}{1 + \theta} \Longrightarrow \widehat{\theta} = 1\left/\overline{\frac{1}{1+X^2}}\right. - 1
    \end{gather*}
    Получилось весьма недурно. Но проверим, как наша оценка в плане асимптотической дисперсии:
    \[
    \sqrt{n}\left(\overline{g(X)} - \frac{1}{1+\theta}\right) \stackrel{d_{\theta}}{\to} \mathcal{N}(0, \va g(X_1))
    \]
    Дисперсию посчитаем с помощью Wolfram Alpha: $\va g(X_1) = \me g(X_1)^2 - (\me g(X_1))^2 = \frac{\theta + 2}{2(\theta + 1)^2} - \frac{1}{(\theta+1)^2} = \frac{\theta}{2(\theta + 1)^2}$. Применяя дельта-метод, получаем, что
    \[
    \sqrt{n}\left(\widehat{\theta} - \theta\right) \stackrel{d_{\theta}}{\to} \mathcal{N}\left(0, \frac{\theta}{2(\theta + 1)^2} \cdot (1+\theta)^4\right) = \mathcal{N}\left(0, \frac{\theta(\theta+1)^2}{2}\right).
    \]
    Как мы видим, асимптотическая дисперсия оценки по методу моментов получилась на порядок хуже, чем через выборочный квантиль (хотя стоит признать, для маленьких значений $\theta$ она будет всё же меньше). Можно воспринимать это как напоминание о том, что топорный метод моментов не всегда даёт лучший результат.
\end{solution}


