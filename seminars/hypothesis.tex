\section{Проверка статистических гипотез}

В задачах выше мы предполагаем, что семейство распределений, откуда в теории могла прийти выборка, нам известно. Спрашивается: с какого перепугу мы его знаем? Да, можно до него догадаться, например, методом пристального взгляда (<<О, на гистограмме данные образуют красивый холмик, значит, это что-то нормальное>>), но даже в таком случае у нас может возникнуть множество потенциальных кандидатов на роль семейства распределений (чаще всего мы будем рассматривать только два), и среди них надо откинуть самые бесперспективные. Отсюда хотелось бы иметь теоретически обоснованную возможность отвергать какие-либо предположения о распределении, которому подчиняется выборка. Распишем более формально.

\begin{definition}
\textit{Статистической гипотезой} $H$ называют предположение о принадлежности истинного распределения $\pth[]$ некоторому классу $\mathcal{P}$. Обозначается как $H\colon \pth[] \in \mathcal{P}$.
\end{definition}

Предположим, что истинное распределение данных лежит в некотором семействе распределений $\mathcal{P}$, в котором имеются два непересекающихся подмножества $\mathcal{P}_0$ и $\mathcal{P}_1$ -- это и есть наши догадки. Мы подвергаем сомнению, что имеет место принадлежность к классу $\mathcal{P}_0$, и в качестве противовеса берём класс $\mathcal{P}_1$.

\begin{definition}
В таком случае гипотеза $H_0\colon \pth[] \in \mathcal{P}_0$ называется \textit{основной гипотезой}, а гипотеза $H_1\colon \pth[] \in \mathcal{P}_1$ -- \textit{альтернативой}. Обозначается как
\[
H_0\colon \pth[] \in \mathcal{P}_0 \vs H_1\colon \pth[] \in \mathcal{P}_1.
\]
\end{definition}

Часто приведённые классы распределений задаются некоторыми параметрами:
$\mathcal{P} = \{\pth\colon \theta \in \Theta\}$. В таком случае гипотезу можно сформулировать с терминах принадлежности некоторым подмножествам $\Theta_0, \Theta_1 \subset \Theta$:
\[
H_0\colon \theta \in \Theta_0 \vs H_1\colon \theta \in \Theta_1.
\]

Так как принятие решения о том, отвергать ли $H_0$ или нет, зависит только от реализации выборки $X$, то критерий выбора можно задать некоторым измеримым множеством $R \subset \mathcal{X}$:
\[
\begin{aligned}
X \in R &\Longrightarrow \text{ отвергаем } H_0\\
X \notin R &\Longrightarrow \text{ не отвергаем } H_0.
\end{aligned}
\]

\begin{definition}
Множество $R$, попадание в которое равносильно отвержению основной гипотезы, называется \textit{критическим} или \textit{критерием}.
\end{definition}

\begin{remark}
Важно подчеркнуть, что <<не отвергаем>> и <<безоговорочно принимаем>> $H_0$ -- разные вещи. Если мы не смогли найти весомый довод против основной гипотезы, то это вовсе не значит, что она верна. Возможно, это не так, но из-за каких-то причин (плохой критерий, неудачная выборка и т. д.) мы не смогли её отвергнуть. Надо помнить, что \textit{наша ключевая цель -- найти весомые косвенные доказательства её неверности в пользу $H_1$}, а если таковых не нашлось, то мы либо принимаем гипотезу на веру (куда деваться?), либо подбираем другие критерии в надежде её опровергнуть. Удачное сравнение можно встретить в \cite{wasserman}:
\begin{quote}
    Hypothesis testing is like a legal trial. We assume someone is innocent unless the evidence strongly suggests that he is guilty. Similarly, we retain $H_0$ unless there is strong evidence to reject $H_0$.
\end{quote}
Хотя формально постановка задачи симметрична, часто мы подразумеваем неравнозначность гипотез. Это можно проиллюстрировать следующим хрестоматийным примером.
\end{remark}

\begin{example*}
Предположим, что мы работаем в госпитале и проводим анализы на присутствие в организме раковых клеток. По сути, мы по реализации выборки из различных показателей (кровь, рентген, МРТ и т. д.) должны проверить гипотезу $H_0\colon\textit{пациент болен раком}$ против альтернативы $H_1\colon\textit{пациент здоров}$. Если мы верно поставили диагноз, то всё ок. Иначе мы можем совершить одну из двух ошибок:

\begin{center}
    \begin{tabular}{|c|c|c|}
\hline
 & Принимаем $H_0$ & Отвергаем $H_0$\\
\hline
$H_0$ верна& Мы молодцы! & Ошибка I рода\\
\hline
$H_1$ верна& Ошибка II рода & Мы молодцы!\\
\hline
\end{tabular}
\end{center}

В случае \textit{ошибки I рода} мы не окажем помощь больному человеку и обречём его на смерть, а в случае \textit{ошибки II рода} мы начнём лечить здорового, и потеряем много денег. Обе ситуации неприятны, но с точки зрения морали первая куда хуже. Выбор гипотезы о том, что пациент болен, в качестве основной, а не наоборот, согласуется со сказанным в замечании: мы стараемся найти действительно убедительные свидетельства того, что пациент здоров (то есть неверна $H_0$), ибо в случае беспочвенного опровержения верной гипотезы мы буквально похороним пациента, и если таковых нет, то мы (может и с некоторым скепсисом) примем её. 

Можно привести и такой пример: как известно, законы Ньютона не являются исчерпывающим описанием Вселенной и не работают корректно как в макро-, так и в микромире, то есть гипотеза $H\colon\textit{Выполняются законы Ньютона }$ неверна, при этом её часто принимают на веру. Это происходит не из-за того, что физики такие глупые (хотя...), а так как она вполне себе допустима для несложных физических моделей. Так и в общем случае: если гипотеза достаточно хорошо описывает происходящее, то её можно принять, даже несмотря на то, что в действительности она неверна. Таким образом, ошибка II рода не так опасна, в отличие от I рода, когда мы отвергаем верную гипотезу и больше к ней не возвращаемся.
\end{example*}

Как же понять, когда критерий хороший, а когда не очень? Полезной можно найти следующую характеристику:
\begin{definition}
\textit{Функцией мощности критерия $R$} называется функция $$\beta(\pth[], R) = \pth[](X \in R).$$
\end{definition}

Понятно, что в случае верности основной гипотезы $H_0$ (то есть когда $\pth[] \in \mathcal{P}_0$) вероятность попадания в критическое множество должна быть низкой, а если верна $H_1$ -- как можно больше. Возникает вопрос -- как минимизировать одно и максимизировать другое? В контексте примера выше более верным представляется следующий подход: сначала надо поставить некое маленькое заранее оговоренное ограничение сверху на функцию мощности для $\pth[] \in \mathcal{P}_0$, чтобы вероятность ошибки I рода была меньше фиксированного числа. В связи с этим важным является следующее
\begin{definition}
\textit{Размером критерия R} называется
\[
\sup_{\pth[] \in \mathcal{P}_0} \beta(\pth[], R).
\]
Говорят, что критерий $R$ \textit{имеет уровень значимости $\alpha$}, если его размер не превышает $\alpha$.
\end{definition}

Отныне мы работаем с критериями, у которых можно явно задать уровень значимости $\alpha$ (например, часто берут $0.05$). Среди таковых надо подобрать критерий с как можно меньшей ошибкой II рода, то есть максимизировать вероятность попадания в $R$ при верности $H_1$. Тут, как это было при сравнении оценок, возникает проблема сравнения двух функций (как понять, какая лучше?). Возможное решение аналогично:
\begin{definition}
Говорят, что \textit{критерий $R_1$ мощнее критерий $R_2$}, если $\forall \pth[] \in \mathcal{P}_1\colon \beta(\pth[], R_1) \ge \beta(\pth[], R_2)$. Критерий $R$ называется \textit{равномерно наиболее мощным (или сокращённо р. м. н. к.) уровня значимости $\alpha$}, если он мощнее любого другого критерия уровня значимости $\alpha$.
\end{definition}

Также бывают полезным проверять потенциальный критерий на наличие следующих естественных свойств.
\begin{definition}
Критерий $R$ для проверки
\[
H_0\colon \pth[] \in \mathcal{P}_0 \vs H_1\colon \pth[] \in \mathcal{P}_1
\]
называется \textit{несмещённым}, если
\[
\sup_{\pth[] \in \mathcal{P}_0} \beta(\pth[], R) \le \inf_{\pth[] \in \mathcal{P}_1} \beta(\pth[], R).
\]
Последовательность критериев $R_n$ для выборки $X=(X_1, \ldots, X_n)$ называется \textit{состоятельной}, если $\forall \pth[] \in \mathcal{P}_1\colon \beta(\pth[], R_n) \to 1$ при $n \to \infty$ (то есть ошибка II рода постепенно исчезает).
\end{definition}

\begin{example*}
Рассмотрим модель сдвига $X_i \sim \mathcal{N}(\theta, 1)$. Предположим, в наших расчётах удобно полагать $\theta = \theta_0$, но нам хотелось бы убедиться, что это допущение состоятельно по сравнению с альтернативой $\theta > \theta_0$. Таким образом, проверим гипотезу
\[
H_0 \colon \theta = \theta_0 \vs H_1\colon \theta > \theta_0.
\]
Логично использовать критерий, основанный на статистике $T(X) = \overline{X}$ (тем более она достаточна, что как бы оправдывает такой выбор), а именно: если мы попадаем в множество $R=\{\mathbf{x}\colon T(\mathbf{x}) \ge c\}$ для некоторого $c$, то среднее слишком велико, и скорее всего предположение $H_0$ неверно, а иначе оно вполне допустимо. Подберём число $c$ так, чтобы наш критерий имел уровень значимости $\alpha$, то есть
\[
\alpha = \pth[\theta_0](\overline{X} \ge c) = \pth[\theta_0](\underbrace{\sqrt{n}(\overline{X} - \theta_0)}_{\sim \mathcal{N}(0, 1)} \ge \sqrt{n}(c - \theta_0)) \Longrightarrow \sqrt{n}(c - \theta_0) = x_{1-\alpha},
\]
где $x_p$ -- $p$-квантиль для $\mathcal{N}(0, 1)$. Таким образом, $c = \theta_0 + x_{1-\alpha}/\sqrt{n}$ доставляет нам критерий с требуемым уровнем значимости. Посмотрим, как выглядит функция мощности для $\theta > \theta_0$:
\[
\beta(\theta) = \pth\left(\overline{X} \ge c\right) = \pth\left(\sqrt{n}(\overline{X} - \theta) \ge \sqrt{n}(\theta_0 - \theta) + x_{1-\alpha}\right) = 1 - \Phi\left(\sqrt{n}(\theta_0 - \theta) + x_{1-\alpha}\right) \eqcirc
\]
где $\Phi$ -- функция распределения $\mathcal{N}(0, 1)$. Из её симметричности имеем
\[
\eqcirc \,\Phi\left(\sqrt{n}(\theta - \theta_0) - x_{1-\alpha}\right).
\]
В силу возрастания $\Phi$ функция мощности $\beta(\theta)$ будет также возрастать, поэтому $\forall \theta > \theta_0\colon \beta(\theta) \ge \alpha$, и критерий $R$ будет несмещённым. Также при $n \to \infty$ аргумент функции $\Phi$ стремится к $+\infty$, поэтому $\forall \theta > \theta_0\colon \beta(\theta) \to 1$, а значит, критерий ещё и состоятелен.
\end{example*}

\subsection{Простые гипотезы}

Как вы может быть заметили, в последнем примере мы не проверяли критерий на предмет р. м. н. к., потому что \sout{мне лень} это довольно сложно сделать на практике. Чаще всего р. м. н. к. просто не существует. Но для игрушечных гипотез такой можно явно предъявить.

\begin{definition}
Гипотеза $H\colon \pth[] \in \mathcal{P}$ называется \textit{простой}, если множество предполагаемых распределений состоит из единственного кандидата: $\mathcal{P} = \{\pth[]\}$. Иначе она называется \textit{сложной}.
\end{definition}

Предположим, нам надо столкнуть лбами две простые гипотезы:
\[
H_0\colon \pth[] = \pth[0] \vs H_1\colon \pth[] = \pth[1],
\]
причём оба кандидата $\pth[0]$ и  $\pth[1]$ абсолютно непрерывны относительно некоторой меры $\mu$ и имеют по ней плотности $p_0(t)$ и $p_1(t)$ соответственно.

\begin{lemma*}[Нейман-Пирсон]
Рассмотрим критерий $R_{\lambda} = \{x\in \mathcal{X}\colon p_1(x) - \lambda p_0(x) \ge 0\}$, где $\lambda > 0$. Если он обладает минимальным уровнем значимости $\alpha$, то есть $\pth[\theta_0](X \in R_{\lambda}) = \alpha$, то он является несмещённым р. м. н. к. уровня значимости $\alpha$.
\end{lemma*}

\begin{problem}
Пусть $X_1, \ldots, X_n$ -- выборка из распределения $\mathcal{N}(0, \sigma^2)$. Постройте р.н.м.к. уровня значимости $\alpha$ для проверки гипотезы $H_0\colon \sigma^2 = \sigma^2_0$ против альтернативы $H_1\colon \sigma^2 = \sigma^2_1$.
\end{problem}

\begin{solution}
По лемме выше для подходящего $\lambda$ критерий
\[
R_{\lambda} = \left\{\mathbf{x}\in\R^n\colon \frac{\rho_1(\mathbf{x})}{\rho_0(\mathbf{x})} \ge \lambda\right\} = \left\{\mathbf{x}\colon \left(\frac{\sigma^2_0}{\sigma^2_1}\right)^{n/2}\cdot \exp{\left[-\frac{1}{2}\left(\frac{1}{\sigma^2_1}-\frac{1}{\sigma^2_0}\right)\sum x_i^2\right]} \ge \lambda\right\}
\]
будет удовлетворять условию. Осталось сделать так, чтобы размер критерия был в точности равен $\alpha$. Без потери общности скажем, что $\sigma^2_0 > \sigma^2_1$. Тогда 
\begin{gather*}
    R_{\lambda} = \left\{\mathbf{x}\in\R^n\colon \exp{\left[-\frac{1}{2}\left(\frac{1}{\sigma^2_1}-\frac{1}{\sigma^2_0}\right)\sum x_i^2\right]} \ge \lambda\left(\frac{\sigma^2_1}{\sigma^2_0}\right)^{n/2}\right\} = \\
    = \left\{\mathbf{x}\colon -\frac{1}{2}\left(\frac{1}{\sigma^2_1}-\frac{1}{\sigma^2_0}\right)\sum x_i^2 \ge \ln{\lambda} + \frac{n}{2}\ln\frac{\sigma^2_1}{\sigma^2_0}\right\} = 
    \left\{\mathbf{x}\colon \sum x_i^2 \le \frac{\sigma_0^2\sigma^2_1}{\sigma_1^2 - \sigma^2_0}\left(2\ln{\lambda} + n\ln\frac{\sigma^2_1}{\sigma^2_0}\right)\right\}.
\end{gather*}
В силу независимости элементов выборки $\sum X_i^2 \sim \chi^2_n$, поэтому $\lambda$ и $\alpha$ связывает следующее соотношение:
\[
\frac{\sigma_0^2\sigma^2_1}{\sigma_1^2 - \sigma^2_0}\left(2\ln{\lambda} + n\ln\frac{\sigma^2_1}{\sigma^2_0}\right) = x_{\alpha},
\]
где $x_p$ -- $p$-квантиль соответствующего распределения. Отсюда
\[
\lambda = \left(\frac{\sigma^2_0}{\sigma^2_1}\right)^{n/2}\cdot \exp{\left[-\frac{1}{2}\left(\frac{1}{\sigma^2_1}-\frac{1}{\sigma^2_0}\right)x_{\alpha}\right]}.
\]
В случае $\sigma^2_0 < \sigma^2_1$ формула останется прежней за исключением замены $x_{\alpha}$ на $x_{1-\alpha}$ (подумайте, почему).
\end{solution}

\begin{problem}
Пусть $X_1$ -- выборка размера 1. Рассмотрим гипотезы
\[
H_0\colon X_1 \sim U(0; 1) \vs H_1\colon X_1 \sim Exp(1).
\]
Постройте р.н.м.к. для проверки $H_0$ против $H_1$ и вычислите его мощность.
\end{problem}

\begin{solution}
Параметра в этой модели на прямую не дали, но это не значит, что мы не может применить лемму Неймана-Пирсона. Из монотонности $\rho_1(t) = e^{-t}$ (см. рис. ниже)
\begin{center}
\begin{asy}
import graph;
size(250,0);

real f(real x) {return exp(-x);}
real g(real x) {return 0.6;}
pair F(real x) {return (x,f(x));}


draw(graph(f, 0, 3.8, operator ..),green);
draw(graph(g, 0, 1, operator ..),blue);
path line=(1,0)--(1,0.6);
draw(line,dashed);
real t=-log(0.6);
draw((t,0)--(t,0.6),dashed);

yaxis("$y$",EndArrow, ymax=1.5);
xaxis("$x$",EndArrow, xmax=4);
label("$\lambda$", (0, 0.6), W);
label("$0$", (0, 0), SW);
label("$1$", (1, 0), S);
label("$c$", (t, 0), NE);
label("$\rho_1(x)$", F(1.5), NE, green);
label("$\lambda\rho_0(x)$", (0.8, 0.6), NE, blue);

draw((-0.15,0)--(t,0), 2bp+red);
draw((1,0)--(3.85,0), 2bp+red);

draw((2, -0.5)--(0.3,-0.1),EndArrow);
draw((2, -0.5)--(2.425,-0.1),EndArrow);
label("$R_{\lambda}$", (2, -0.5), S, red);
\end{asy}
\end{center}
легко понять, что р.м.н.к. здесь будет
\[
R_{\lambda} = \{x \in \R\colon \rho_1(x) \ge \lambda\rho_0(x)\} = (-\infty; c] \cup [1; +\infty],
\]
где $c$ удовлетворяет равенствам $\lambda = e^{-c}$ и $\alpha = \pth[0](R_{\lambda}) = c$, то есть $\lambda  = e^{-\alpha}$. Отсюда также несложно посчитать мощность нашего критерия:
\[
\beta(R_{\lambda}) = \pth[1](R_{\lambda}) = 1 - \int_c^1 \rho_1(t)\,dt = 1 +\left. e^{-t}\right|_c^1 = 1 + e^{-1} - e^{-\alpha}.
\]
\end{solution}


\subsection{Сложные гипотезы}

Как и ранее, будет будем предполагать, что все потенциальные распределения $\pth$ (как из $\mathcal{P}_0$, так и из $\mathcal{P}_1$) имеют плотность $\rho_{\theta}$ относительно некоторой меры. Среди всевозможных сложных гипотез рассмотрим те, в которых мы делим множество параметров в виде прямой на два луча, но сначала нам надо ввести следующее

\begin{definition}
Говорят, что семейство $\{\pth\colon \theta \in \Theta\}$ обладает \textit{монотонным отношением правдоподобия по статистике $T(\mathbf{x})$}, если для всех $\theta_0$ и $\theta_1$ из $\Theta$ таких, что $\theta_0 < \theta_1$, функция $\dfrac{\rho_{\theta_1}(\mathbf{x})}{\rho_{\theta_0}(\mathbf{x})}$ является монотонной по $T(\mathbf{x})$ с одним и тем же типом монотонности (для уточнения этот тип монотонности добавляют в название, например, неубывающее/невозрастающее отношение правдоподобия).
\end{definition}

\begin{theorem*}[о монотонном отношении правдоподобия]
Пусть $\{\pth\colon \theta \in \Theta\}$ -- семейство с неубывающим отношением правдоподобия по статистике $T(\mathbf{x})$. Поставим проблему проверки
\[
H_0\colon \theta \le \theta_0 \vs H_1\colon \theta > \theta.
\]
Если существует некоторое $c$ такое, что $\pth[\theta_0](T(X) \ge c) = \alpha$, то критерий $R = \{\mathbf{x}\colon T(\mathbf{x}) \ge c\}$ является р. м. н. к. с уровнем значимости $\alpha$.
\end{theorem*}
\begin{remark}
В условии теоремы основную гипотезу можно поставить и как $H_0\colon \theta = \theta_0$.
\end{remark}

\begin{problem}
Пусть $X_1, \ldots, X_n$ -- выборка из распределения $Exp(\lambda)$. Постройте р.н.м.к. уровня значимости $\alpha$ для проверки гипотезы $H_0\colon \lambda = \lambda_0$ против альтернативы \textbf{(а)} $H_1\colon \lambda < \lambda_0$; \textbf{(б)} $H_1\colon \lambda > \lambda_0$.
\end{problem}

\begin{solution}
\textbf{(a)} Сведём задачу к теореме выше введением иного параметра $\nu := -\lambda$. Тогда $\rho_{\nu}(t)=-\nu e^{\nu t}$, и гипотезы перепишутся как
\[
H_0\colon \nu = \nu_0 \vs H_1\colon \nu > \nu_0.
\]
Рассмотрим отношение совместных плотностей для $\nu_2 > \nu_1$:
\[
\frac{\rho_{\nu_2}(\mathbf{x})}{\rho_{\nu_1}(\mathbf{x})} = 
\left(\frac{-\nu_2}{-\nu_1}\right)^n \exp{\left[(\nu_2 - \nu_1)\sum x_i\right]},
\]
что есть возрастающая функция от $\sum x_i$, то есть новое семейство распределений обладает неубывающим отношением правдоподобия. Из независимости $X_i$ получаем, что если $H_0$ верна, то $\sum X_i \sim \Gamma(n, -\nu_0)=\Gamma(n, \lambda_0)$. Тогда положив $c = z_{1 - \alpha}$, где $z_p$ -- $p$-квантиль $\Gamma(n, \lambda_0)$, по теореме о монотонном отношении правдоподобия критерий $R = \left\{\mathbf{x}\colon\sum x_i \ge c\right\}$ будет р.н.м.к.

\textbf{(б)} Теперь уже альтернатива выглядит ровно как в теореме выше, только теперь для $\lambda_2 > \lambda_1$
\[
\frac{\rho_{\lambda_2}(\mathbf{x})}{\rho_{\lambda_1}(\mathbf{x})} = 
\left(\frac{\lambda_2}{\lambda_1}\right)^n \exp{\left[(\lambda_1 - \lambda_2)\sum x_i\right]},
\]
и теперь семейство распределений обладает невозрастающим отношением правдоподобия по $\sum x_i$. Это легко чинится, если рассмотреть отношение плотностей как функцию от $-\sum x_i$. Тогда $R = \left\{\mathbf{x}\colon \sum x_i \le c\right\}$ будет искомым критерием, где $c=z_{\alpha}$.
\end{solution}

\begin{problem}
Пусть $X_1, \ldots, X_n$ -- выборка из распределения $\mathcal{N}(\theta, 1)$. Постройте р.н.м.к. уровня значимости $\alpha$ для проверки гипотезы \textbf{(а)} $H_0 \colon \theta \ge \theta_0$ против альтернативы $H_1\colon \theta < \theta_0$; \textbf{(б)} $H_0\colon \theta \le \theta_0$ против альтернативы $H_1\colon \theta > \theta_0$.
\end{problem}

\begin{solution}
\textbf{(а)} Введём параметр $\mu := -\theta$. Тогда плотность имеет вид
\[
\rho_{\mu}(t) = \frac{1}{\sqrt{2\pi}}e^{-(t + \mu)^2/2}.
\]
Отношение совместных плотностей для $\mu_2 > \mu_1$ есть
\[
\frac{\rho_{\mu_2}(\mathbf{x})}{\rho_{\mu_1}(\mathbf{x})} = \exp{\left[\frac{1}{2}\sum (x_i + \mu_1)^2-\frac{1}{2}\sum (x_i + \mu_2)^2\right]} = \exp{\left[\frac{n}{2}(\mu_1^2 - \mu_2^2) + (\mu_1-\mu_2)\sum x_i\right]},
\]
что является убывающей по $\sum x_i$ функцией. С учётом того, что $\sum X_i \sim \mathcal{N}(n\theta_0, n)$ при верности $H_0$, требуемым критерием будет являться $R = \left\{\mathbf{x}\colon \sum x_i \le n\theta_0 + \sqrt{n}x_{\alpha}\right\}$, где $x_{p}$ -- $p$-квантиль распределения $\mathcal{N}(0, 1)$.

\textbf{(б)} Тут никаких подводных камней нет, тупо теорема о монотонном отношении правдоподобия:
\begin{gather*}
    \frac{\rho_{\theta_2}(\mathbf{x})}{\rho_{\theta_1}(\mathbf{x})} = \exp{\left[\frac{1}{2}\sum (x_i - \theta_1)^2-\frac{1}{2}\sum (x_i - \theta_2)^2\right]} =\\ =\exp{\left[\frac{n}{2}(\theta_1^2 - \theta_2^2) + (\theta_2-\theta_1)\sum x_i\right]}\text{ --- возрастает при } \theta_2 > \theta_1 \Longrightarrow\\
    R = \left\{\mathbf{x}\colon \sum x_i \ge n\theta_0 + \sqrt{n} x_{1-\alpha}\right\} \text{ -- р.н.м.к.}
\end{gather*}
\end{solution}

Выйдем из мира розовых поней, где у нас имеется р.н.м.к., и вернёмся в суровую реальность.

\begin{problem}
$X_1, \ldots, X_n$ -- выборка из распределения $Bern(\theta)$. Докажите, что не существует равномерного наиболее мощного критерия произвольного уровня значимости $\alpha$ для проверки гипотезы $H_0\colon \theta = \theta_0$ против альтернативы $H_1\colon \theta \ne \theta_0$.
\end{problem}

\begin{solution}
На самом деле, утверждение задачи не совсем правда. Например, положим $\theta_0 = 1/2$, а $\alpha < 2^{-n}$. Тогда пустой критерий будет единственным с допустимым уровнем значимости, а значит, он автоматически р.м.н.к. Избавимся от сего нелепого случая, подразумевая под задачей, что для фиксированных $\theta_0$ и $\alpha$ р.м.н.к. не существует для достаточного большого $n$.

Пусть существует р.м.н.к. $R$. Рассмотрим критерии вида $S_1 = \left\{\mathbf{x}\colon \sum x_i \ge c_1\right\}$ и $S_2 = \left\{\mathbf{x}\colon \sum x_i \le c_2\right\}$, где константы $c_1$ и $c_2$ мы подберём <<впритык>> так, чтобы они имели уровень значимости $\alpha$. Так как $R$ -- р.м.н.к., то его мощность должна быть больше мощностей этих критериев при любых $\theta \ne \theta_0$. Рассмотрим первый из них при $\theta \to 1-0$.

Выберем $\theta$ настолько близкой к единице, чтобы произвольное наблюдение с $k$ единицами из $n$ было вероятнее, чем все возможные наблюдения с меньшим количеством единиц, то есть
\[
\pth\left(X = (\underbrace{\ldots\ldots}_{k\text{ единиц}})\right) > \sum_{\mathbf{x}\colon \sum x_i < k}\pth\left(X=\mathbf{x}\right).
\]
Так сделать можно: в правой части не больше $2^n$ слагаемых с вероятностью не большей $\theta^{k-1}(1-\theta)^{n-k+1}$, а вероятность слева равна $\theta^{k}(1-\theta)^{n-k}$, то есть отношение левой части к правой не меньше $\frac{\theta}{1-\theta}\cdot 2^{-n}$, что при фиксированном $n$ можно сделать сколь угодно большим.

Спрашивается: а зачем нам это всё? Из этого следует, что критерий $R$ \textit{обязан} содержать $S_1$ как подмножество. Действительно, выберем максимальный $k$ такой, что оно не содержит какой-то вектор с $k \ge c_1$ единицами. Но тогда чтобы вероятность $R$ была больше вероятности $S_1$ при выбранном $\theta$, надо взять другие наблюдения с меньшим числом единиц, что всё равно не позволит получить нужную вероятность по выбору $\theta$ -- противоречие. Аналогично $S_2 \subset R$.

А теперь вспомним, что константы для $S_1$ и $S_2$ мы выбирали так, чтобы они тютелька в тютельку были с нужным уровнем значимости. Поэтому если мы возьмём настолько большое $n$, чтобы $\pth[\theta_0]\left(\sum X_i = k\right) < \alpha / 2$ для всех $k$, то $S_1$ и $S_2$ будут иметь уровень значимости больше $\alpha / 2$. И вправду: если бы, например, $\pth[\theta_0](S_1) \le \alpha / 2$, то $c_1$ можно было бы уменьшить на единичку, что не сильно бы увеличило уровень значимости по выбору $n$. Таким образом, так как $S_1 \cup S_2 \subset R$, то либо $S_1 \cap S_2 \ne \varnothing$, и $R$ есть все исходы и, стало быть, имеет размер 1, либо $S_1 \cap S_2 = \varnothing$, и $R$ имеет минимальный уровень значимости больше, чем $\alpha$ -- противоречие.
\end{solution}

\begin{problem}
$X_1, \ldots, X_n$ -- выборка из равномерного распределения на отрезке
$[0; \theta]$, $\theta > 0$. Постройте равномерно наиболее мощный критерий уровня значимости $\alpha$ для проверки гипотезы $H_0\colon \theta = \theta_0$ против альтернативы $H_1\colon \theta \ne \theta_0$.
\end{problem}

\begin{solution}
Очевидно, что в случае, когда $X_{(n)} > \theta_0$, гипотеза $H_0$ однозначно неверна, поэтому такие выборки следует отнести в критическое множество. Также понятно, что слишком маленькое значение $X_{(n)}$ является серьёзным доводом для отвержения гипотезы $H_0$. Итого, давайте возьмём в качестве критерия множество
\[
R = \{\mathbf{x}\colon x_{(n)} > \theta_0\} \cup \{\mathbf{x}\colon x_{(n)} \le c\},
\]
где $c$ мы подберём так, чтобы размер $R$ был в точности $\alpha$:
\[
\alpha = \pth[\theta_0](R) = \pth[\theta_0](X_{(n)} \le c) = \pth[\theta_0](X_{1} \le c)^n = \frac{c^n}{\theta_0^n} \Longrightarrow c = \theta_0 \sqrt[n]{\alpha}.
\]
Докажем, что он и будет р.м.н.к.

Для $\theta \le c$ всё очевидно: критерий полностью покрывает носитель плотности, то есть при таких $\theta$ у нас $\pth(R) = 1$, и больше уже и не сделаешь.

Возьмём $c < \theta < \theta_0$. Пусть существует критерий $S$ с большей мощностью для данной $\theta$, то есть $\pth(S) > \pth(R)$. Но тогда
\begin{gather*}
    \pth[\theta_0](S) = \int_{S} \rho_{\theta_0}(\mathbf{x})\,d\mathbf{x} = \int\limits_{S\cap\{0 \le x_i \le \theta_0\}} \frac{d\mathbf{x}}{\theta_0^n} \ge \frac{\theta^n}{\theta_0^n} \int\limits_{S\cap\{0 \le x_i \le \theta\}} \frac{d\mathbf{x}}{\theta^n} = \frac{\theta^n}{\theta_0^n} \pth(S) >\\ > \frac{\theta^n}{\theta_0^n} \pth(R) = \frac{\theta^n}{\theta_0^n} \int_{R} \rho_{\theta}(\mathbf{x})\,d\mathbf{x} = \frac{\theta^n}{\theta_0^n} \int\limits_{R \cap \{0 \le x_i \le \theta\}} \frac{d\mathbf{x}}{\theta^n} = \int\limits_{R \cap \{0 \le x_i \le \theta_0\}} \frac{d\mathbf{x}}{\theta_0^n} = \pth[\theta_0](R) = \alpha,
\end{gather*}
то есть критерий $S$ априори не может иметь требуемый уровень значимости.

Случай $\theta > \theta_0$ аналогичен предыдущему: мы захотим получить множество большей мощности по $\theta$, но это непременно приведёт к увеличению мощности по $\theta_0$, то есть размера критерия, в силу пропорциональности этих вероятностей и выбора множества $R$.
\end{solution}


