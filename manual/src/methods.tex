\section{Методы нахождения оценок}

\subsection{Метод подстановки и эмпирическая функция распределения}\label{substitute}

Идеологически большинство методов нахождения оценок устроены схожим образом и являются частными случаями \textit{метода подстановки} (англ. \textsf{plug-in estimator}). Вот его основная идея. Часто интересующий нас параметр можно выразить как функционал от неизвестного распределения. Это может быть матожидание, медиана, мода и т. д. Причём обычно такие функционалы являются \textit{достаточно хорошими} в плане непрерывности --- для близких распределений значения функционала тоже будут близки. Отсюда появляется логичное желание подменить истинное распределение в функционале тем, что мы считаем хорошим приближением этого распределения.

В качестве такого приближения можно взять так называемую \textit{эмпирическую функцию распределения}. Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка из независимых случайных величин из неизвестного распределения $\pth[]$ с функцией распределения $F(x)$. Тогда эмпирическая функция распределения, построенная по сей выборке, имеет вид
\[
F^*_n(\omega, x) = \frac{1}{n}\sum_{i=1}^n I(X_i(\omega) \le x)
\]

\begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{pic/ecdf/ecdf.pdf}
\end{wrapfigure}

Таким образом, эмпирическая функция есть случайный элемент, который по элементарному исходу выдаёт функцию, являющуюся, как несложно заметить, функцией распределения. Она представляет собой непрерывную справа кусочно-постоянную функцию со скачками в точках из выборки, а соответствующая вероятностная мера есть мера Дирака, расположенная в точках выборки, причём мера точки пропорциональна числу <<выпаданий>> этого значения.

Если нам нужно оценить функцию от параметра $\tau(\theta)$, которую можно записать как значение функционала $G$ от истинного распределения $\pth$, то оценкой по методу постановки будет случайная величина $G(\mathsf{P}_n^*)$, где $\mathsf{P}_n^*$ – эмпирическое распределение по выборке. Например, выборочную характеристику функции $g$ можно представить в виде функционала $G$ от эмпирического распределения, равного матожиданию функции $g(x)$, отчего её выбор в качестве оценки $\me g(X_i) = G(\pth)$ становится ещё более оправданным:

\begin{gather*}
    \frac{1}{n} \sum_{i=1}^n g(X_i) = \sum_{x \in \{X_1, \ldots, X_n\}} g(x) \cdot \frac{\text{\# $x$ в выборке}}{n}  =\\
    =\sum_{x \in \{X_1, \ldots, X_n\}} g(x) \cdot \mathsf{P}_n^*(\{x\}) = \int_{\R} g(x)\,\mathsf{P}_n^*(dx) = G(\mathsf{P}_n^*).
\end{gather*}

Выбор именно такой аппроксимации истинной функции распределения объясняется её поточечной сходимостью к оной. Действительно, так как случайные величины $I(X_i \le x)$ независимы, распределены одинаково и имеют конечный момент, то по УЗБЧ
\[
F^*_n(x) = \frac{1}{n}\sum_{i=1}^n I(X_i \le x) \stackrel{\text{п.н.}}{\longrightarrow} \me[] I(X_1 \le x) = \pth[](X_1 \le x) = F(x).
\]

Однако верен даже более сильный результат: сходимость почти наверное будет равномерной.

\begin{theorem}{Гливенко, Кантелли}{}
    Пусть $F^*_n(x)$ --- эмпирическая функция распределения, построенная по выборке $\mathbf X = (X_1, \ldots, X_n)$ неограниченного размера из независимых случайных величин с функцией распределения $F(x)$. Тогда
    \[
    D_n(\omega) := \sup_{x \in \R} |F^*_n(x) - F(x)|  \stackrel{\text{п.н.}}{\longrightarrow} 0, \;\;\;n \to \infty.
    \]
\end{theorem}

\subsection{Метод моментов}

Допустим, что распределение элементов выборки зависит от $k$ неизвестных параметров $\theta_1, \ldots, \theta_k$, где вектор $\theta = (\theta_1, \ldots, \theta_k)$ принадлежит некоторой области $\Theta$ в $\R^k$. Для построения оценки по методу моментов возьмём такие борелевские $g_1, \ldots, g_k\colon \R \to \R$, что $\forall {i\in\{1,\ldots, k\}}$ определено $\me g_i(X_1) = m_i(\theta)$. Предположим, что у уравнения $m(\theta) = \overline{g(\mathbf X)}$ имеется единственное решение, где $m = (m_1, \ldots, m_k)$, $g = (g_1, \ldots, g_k)$. Так как из закона больших чисел мы знаем, что $\overline{g_i(\mathbf X)}$ примерно равно $\me[]g_i(X_1)$, то логично положить решение уравнения выше за оценку параметра. Эту же оценку можно получить как оценку методом подстановки для функционала
\[
G(\pth[]) = m^{-1}\left(\int g_1(x)\,\pth[](dx), \ldots, \int g_k(x)\,\pth[](dx)\right)
\]

Для упрощения вычислений часто в качестве $g_i(t)$ берут $t^i$ (такие функции называют \textit{пробными}), и тогда соответствующая $m_i(\theta)$ называется \textit{моментом $i$-ого порядка}, откуда собственно и пошло название метода.

\begin{example}\label{moment_examples}
    Рассмотрим некоторые примеры нахождения оценок по методу моментов.

    \begin{itemize}
        \item $X_i \sim \poisd(\lambda)$. Так как $m(\lambda) = \me[\lambda] X_1 = \lambda$ -- тождественная, то в качестве оценки параметра можно взять $\overline{\mathbf X}$.

        \item $X_i \sim \geomd(p)$. В данном случае $m(p) = \me[p] X_1 = \frac{1-p}{p} = \frac{1}{p} - 1$, тогда $m^{-1}(t) = \frac{1}{t+1}$. Таким образом, оценка по методу моментов $\widehat{p} = \frac{1}{\overline{\mathbf X}+1}$.

        \item $X_i \sim \betad(\alpha, \beta)$. Тут уже надо оценивать двумерный параметр $\theta = (\alpha, \beta)$, поэтому найдём первый и второй моменты:
        \begin{gather*}
        	m_1(\alpha, \beta) = \me X_1 = \frac{\alpha}{\alpha + \beta} = x, \;\;\; m_2(\alpha, \beta) = \me X_1^2 = \va X_1 + (\me X_1)^2 = \\
        	= \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} + \frac{\alpha^2}{(\alpha + \beta)^2} = \frac{\alpha^3+\alpha^2\beta+\alpha^2+\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} = \frac{\alpha(\alpha+1)}{(\alpha + \beta)(\alpha + \beta + 1)} = y.
        \end{gather*}
        Тогда
    \begin{gather*}
    \left.
    \begin{aligned}
    \frac{x}{y} = \frac{\alpha + \beta + 1}{\alpha + 1} = 1 + \frac{\beta}{\alpha + 1},\;\;\;\frac{\beta}{\alpha+1} = \frac{x}{y} - 1 ,\;\;\;\frac{\alpha}{\beta} + \frac{1}{\beta}= \frac{y}{x - y}\\
    \frac{1}{x} = \frac{\alpha + \beta}{\alpha} = 1 + \frac{\beta}{\alpha},\;\;\; \frac{\alpha}{\beta} = \frac{x}{1 - x}
    \end{aligned}
    \right\} \Longrightarrow \\
    \frac{1}{\beta} = \frac{y}{x-y} - \frac{x}{1-x},\;\;\;
    \beta = \frac{(x-y)(1-x)}{y-x^2} ,\;\;\;\alpha = \frac{x(x-y)}{y-x^2}.
    \end{gather*}
    С учётом того, что $\overline{\mathbf X^2} - \overline{\mathbf X}^2=s^2$, оценку по методу моментов можно записать как
    \[
    \widehat{\alpha} = \frac{\overline{\mathbf X}(\overline{\mathbf X} - \overline{\mathbf X^2})}{s^2},\;\;\;\widehat{\beta} = \frac{(1 - \overline{\mathbf X})(\overline{\mathbf X} - \overline{\mathbf X^2})}{s^2}.
    \]

        \item $X_i \sim \ud(a, b)$. Тогда 
        \begin{gather*}
            m_1(a, b) = \me X_1 = \frac{a+b}{2}=x,\\
            m_2(a, b) = \me X_1^2 = \va X_1 + (\me X_1)^2 = \frac{(b-a)^2}{12} + \frac{(a+b)^2}{4} = \frac{a^2+b^2+ab}{3} = y.
        \end{gather*}
        Откуда $a+b = 2x$, $ab = a^2 + 2ab + b^2 - 3y = 4x^2 - 3y$, что есть коэффициенты квадратного уравнения с корнями $a$ и $b$. С учётом того, что $a \le b$, получаем, что $a = x - \sqrt{3y - 3x^2}$, $b = x + \sqrt{3y - 3x^2}$. Итого, получаем следующую оценку по методу моментов:
    \[
    \widehat{a} = \overline{\mathbf X} - \sqrt{3s^2},\;\;\;\widehat{b} = \overline{\mathbf X} + \sqrt{3s^2}.
    \]
    \end{itemize}
\end{example}

При всей своей простоте у оценки по методу моментов имеются замечательные свойства.

\begin{theorem}{сильная состоятельность оценки по методу моментов}{}
Пусть $m\colon \Theta \to m(\Theta)$ --- биекция, и функция $m^{-1}$ определена и непрерывна в каждой точке множества $m(\Theta)$. Также, $\me[]g_i(X_1) < \infty$ $\forall{i\in\{1,\ldots, k\}}$, $\forall{\theta \in \Theta}$. Тогда оценка по методу моментов является сильно состоятельной оценкой параметра $\theta$.
\end{theorem}

\begin{proof}
    Следует из сохранения сходимости почти наверное под действием непрерывной функцией.
\end{proof}

\begin{theorem}[label=an_for_moment]{асимптотическая нормальность оценки по методу моментов}{}
Если в условиях предыдущей теоремы функция $m^{-1}$ дифференцируема на $m(\Theta)$, и $\me[]g_i(X_1)^2 < \infty$ $\forall{i\in\{1,\ldots, k\}}$, $\forall{\theta \in \Theta}$, то оценка, полученная по методу моментов, асимптотически нормальна.
\end{theorem}

\begin{proof}
    Так как вторые моменты $g_i(X_1)$ конечны, то по многомерной ЦПТ
    \[
    \sqrt{n}
    \left(
    \begin{pmatrix}
        \overline{g_1(\mathbf X)}\\
        \ldots\\
        \overline{g_k(\mathbf X)}
    \end{pmatrix}
    -
    \begin{pmatrix}
        m_1(\theta)\\
        \ldots\\
        m_k(\theta)
    \end{pmatrix}
    \right)
    \stackrel{d_{\theta}}{\longrightarrow}
    \mathcal{N}(0, \Sigma(\theta))
    \]
    для некоторой ковариационной матрицы $\Sigma$. По следствию из теоремы \ref{delta_method} о сохранении асимптотической нормальности получаем
    \[
    \sqrt{n}(\theta^*(\mathbf X) - \theta) = \sqrt{n}
    \left(
    m^{-1}
    \begin{pmatrix}
        \overline{g_1(\mathbf X)}\\
        \ldots\\
        \overline{g_k(\mathbf X)}
    \end{pmatrix}
    -
    m^{-1}\begin{pmatrix}
        m_1(\theta)\\
        \ldots\\
        m_k(\theta)
    \end{pmatrix}
    \right)
    \stackrel{d_{\theta}}{\longrightarrow}
    \mathcal{N}\left(0, S \cdot \Sigma(\theta) \cdot S^T \right),
    \]
    где $S = d_{m(\theta)}m^{-1}$.
\end{proof}

Для упрощения вычисления асимптотической дисперсии заметим, что если $m$ есть диффеоморфизм $\Theta$ на $m(\Theta)$, то $S = d_{m(\theta)}m^{-1} = [d_{\theta}m]^{-1}$, то есть достаточно найти матрицу Якоби для функции $m$, а потом её обратить (или в одномерном случае просто поделить на квадрат производной).

\begin{example}
    Посчитаем для одной из оценок примера \ref{moment_examples} асимптотическую дисперсию, а именно для выборки $X_i \sim \geomd(p)$. По ЦПТ мы знаем, что
        \[
        \sqrt{n}\left(\overline{\mathbf X} - \frac{1-p}{p}\right) \stackrel{d_{p}}{\rightarrow} \mathcal{N}(0, \va[p]X_1) = \mathcal{N}\left(0, \frac{1-p}{p^2}\right).
        \]
        Мы получили, что $m(p) = \frac{1}{p} - 1$, поэтому $m'(p) = -\frac{1}{p^2}$. Из доказательства теоремы \ref{an_for_moment} следует, что
        \[
        \sqrt{n}\left(\frac{1}{\overline{\mathbf X}+1} - p\right) \stackrel{d_{p}}{\rightarrow} \mathcal{N}\left(0,  \left. \frac{1-p}{p^2}\right/\left(-\frac{1}{p^2}\right)^2\right) = \mathcal{N}\left(0, (1-p)p^2\right)
        \]
\end{example}

Как можно видеть, оценки по методу моментов интуитивно понятны и легки в построении. Однако обычно асимптотическая дисперсия оценок, полученных по методу моментов, довольно велика, в то время как оценки, построенные другими методами, оказываются более выигрышными. К тому же не факт, что они будут несмещёнными (хотя, как например в случае равномерного распределения выше, легко получить несмещённую оценку, используя несмещённую оценку дисперсии).

\subsection{Выборочные квантили}

Помимо недостатков, упомянутых ранее, у оценок по методу моментов (впрочем, как и у любых статистик, использующих выборочные характеристики) есть существенный минус --- они крайне чувствительны к \textit{выбросам}, то есть элементам выборки, которые сильно выбиваются из целевого распределения (они могут порождаться, например, ошибками измерения). Из-за них значение статистики может сильно поменяться, испортив результаты. В меньшей степени такой проблеме подвержены оценки, использующие порядковые статистики: даже с очень большим выбросом значение оценки поменяется незначительно (см. рис). Ещё говорят, что такие оценки \textit{робастные}, то есть устойчивы к выбросам.

\begin{figure}[h]
    \centering
    \includegraphics{pic/outlier/image-5.pdf}
\end{figure}

Самым простым примером оценки, построенной с помощью порядковых статистик, является выборочный квантиль. Для начала сформулируем определение обычного квантиля.

\begin{definition}
\textit{$p$-квантилем} распределения $\pth[]$ называется $z_p = \inf\{x\colon F_{\pth[]}(x) \ge p\}$, где $p \in (0; 1)$.
\end{definition}

Таким образом, $p$-квантиль --- некоторый функционал от распределения. Значит, его можно приблизить, взяв функционал от эмпирического распределения.

\begin{definition}
    Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка. Статистика
    \[
    z_{n,p} = \left\{
    \begin{aligned}
    &X_{([np]+1)},&np \notin \Z&,\\
    &X_{(np)}, &np \in \Z&
    \end{aligned}
    \right.
    \]
    называется \textit{выборочным квантилем}.
\end{definition}

\begin{theorem}[label=quantile]{о выборочном квантиле}{}
    Пусть $\mathbf X = (X_1,\ldots X_n)$ --- выборка из распределения $\pth[]$ с плотностью $\rho(x)$. Пусть $z_p$ --- $p$-квантиль распределения $\pth[]$, причем $\rho(x)$ непрерывна в точке $z_p$ и $\rho(z_p) > 0$. Тогда
    \[
    \sqrt{n}(z_{n, p} - z_p) \stackrel{d}{\to} \mathcal{N}\left(0, \frac{p(1-p)}{\rho^2(z_p)}\right).
    \]
\end{theorem}

Особенно часто выделяют случай, когда $p=1/2$. Такой $p$-квантиль называют \textit{медианой}, но выборочный аналог обычно определяют несколько иначе:
\begin{definition}
    \textit{Выборочной медианой} для выборки $\mathbf X = (X_1, \ldots, X_n)$ называется
    \[
    \widehat{\mu} = \left\{
    \begin{aligned}
    X_{(k+1)},\,\,\,n = 2k+1,\\
    \frac{X_{(k)}+X_{(k+1)}}{2},\,\,\, n = 2k
    \end{aligned}
    \right.
    \]
\end{definition}

Для неё также справедлива теорема выше, только $z_{n, 1/2}$ заменяется на $\widehat{\mu}$.

\begin{example}\label{quantile_example}
    Построим асимптотически нормальную оценку для параметра масштаба в модели распределения Коши:
    \[
    \rho_{\theta}(x) = \frac{\theta}{\pi(\theta^2+x^2)}.
    \]

    Метод выборочного квантиля здесь оказывается весьма полезным и простым. Заметим, что функция распределения будет равняться
    \[
    F_{\theta}(x) = \int_{-\infty}^x \frac{\theta}{\pi(\theta^2+t^2)}\,dt = \left[s = \frac{t}{\theta}\right] = \int_{-\infty}^{\frac{x}{\theta}} \frac{1}{\pi(1+s^2)}\,ds = \left.\frac{1}{\pi}\arctg{s}\right|_{-\infty}^{\frac{x}{\theta}} = \frac{1}{\pi}\arctg{\frac{x}{\theta}} + \frac{1}{2}.
    \]
    Следовательно, $\frac{3}{4}$-квантилем для данного семейства распределений будет в точности $z_{3/4} = \theta$. Тогда по теореме о выборочном квантиле $z_{n, 3/4}$ будет асимптотически нормальной оценкой параметра $\theta$:
    \[
    \sqrt{n}(z_{n, 3/4} - \theta) \stackrel{d}{\to} \mathcal{N}\left(0, \frac{3/4(1 - 3/4)}{\rho^2(\theta)}\right) = \mathcal{N}\left(0, \frac{3\pi^2\theta^2}{4}\right).
    \]
    
    Для сравнения решим задачу методом моментов. С пробными функциями он работать не будет, так как у распределения Коши нет матожидания. Поэтому рассмотрим $g(t) = \frac{1}{1+t^2}$. Для неё
    \begin{gather*}
        m(\theta) = \me g(X_1) = \int_{\R} \frac{\theta}{\pi(\theta^2+t^2)(1+t^2)}\,dt = \frac{\theta}{\pi(1-\theta^2)} \int_{\R} \left(\frac{1}{\theta^2+t^2} - \frac{1}{1+t^2}\right)\,dt = \\
        = \frac{\theta}{\pi(1-\theta^2)} \left(\frac{\pi}{\theta} - \pi\right) = \frac{1}{1 + \theta} \Longrightarrow \widehat{\theta}(\mathbf X) = m^{-1}(\mathbf X) = 1\left/\overline{\frac{1}{1+\mathbf X^2}}\right. - 1
    \end{gather*}
    Получилось весьма недурно. Но проверим, какова асимптотическая дисперсия полученной оценки:
    \[
    \sqrt{n}\left(\overline{g(\mathbf X)} - \frac{1}{1+\theta}\right) \stackrel{d_{\theta}}{\to} \mathcal{N}(0, \va g(X_1))
    \]
    Проведя несложные расчёты, получаем: $\va g(X_1) = \me g(X_1)^2 - (\me g(X_1))^2 = \frac{\theta + 2}{2(\theta + 1)^2} - \frac{1}{(\theta+1)^2} = \frac{\theta}{2(\theta + 1)^2}$. Применяя дельта-метод, приходим к тому, что
    \[
    \sqrt{n}\left(\widehat{\theta} - \theta\right) \stackrel{d_{\theta}}{\to} \mathcal{N}\left(0, \frac{\theta}{2(\theta + 1)^2} \cdot (1+\theta)^4\right) = \mathcal{N}\left(0, \frac{\theta(\theta+1)^2}{2}\right).
    \]
    Как мы видим, асимптотическая дисперсия оценки по методу моментов получилась на порядок хуже, чем через выборочный квантиль (хотя стоит признать, для маленьких значений $\theta$ она будет всё же меньше).
\end{example}

% \subsection{M-оценки}

% TODO М-оценки

% To be continued...
 
\subsection*{Задачи}

\begin{problem}
	Найдите оценку вектора параметров $(\alpha, \lambda)$ по методу моментов для выборки $\mathbf X = (X_1, \ldots, X_n)$ из $\Gamma(\alpha, \lambda)$.
\end{problem}

\begin{problem}
	Рассмотрим модель сдвига для распределения Лапласа:
\[
\rho_{\theta}(x) = \frac{1}{2}e^{-|x-\theta|},\;\;\;\theta \in \R.
\]

Постройте оценки параметра $\theta$ по выборке $\mathbf{X} = (X_1, \ldots, X_n)$ с помощью метода моментов и метода квантилей. Сравните полученные оценки по их асимптотическим дисперсиям.
\end{problem}

\begin{problem}
    В этой задаче предлагается доказать теорему \ref{quantile} об асимптотической нормальности оценки по методу квантилей.\\
    \textbf{(а)} Пусть $\xi_1, \ldots, \xi_n \sim \ud[0; 1]$. Докажите, что
    \[
    \xi_{(k)} \sim \frac{S_k}{S_{n+1}}, \;\;\;S_m = \eta_1 + \ldots + \eta_m,\;\;\; \eta_i \text{ --- н.о.р. с распределением $\expd(1)$}.
    \]
    \textbf{(б)} Докажите, что если $\alpha(n) - np = O(1)$, то
    \[
    \sqrt{n}\left(\frac{S_{\alpha(n)}}{n} - p\right) \stackrel{d}{\longrightarrow} \mathcal{N}(0, p);
    \]
    \textbf{(в)} Докажите теорему \ref{quantile} для выборки из равномерного распределения.\\
    \textbf{(г)} Докажите теорему в общем случае. \textit{Указание.} Вспомните из теорвера, как от произвольного распределения перейти к равномерному.
\end{problem}


