\section{Оценки и их свойства}

В теории вероятности мы в основном работали над созданием инструментов для описания различных распределений случайных элементов. Мы фиксировали вероятностное пространство, притворяясь, что с ним знакомы, рассматривали случайные элементы на нём и с помощью некоторого арсенала описывали поведение случайной величины. Таким образом, теория вероятности отвечает на вопрос: если случайная величина распределена именно так, то какие свойства (например, асимптотические) она имеет?

Математическая статистика делает всё с точностью до наоборот: по свойствам того, что нам позволено лицезреть, необходимо определить, из какого распределения пришла наблюдаемая величина. Часто набор распределений, которые являются кандидатами на роль истинного распределения, можно описать набором параметров, поэтому в основном нашей задачей будет определить с некоторой точностью значение параметра по реализации выборки.

\subsection{Вероятностно-статистическая модель}

Прежде всего необходимо провести некоторые косметические изменения в модели вероятностного пространства, с которым мы работали ранее. Для начала определимся с его измеримой частью, а именно с множеством элементарных исходов $\mathcal{X}$ и $\sigma$-алгеброй событий $\mathcal{F}$ на нём. Обычно в качестве первого берут непосредственно множество результатов наблюдения, а в качестве второго --- борелевскую $\sigma$-алгебру $\mathcal{B}(\mathcal{X})$ на нём, которую ещё со времён теории вероятностей мы считаем вполне естественной. В частности, это могут быть (и чаще всего и будут):
\begin{itemize}
    \item $\mathcal{X} = \R$, $\mathcal{F} = \mathcal{B}(\R)$, если наблюдение в ходе эксперимента представляет собой одну вещественную величину;
    \item $\mathcal{X} = \R^n$, $\mathcal{F} = \mathcal{B}(\R^n)$, если мы наблюдаем за несколькими величинами или целым случайным вектором. Напомним, что $\mathcal{B}(\mathcal{X})$ в данном случае можно породить, например, открытыми множествами в $\R^n$, открытыми брусами в том же пространстве или декартовыми произведениями $B_1 \times \ldots \times B_n$ борелевских множеств $B_1, \ldots, B_n \in \mathcal{B}(\R)$, в зависимости от удобства в конкретном случае;
    \item $\mathcal{X} = \R^{\infty}$, $\mathcal{F} = \mathcal{B}(\R^{\infty})$, если наблюдения представляют собой последовательность случайных величин или векторов. Конечно, на практике мы не имеем бесконечного числа экспериментов, но такое пространство удобно при изучении асимптотических свойств оценок, когда количество наблюдаемых величин сколь угодно большое. Не будем вдаваться в подробности устройства этой $\sigma$-алгебры (как мы меры на ней), но отметим, что она также порождается конечными декартовыми произведениями $B_{n_1} \times \ldots \times B_{n_k}$, где $n_i \in \N$, а $B_{n_i}$ могут быть как промежутками, так и вообще борелевскими множествами в $\R$.
\end{itemize}

Так как достоверно истинное распределение нам неизвестно, то и вероятностная мера на нашем пространстве определена не однозначно. Отсюда логично рассматривать семейство $\mathcal{P}$ вероятностных мер на измеримом пространстве на $\mathcal{B}(\mathcal{X})$. Чаще всего это семейство можно описать одним или несколькими параметрами, то есть будет иметь вид $\mathcal{P} = \{\pth\colon \theta \in \Theta\}$, где $\Theta$ --- множество допустимых значений параметров. Например, нормальное распределение $\mathcal{N}(a, \sigma^2)$ можно описать двумя параметрами: его матожиданием $a$ и дисперсией $\sigma^2$, таким образом, оно однозначно описывается параметром $(a, \sigma^2) \in \R \times \R_+$. Так как распределение теперь не фиксировано, то часто в обозначения матожидания, сходимости по вероятности и т. д., которые используют конкретное распределение, мы будем писать индекс $\theta$, чтобы подчеркнуть, какое распределение используется в данный момент.

Итого, мы построили \textit{вероятностно-статистическую модель} $(\mathcal{X}, \mathcal{B}(\mathcal{X}), \mathcal{P})$. Так как мы договорились в качестве $\mathcal{X}$ брать пространство значений наблюдаемых величин, то само \textit{наблюдение} можно задать как отображение $X\colon \mathcal{X} \to \mathcal{X}$, $X(\omega) = \omega$, то есть величина $X$ будет непосредственно показывать, какой элементарный исход был разыгран волей случая. Легко понять, что функция $X$ является измеримой, и её распределение совпадает с истинным распределением $\pth[] \in \mathcal{P}$. Также зачастую наблюдения на практике бывают независимыми (или мы слепо верим в то, что они независимы), поэтому в основном мы будем работать с семействами распределений $\mathcal{P}$ такими, при которых элементы выборки являются независимыми в совокупности случайными величинами. То есть если $\mathbf X = (X_1, X_2, \ldots)$ --- наблюдение, то для любого $\pth[] \in \mathcal{P}$ выполнено
\[
\pth[](X_1 \in B_1, \ldots, X_n \in B_n) = \prod_{i=1}^n \pth[](X_i \in B_i).
\]

Читатель может резонно заметить, не умаляется ли общность введением выборочного пространства? Ведь на практике вероятностное пространство устроено куда сложнее, чем просто набор возможных значений эксперимента. Однако такое допущение не будет влиять на выводимые нами результаты. Во-первых, выборочное пространство порождается самим экспериментом. Грубо говоря, исходное пространство, которое слишком сложное либо вообще неизвестное, содержит в себе некоторый интересующий нас кусок, который и описывает распределение величины из эксперимента. Отсюда и логично использовать для дальнейших выкладок выборочное пространство, которое непосредственно связано с наблюдением. Во-вторых, все наши дальнейшие действия будут вестись не с самими наблюдениями, а с функциями от них. То есть мы не будем работать с самими элементами пространства, которое может быть устроено иначе, а с тем, что это пространство нам выдаёт.

\begin{definition}
Пусть $(\Omega, \mathcal{E})$ --- измеримое пространство. Произвольная композиция $(\mathcal{B}(\mathcal{X})|\mathcal{E})$-измеримой функции $S\colon \mathcal{X} \to \Omega$ и наблюдения $\mathbf X$ называется \textit{статистикой} (иногда под статистикой будет иметься в виду само отображение $S$). Обозначается как $S(\mathbf X)$.
\end{definition}

\begin{example}
    Рассмотрим некоторые широко применимые примеры статистик.
    \begin{itemize}
        \item Если интересующую функцию от параметра можно записать как матожидание от некоторой борелевской функции $g$, то логично в качестве оценки брать среднее арифметическое этой функции от элементов выборки, в широком наборе случаев оно будет стремиться к истинному матожиданию по ЗБЧ. Такая статистика
        \[
        \overline{g(\mathbf X)} = \frac{1}{n}\sum_{i=1}^n g(X_i)
        \]
        называется \textit{выборочной характеристикой функции $g$}. В частности, если $g(x) = x^k$, то её выборочную характеристику
        \[
        \overline{\mathbf X^k} = \frac{1}{n}\sum_{i=1}^n X_i^k
        \]
        называют \textit{выборочным $k$-ым моментом}, для $k=1$ её обычно называют просто \textit{выборочным средним}.

        \item Мы научились приближать матожидание, а что с дисперсией? Её оценку можно получить, если усреднить отклонение наблюдений от выборочного среднего:
        \[
        s^2(\mathbf X) = \frac{1}{n}\sum_{i=1}^n \left(X_i - \overline{\mathbf X}\right)^2
        \]
        Такую статистику называют \textit{выборочной дисперсией}. Иногда её удобно перезаписать в следующем виде:
        \[
        s^2 = \sum_{i=1}^n \left( \frac{X^2_i}{n} - \frac{2 X_i \cdot \overline{\mathbf X}}{n} + \frac{\overline{\mathbf X}^2}{n} \right) = \frac{\sum X^2_i}{n} -  2\overline{\mathbf X} \cdot \frac{\sum X_i}{n} + \overline{\mathbf X}^2 = \overline{\mathbf X^2} - \overline{\mathbf X}^2.
        \]

        \item Полезно также рассмотреть \textit{$k$-ую порядковую статистику} $X_{(k)}$ из вариационного ряда
        \[
        X_{(1)} \le X_{(2)} \le \ldots \le X_{(n)},
        \]
        который получается упорядочиванием первоначальной выборки $\mathbf X = (X_1, \ldots, X_n)$. 
    \end{itemize}
\end{example}

Обратим внимание, что значения статистики по определению не зависят от параметра, он влияет лишь на её распределение. Требование весьма логичное, иначе получается странно: хотим оценить неизвестный параметр с помощью статистики, при этом она почему-то включает в себя этот параметр. Например, $X - \me X$ не является статистикой. Далее нам встретятся менее очевидные примеры, когда в определении функции используется значение параметра, но при этом как таковой зависимости от него нет.

Не все статистики подходят для оценивания неизвестного параметра, например, для оценки дисперсии логично использовать лишь неотрицательные функции. Какой бы хорошей статистикой $S(\mathbf X)$ ни была, если она периодически выдаёт недопустимое значение параметра, то как оценка она неадекватна. Поэтому логично ввести следующее

\begin{definition}
Пусть $\mathcal{P} = \{\pth\colon \theta \in \Theta\}$, а $\Omega = \Theta$, тогда статистика $S\colon \mathcal{X} \to \Omega$ называется \textit{оценкой} параметра $\theta$. Обычно их записывают с <<крышечкой>>, как $\widehat{\theta}(\mathbf X)$.
\end{definition}

Периодически нам нужно будет оценивать не сам параметр, а некую функцию от него $\tau(\theta)$. Соответственно, оценка для этой функции должна лежать в множестве $\tau(\Theta)$. Например, пусть имеются наблюдения с распределением $\mathcal{N}(\theta_1, 1)$, а также наблюдения с распределением $\mathcal{N}(\theta_2, 1)$, и мы хотим понять, различаются ли они (то есть правда ли, что $\theta_1 = \theta_2$), в таком случае логично оценивать не сами параметры, а функцию $\tau(\theta) = \theta_1 - \theta_2$.

\subsection{Основные свойства}

Чтобы понимать, какие оценки хорошие, а какие --- не очень, нужно выделить некоторые свойства оценок, которые было бы крайне желательно иметь. Многие из них звучат как <<для всех $\theta \in \Theta$ выполнено...>>: действительно, было бы обидно для одних параметров иметь свойство, а для других --- нет. Поначалу мы будем честно прописывать это, но далее всегда считаем, что рассматриваемое свойство выполнено для любого значения параметра.

Первое из них говорит, что если нам будут поступать раз за разом выборки, то оценки для них будут в среднем похожи на истинный параметр.

\begin{definition}
Оценка $\widehat{\theta}(\mathbf X)$ называется \textit{несмещённой} оценкой параметра $\tau(\theta)$, если для любого $\theta \in \Theta$ выполнено $\me \widehat{\theta} = \tau(\theta)$.
\end{definition}

\begin{example}
    Многие естественные оценки этим свойством обладают в силу линейности матожидания. Например, если $\mathcal{P} = \{\poisd(\lambda)\colon \lambda > 0\}$, то $\overline{\mathbf X}$ будет несмещённой оценкой $\lambda$, так как для любого $i$ верно $\me[\lambda] X_i = \lambda$. Впрочем, даже относительно простые оценки могут оказаться смещёнными, например, выборочная дисперсия $s^2$:
    \begin{gather*}
        \me s^2 = \me \left( \frac1n\sum X_i^2 - \frac{1}{n^2}\sum_{i, j} X_i X_j \right) = \frac{1}{n}\sum \me X_i^2 - \frac{1}{n^2} \sum \me X_i^2 - \frac{1}{n^2} \sum_{i \ne j} \me(X_i X_j) =\\
        = \me X_1^2 - \frac{1}{n} \cdot \me X_1^2 - \frac{1}{n^2} \cdot \underbrace{\sum_{i \ne j} \me X_i \cdot \me X_j}_{n^2 - n\text{ слагаемых}} = \frac{n-1}{n}\cdot\left( \me X_1^2 - (\me X_1)^2\right) = \\
        = \frac{n-1}{n} \cdot \va X_1 = \frac{n-1}{n} \cdot\sigma^2.
    \end{gather*}
    При больших $n$ это отклонение будет невелико, однако если выборка мала, множитель $\frac{n-1}{n}$ может существенно поменять оценку, поэтому часто вместо обычной выборочной дисперсии берут её несмещённый аналог $S^2 = \frac{n}{n-1}s^2$ (обозначая её большой буквой, мы как бы указываем на её превосходство над обычной выборочной дисперсией).
\end{example}

Это свойство весьма логичное, но его очевидно недостаточно, чтобы утверждать, что оценка хоть сколь-нибудь пригодна. Например, если $\me X_1 = \theta$, то $X_1$ -- несмещённая оценка параметра $\theta$, хотя она не использует всю мощь выборки. Это подводит нас к асимптотическим свойствам оценок: нам бы хотелось, чтобы с ростом размера выборки увеличилась бы и точность в предсказании параметра.
\begin{definition}
Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка. Оценка $\theta^*_n = \theta^*_n(\mathbf X)$ называется \textit{состоятельной} оценкой параметра $\tau(\theta)$, если для любого $\theta \in \Theta$ выполнено $\theta^*_n \stackrel{\pth}{\to} \theta$. Оценка $\theta^*_n$ называется \textit{сильно состоятельной} оценкой параметра $\tau(\theta)$, если для любого $\theta \in \Theta$ выполнено $\theta^*_n \stackrel{\pth\text{-п.н.}}{\longrightarrow} \theta$.
\end{definition}

Вообще под (сильно) состоятельной оценкой подразумевают последовательность оценок, но обычно все и так понимают, о чём речь. Если из контекста понятно, как именно оценка зависит от параметра $n$, то нижний индекс убирают.

Немаловажную роль играет и то, с какой скоростью оценка стремится к истинному значению параметра. Сходимость, конечно, хорошее свойство, но оно не говорит нам о том, как близко к параметру мы находимся и сколько нужно элементов выборки для достаточно точного приближения. 

\begin{definition}
Оценка $\theta^*_n$ называется \textit{асимптотически нормальной} оценкой параметра $\tau(\theta)$, если для любого $\theta \in \Theta$
\begin{equation}\label{asympt_norm}
    \sqrt{n}(\theta^*_n - \tau(\theta)) \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}(0, \sigma^2(\theta)).
\end{equation}
Величина $\sigma^2(\theta)$ называется \textit{асимптотической дисперсией} (в многомерном варианте она представляет собой ковариационную матрицу).
\end{definition}

Такая форма сходимости выбрана неслучайно: многие оценки будут асимптотическими нормальными по ЦПТ. Например, если $X_1, \ldots, X_n$ имеют конечные дисперсии, то оценка $\overline{\mathbf X}$ функции $\me X_1$ будет асимптотически нормальна, так как
\[
\sqrt{n}(\overline{\mathbf X} - \me X_1) = \frac{X_1 + \ldots + X_n - n\me X_1}{\sqrt{n}} \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}(0, \va X_1).
\]

Также важен предельный закон в виде нормального распределения. Как известно, оно имеет очень <<лёгкие>> хвосты, и поэтому основная вероятностная масса сосредоточена вблизи матожидания. Такой результат ещё называют <<правилом трёх сигм>>: с вероятностью $>0.997$ величина $\xi \sim \mathcal{N}(0, \sigma^2)$ принимает значения из интервала $(-3\sigma; 3\sigma)$, поэтому можно считать, что  асимптотически нормальная оценка стремится к истинному значению параметра со скоростью порядка $\sigma(\theta)/\sqrt{n}$. Это также поясняет важность понятия асимптотической дисперсии: чем она меньше, тем точнее будет результат приближения, на таком соображении основан принцип сравнения оценок, описанный в разделе \ref{compare_methods}.

\begin{wrapfigure}[10]{l}{0.4\textwidth}
\includegraphics[width=0.4\textwidth]{pic/asympt_norm/asympt_norm.pdf}
\end{wrapfigure}

Конечно, оценки могут стремиться к параметру и с большей скоростью (см. пример \ref{example_of_properties}), и тогда величина в левой части \eqref{asympt_norm} стремится к нулевому распределению. В \textcolor{red}{н}\textcolor{orange}{е}\textcolor{yellow}{к}\textcolor{green}{о}\textcolor{blue}{т}\textcolor{purple}{о}\textcolor{red}{р}\textcolor{orange}{ы}\textcolor{yellow}{х} \textcolor{green}{и}\textcolor{blue}{с}\textcolor{purple}{т}\textcolor{red}{о}\textcolor{orange}{ч}\textcolor{yellow}{н}\textcolor{green}{и}\textcolor{blue}{к}\textcolor{purple}{а}\textcolor{red}{х} такие оценки не считают асимптотически нормальными, так как их интересует нетривиальный предельный закон. Однако мы так делать не будем, ведь нам важна оценка на скорость сходимости, тем более мы считаем константу нормально распределённой с дисперсией нуль.

\begin{proposition}\label{relation_between_conv}
    Состоятельность оценки $\widehat{\theta}(\mathbf X)$ следует из её сильной состоятельности или асимптотической нормальности.
\end{proposition}

\begin{proof}
    Первый факт очевиден, так как из сходимости почти всюду следует сходимость по вероятности. Докажем второе утверждение.

    С одной стороны, по условию $$\sqrt{n}\left(\widehat{\theta}(\mathbf X)-\theta\right) \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}(0, \sigma^2(\theta)).$$
    С другой, очевидно выполняется $1 / \sqrt{n} \stackrel{d_{\theta}}{\longrightarrow} 0$. Тогда по лемме Слуцкого $\widehat{\theta}(\mathbf X) - \theta \stackrel{d_{\theta}}{\longrightarrow} 0$. Из сходимости по распределению к константе следует сходимость к ней по вероятности, значит, $\widehat{\theta}(\mathbf X)
    \stackrel{\pth}{\longrightarrow} \theta$.
\end{proof}

\begin{example}\label{example_of_properties}
    Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка из $U(0, \theta)$. Проверим на несмещённость, состоятельность, сильную состоятельность и асимптотическую нормальность следующие оценки параметра $\theta$: $\widehat{\theta}(\mathbf X) = \overline{\mathbf X} + X_{(n)}/2$, $\theta^*(\mathbf X) = (n + 1)X_{(1)}$ и $\widetilde{\theta}(\mathbf X) = X_{(1)} + X_{(n)}$.

    Для начала поймём, как распределены первая и последняя порядковые статистики. Для $t \in (0; 1)$:
    \begin{gather*}
        \pth(X_{(n)} \le t) = \pth(X_1, \ldots, X_n \le t) = \prod \pth(X_i \le t) = \pth(X_1 \le t)^n = \frac{t^n}{\theta^n};\\
        \rho_{X_{(n)}}(t) = \frac{nt^{n-1}}{\theta^n}I(0 < t < \theta).
    \end{gather*}
    \begin{gather*}
        \pth(X_{(1)} \le t) = 1 - \pth(X_{(1)} > t) = 1 - \pth(X_1, \ldots, X_n > t) = 1 - \prod \pth(X_i > t) =\\
        = 1 - (1 - \pth(X_1 \le t))^n = 1 - \left(1 - \frac{t}{\theta}\right)^n;\;\;\; \rho_{X_{(1)}}(t) = \frac{n}{\theta}\left(1 - \frac{t}{\theta}\right)^{n-1} I(0 < t < \theta).
    \end{gather*}
    Также полезным для проверки на несмещённость будут их матожидания:
    \begin{gather*}
        \me X_{(n)} = \int_0^{\theta} t \cdot \frac{nt^{n-1}}{\theta^n}\,dt = \frac{n}{n+1} \cdot \theta, \;\;\; \me X_{(1)} = \int_0^{\theta} t \cdot \frac{n}{\theta}\left(1 - \frac{t}{\theta}\right)^{n-1}\,dt = \\
        = n\theta \int_0^1 s(1-s)^{n-1}\,ds = n\theta \cdot B(2, n) = n\theta \cdot \frac{\Gamma(2)\Gamma(n)}{\Gamma(n+2)} = \frac{\theta}{n+1}.
    \end{gather*}
    Теперь мы готовы к решению задачи.
    
    \textit{Несмещённость.} Из линейности матожидания легко видеть, что несмещёнными будут $\theta^*(\mathbf X)$ и $\widetilde{\theta}(\mathbf X)$, а вот $\me \widehat{\theta}(\mathbf X) = \theta / 2 + \frac{n\theta}{2(n+1)} \ne \theta$, поэтому $\widehat{\theta}(\mathbf X)$ имеет небольшое, но всё-таки смещение.
    
    \textit{Состоятельность.} $\overline{\mathbf X} \stackrel{\pth\text{-п.н.}}{\longrightarrow} \theta/2$ из УЗБЧ. Для произвольного $0 < \epsilon < \theta$ (для $\epsilon > \theta$ всё ясно):
    \[
    \pth(|X_{(n)} - \theta|>\epsilon) = \underbrace{\pth(X_{(n)} > \theta + \epsilon)}_{\phantom{1}=0} + \pth(X_{(n)} < \theta - \epsilon) = \frac{(\theta - \epsilon)^n}{\theta^n} \to 0.
    \]
    Что же насчёт первой порядковой статистики, то
    \[
    \pth(|(n+1)X_{(1)} - \theta|>\epsilon) \ge \pth((n+1)X_{(1)} > \theta + \epsilon) = \left(1 - \frac{\theta + \epsilon}{\theta(n+1)}\right)^n \to e^{-\frac{\theta+\epsilon}{\theta}} \ne 0
    \]
    С другой стороны,
    \[
    \pth(|X_{(1)}|>\epsilon) = \pth(X_{(1)} > \epsilon) = \left(1 - \frac{\epsilon}{\theta}\right)^n \to 0.
    \]
    Таким образом, $X_{(n)} \stackrel{\pth}{\rightarrow} \theta$, $X_{(1)} \stackrel{\pth}{\rightarrow} 0$, но $(n+1)X_{(1)} \stackrel{\pth}{\nrightarrow} \theta$. Из всего этого получаем, что $\widehat{\theta}(\mathbf X)$ будет состоятельной (сходимости по вероятности можно складывать), $\theta^*(\mathbf X)$ не является состоятельной оценкой $\theta$, а вот $\widetilde{\theta}(\mathbf X)$ уже будет являться как сумма $X_{(n)}$, стремящейся по вероятности к $\theta$, и $X_{(1)}$, стремящейся по вероятности к нулю.
    
    \textit{Сильная состоятельность.} Тут всё куда проще, ведь при фиксированной выборке что $X_{(n)}$, что $X_{(1)}$ --- монотонны при увеличении $n$, а это значит, что из их сходимости по вероятности будет следовать сходимость $\pth$-п.н. Действительно, как известно из курса теории вероятностей, у последовательности, сходящейся по вероятности, есть подпоследовательность, сходящаяся почти наверное. Тогда из монотонности следует, что и вся последовательность такая. Отсюда, $X_{(n)} \stackrel{\pth\text{-п.н.}}{\longrightarrow} \theta$, $X_{(1)} \stackrel{\pth\text{-п.н.}}{\longrightarrow} 0$, поэтому оценки $\widehat{\theta}(\mathbf X)$ и $\widetilde{\theta}(\mathbf X)$ будут сильно состоятельными. $\theta^*(\mathbf X)$ же таковой не является, так как она даже не состоятельна.
    
    \textit{Асимптотическая нормальность.} Аналогично предыдущему пункту и по утверждению \ref{relation_between_conv} оценка $\theta^*(\mathbf X)$ не асимптотически нормальна. Проверим, как себя ведут порядковые статистики:
    \begin{gather*}
        \pth(\sqrt{n}(X_{(n)} - \theta) \le t) = \pth\left(X_{(n)} \le \theta + \frac{t}{\sqrt{n}}\right) = 
        \left\{
        \begin{aligned}
        &1,\,\,\,t \ge 0;\\
        &\left(1 + \frac{t}{\theta\sqrt{n}}\right)^n \to 0,\,\,\,t<0.
        \end{aligned}
        \right.\\
        \pth(\sqrt{n}X_{(1)} \le t) = \pth\left(X_{(1)} \le \frac{t}{\sqrt{n}}\right) = 
        \left\{
        \begin{aligned}
        &0,\,\,\,t < 0;\\
        & 1 - \left(1 - \frac{t}{\theta\sqrt{n}}\right)^n \to 1,\,\,\,t>0.
        \end{aligned}
        \right.
    \end{gather*}
    Стало быть, $\sqrt{n}(X_{(n)} - \theta), \sqrt{n}X_{(1)} \stackrel{d_{\theta}}{\longrightarrow} 0 \sim \mathcal{N}(0, 0)$ (мы считаем нуль нормально распределённым), и по лемме Слуцкого $\sqrt{n}(\overline{\mathbf X} + X_{(n)}/2 - \theta) \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}(0, \va \overline{\mathbf X})$, $\sqrt{n}(X_{(1)} + X_{(n)} - \theta) \stackrel{d_{\theta}}{\longrightarrow} 0$. Таким образом, эти оценки будут ещё и асимптотически нормальными.
    
    Как мы видим, скорость сходимости оценки $\widetilde{\theta}(\mathbf X)$ ещё больше, чем $\sqrt{n}$, попробуем уточнить её и подберём $\delta$ так, чтобы распределение $n^{\delta}(\widetilde{\theta}(\mathbf X) - X_{(n)})$ было чем-то нетривиальным.
    \begin{gather*}
        \pth(n^{\delta}(\theta - X_{(n)}) \le t) = \pth(X_{(n)} \ge \theta - t n^{-\delta}) = 
        \left\{
        \begin{aligned}
        &0,\,\,\,t \le 0;\\
        &1 - \left(1 - \frac{t}{\theta n^{\delta}}\right)^n,\,\,\,t > 0.
        \end{aligned}
        \right. \eqto
    \end{gather*}
    Как мы видим, при $\delta < 1$ распределение будет тривиальным, а при $\delta > 1$ и вовсе получается что-то неадекватное. При $\delta = 1$ же:
    \[
    \eqto \left\{
        \begin{aligned}
        &0,\,\,\,t \le 0;\\
        &1 - e^{-\frac{t}{\theta}},\,\,\,t > 0.
        \end{aligned}
        \right.,
    \]
    что есть функция распределения для $\expd\left(1/\theta\right)$.
\end{example}

\subsection{Дельта-метод}

Как мы могли видеть ранее, одних (У)ЗБЧ и ЦПТ (в том числе и их многомерных аналогов) уже достаточно для доказательства свойств простых оценок. К тому же сходимости почти наверное и по вероятности можно складывать, умножать или брать от них непрерывные функции. Но для доказательства асимптотической нормальности, и уж тем более нахождения асимптотической дисперсии этого мало, поэтому представляется крайне полезной следующая

\begin{theorem}[label=delta_method]{дельта-метод}{}
    Пусть $\boldxi_n \stackrel{d}{\to} \boldxi$, где $\boldxi, \boldxi_n$ --- $m$-мерные случайные векторы, $F(x) \colon \R^m \to \R^k$ --- функция, дифференцируемая в точке $\mathbf{a}$, и $b_n \to 0$, $b_n \ne 0$. Тогда 
    \[
    \frac{F(\mathbf a + b_n\boldxi_n) - F(\mathbf a)}{b_n} \stackrel{d}{\to} d_{\mathbf a}F(\boldxi).
    \]
    В частности, для $m = k = 1$ эта сходимость имеет вид
    \[
    \frac{F(a + b_n\xi_n) - F(a)}{b_n} \stackrel{d}{\to} F'(a)\xi.
    \]
\end{theorem}

\begin{proof}
    По определению дифференцируемости
    \begin{gather*}
        F(\mathbf a + b_n\boldxi_n) = F(\mathbf a) + d_{\mathbf a}F(b_n\boldxi_n) + o(\|b_n\boldxi_n\|),\\
        \frac{F(\mathbf a + b_n\boldxi_n) - F(\mathbf a)}{b_n} = d_{\mathbf a}F(\boldxi_n) + \boldalpha(b_n\boldxi_n) \|\boldxi_n\|,
    \end{gather*}
    где $\|\boldalpha(\mathbf h)\| \to 0$ при $\|\mathbf h\| \to 0$, поэтому можно считать, что $\boldalpha(\mathbf h)$ непрерывна в нуле. Так как по лемме Слуцкого $b_n\boldxi_n \stackrel{d}{\to} \mathbf{0}$, то по теореме о наследовании сходимости $\boldalpha(b_n\boldxi_n) \stackrel{d}{\to} \mathbf{0}$. Дважды применяя лемму Слуцкого, получаем сначала $\boldalpha(b_n\boldxi_n)\|\boldxi_n\| \stackrel{d}{\to} \mathbf{0}$, а потом $d_{\mathbf a}F(\boldxi_n) + \boldalpha(b_n\boldxi_n) \|\boldxi_n\| \stackrel{d}{\to} d_{\mathbf a}F(\boldxi)$, что приводит нас к требуемой сходимости.
\end{proof}

Из этой теоремы следует замечательное утверждение, в англоязычной литературе именно его называют дельта-методом.

\begin{corollary*}[о наследовании асимптотической нормальности]\label{safe_norm}
    Пусть $\Theta \subset \R^m$, $\widehat{\boldtheta}$ --- асимптотически нормальная оценка параметра $\boldtheta$ с асимптотической ковариационной матрицей $\Sigma(\boldtheta)$, а $\tau\colon \R^m \to \R^k$ дифференцируема на $\Theta$. Тогда $\tau\bigl(\widehat{\boldtheta}\bigr)$ асимптотически нормальная оценка $\tau(\boldtheta)$ с асимптотической ковариационной матрицей $d_{\boldtheta}\tau \cdot \Sigma(\boldtheta) \cdot d_{\boldtheta}\tau^T$, то есть
    \[
    \sqrt{n}\left(\tau\bigl(\widehat{\boldtheta}\bigr) - \tau(\boldtheta)\right) \stackrel{d_{\boldtheta}}{\to} \mathcal{N}(\mathbf 0, d_{\boldtheta}\tau \cdot \Sigma(\boldtheta) \cdot d_{\boldtheta}\tau^T).
    \]
    В частности, для $m = k = 1$, если исходная оценка имела асимптотическую дисперсию $\sigma^2(\theta)$, эта сходимость имеет вид
    \[
    \sqrt{n}\left(\tau\bigl(\widehat{\theta}\bigr) - \tau(\theta)\right) \stackrel{d_{\theta}}{\to} \mathcal{N}(0, \tau'(\theta)^2\cdot \sigma^2(\theta)).
    \]
\end{corollary*}

Этот результат можно понимать так: к $\widehat{\boldtheta}$, которая является <<примерно>> нормально распределённой при больших $n$, применяется преобразование, которое в малой окрестности $\boldtheta$ <<почти что>> линейное с матрицей $d_{\boldtheta}\tau$.

\begin{proof}
    Зафиксируем произвольное $\boldtheta \in \Theta$. По условию имеется сходимость
    \[
    \boldxi_n = \sqrt{n}\left(\widehat{\boldtheta} - \boldtheta\right) \stackrel{d_{\boldtheta}}{\to} \boldxi \sim \mathcal{N}(\mathbf 0, \Sigma(\boldtheta)).
    \]
    Положим $b_n = 1 / \sqrt{n}$, $\mathbf a = \boldtheta$. Тогда по теореме \ref{delta_method}
    \[
    \sqrt{n}\left(\tau\bigl(\widehat{\boldtheta}\bigr) - \tau(\boldtheta)\right) \stackrel{d_{\boldtheta}}{\to} d_{\boldtheta}\tau(\boldxi).
    \]
    Так как $d_{\boldtheta}\tau$ --- линейное преобразование, то $d_{\boldtheta}\tau(\boldxi)$ также является гауссовским вектором, среднее и ковариационная матрица которого равна $\mathbf 0$ и $d_{\boldtheta}\tau \cdot \Sigma(\boldtheta) \cdot d_{\boldtheta}\tau^T$ соответственно.
\end{proof}

\begin{example}\label{delta_method_for_exp}
    Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка из экспоненциального распределения с параметром $\theta$, т.е. $p_{\theta}(t) = \theta e^{-\theta t}I(t > 0)$. Рассмотрим статистику $\left(k! / \overline{\mathbf X^k}\right)^{1/k}$, где $k$ --- некоторое фиксированное натуральное число, и докажем, что она является асимптотически нормальной оценкой $\theta$.

    Для начала вспомним, что $\me X_1^k = k! / \theta^k$ (это можно получить, честно найдя интеграл или рассмотрев характеристическую функцию). По ЦПТ:
    \[
    \sqrt{n}\left(\overline{\mathbf X^k} - \frac{k!}{\theta^k}\right) \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}(0, \va X_1^k) = \mathcal{N}(0, \me X_1^{2k} - (\me X_1^k)^2) = \mathcal{N}\left(0, \frac{(2k)!-k!^2}{\theta^{2k}}\right).
    \]
    Применим теорему о наследовании асимптотической нормальности для $\tau(x) = \left(\frac{k!}{x}\right)^{1/k}$, сначала посчитав её производную:
    \[
    \tau'(x) = - \frac{k!^{1/k}}{k x^{1+1/k}};\;\;\;\tau'\left(\frac{k!}{\theta^k}\right) = \frac{\theta^{k+1}}{k!\cdot k}.
    \]
    Таким образом,
    \[
    \sqrt{n}\left(\left(k! / \overline{\mathbf X^k}\right)^{1/k} - \theta\right) \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}\left(0, \frac{(2k)!-k!^2}{\theta^{2k}}\cdot \frac{\theta^{2k+2}}{k!^2\cdot k^2} \right) = \mathcal{N}\left(0, \frac{\theta^2((2k)!-k!^2)}{k!^2\cdot k^2} \right).
    \]
\end{example}

\begin{example}
    Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка из некоторого распределения с параметром $\theta = \sigma^2 = \va X_1$. Рассмотрим выборочную дисперсию $s^2 = \overline{\mathbf X^2} - \overline{\mathbf X}^2$. Несложно показать, что она является сильно состоятельной оценкой $\sigma^2$: действительно, по УЗБЧ $\overline{\mathbf X} \stackrel{\pth\text{-п.н.}}{\longrightarrow} \me X_1$, а $\overline{\mathbf X^2} \stackrel{\pth\text{-п.н.}}{\longrightarrow} \me X^2_1$. Значит, по теореме о наследовании сходимости почти наверное: $s^2 \stackrel{\pth\text{-п.н.}}{\longrightarrow} \me X^2_1 - (\me X_1)^2 = \va X_1 = \sigma^2$. Докажем асимптотическую нормальность в случае $\me X^4_1 < \infty$.

    По многомерной ЦПТ (её можно применять, так как из конечности $\me X^4_1$ следует конечность вторых моментов у координат вектора):
    \[
    \sqrt{n}\left(
    \begin{pmatrix}
    \overline{\mathbf X}\\
    \overline{\mathbf X^2}
    \end{pmatrix}
    -
    \begin{pmatrix}
    \me X_1\\
    \me X_1^2
    \end{pmatrix}
    \right)
    \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}(0, \Sigma),
    \]
    где $\Sigma$ --- некоторая ковариационная матрица. Применяя теорему о наследовании асимптотической нормальности для $\tau(x, y) = y - x^2$:
    \[
    \sqrt{n}\left(s^2 - \sigma^2\right) = \sqrt{n}\left(\tau(\overline{\mathbf X}, \overline{\mathbf X^2}) - \tau(\me X_1, \me X_1^2)\right) \stackrel{d_{\theta}}{\longrightarrow} \mathcal{N}(0, \nabla \tau \Sigma\nabla \tau^T),
    \]
    что и требовалось.
\end{example}

\begin{example}\label{skewness_and_kurtosis}
    Рассмотрим ещё пару примеров статистических функционалов, которые носят гордое название \textit{коэффициент асимметрии и эксцесса}:
    \[
    \gamma_3 = \frac{\me[] (\xi - \me[]\xi)^3}{\left[\va[] \xi\right]^{3/2}},\;\;\;\gamma_4 = \frac{\me[] (\xi - \me[]\xi)^4}{\left[\va[] \xi\right]^{2}} - 3.
    \]
    Как можно догадаться, первый из них показывает степень симметричности распределения величины $\xi$ относительно своего матожидания. Второй же в некоторой степени отражает тяжесть хвостов распределения. Методом подстановки (см. раздел \ref{substitute}) несложно получить их выборочные аналоги:
    \[
    \alpha_3 = \frac{\dfrac{1}{n}\sum\limits_{i=1}^n \bigl(X_i - \overline{\mathbf X}\bigr)^3}{s^3(\mathbf X)},\;\;\;\alpha_4 = \frac{\dfrac{1}{n}\sum\limits_{i=1}^n \bigl(X_i - \overline{\mathbf X}\bigr)^4}{s^4(\mathbf X)} - 3.
    \]
    Данные коэффициенты используются для проверки нормальности: для гауссовской величины $\xi$ они оба равны 0 (отсюда и вычитаемая тройка в $\gamma_4$), поэтому если для эмпирической функции распределения сии коэффициенты оказались близки к нулю, то допустимо сделать предположение о нормальности данных (более подробно см. в главе \ref{special_test}). Отсюда появляется потребность в рассмотрении их оценок и оценки их дисперсий. Докажем, что для нормальной модели $\alpha_3$ является асимптотически нормальной оценкой $\gamma_3$ и найдём её асимптотическую дисперсию (аналогичная процедура для $\alpha_4$ предлагается читателю в качестве упражнения, см. задачу \ref{asympt_var_of_kurtosis}).

    Несложно убедиться, что значение $\alpha_3$ не меняется при сдвиге и масштабировании выборки, поэтому будем считать, что $X_i \sim \mathcal{N}(0, 1)$. В таком случае удобнее считать моменты:
    \[
    \me[] X_1 = \me[] X_1^3 = \me[] X_1^5 = 0,\;\;\;\me[] X_1^2 = 1,\;\;\;\me[] X_1^4 = 3,\;\;\;\me[] X_1^6 = 15.
    \]
    По многомерной ЦПТ можно убедиться в асимптотической нормальности вектора, составленного из первых трёх выборочных моментов:
    \begin{equation}\label{shit_conv}
        \sqrt{n}\left(
    \begin{pmatrix}
    \overline{\mathbf X}\\
    \overline{\mathbf X^2}\\
    \overline{\mathbf X^3}
    \end{pmatrix}
    -
    \begin{pmatrix}
    \me[] X_1\\
    \me[] X_1^2\\
    \me[] X_1^3
    \end{pmatrix}
    \right)
    \stackrel{d}{\longrightarrow} \mathcal{N}(0, \Sigma), \;\;\text{где}\;\;\Sigma = \va[] 
    \begin{pmatrix}
    X_1\\
    X_1^2\\
    X_1^3
    \end{pmatrix}.
    \end{equation}
    Зная все моменты, посчитать элементы матрицы $\Sigma$ не составляет особого труда:
    \begin{gather*}
        \Sigma_{11} = \va[] X_1 = 1,\;\;\;\Sigma_{22} = \va[] X_1^2 = \me[]X_1^4 - (\me[]X_1^2)^2 = 2,\;\;\;\Sigma_{33} = \va[] X_1^3 = \me[]X_1^6 - (\me[]X_1^3)^2 = 15,\\
        \Sigma_{12} = \Sigma_{21} = \cov(X_1, X_1^2) = \me[]X_1^3 - \me[]X_1 \cdot \me[]X_1^2 = 0,\\
        \Sigma_{23} = \Sigma_{32} = \cov(X_1^2, X_1^3) = \me[]X_1^5 - \me[]X_1^2 \cdot \me[]X_1^3 = 0,\\
        \Sigma_{13} = \Sigma_{31} = \cov(X_1, X_1^3) = \me[]X_1^4 - \me[]X_1 \cdot \me[]X_1^3 = 3.
    \end{gather*}
    Преобразовав формулу для $\alpha_3$, выразим его через выборочные моменты:
    \[
    \alpha_3 = \frac{\dfrac{1}{n}\sum\limits_{i=1}^n \bigl(X_i^3 - 3 X_i^2 \cdot \overline{\mathbf X} + 3 X_i \cdot \overline{\mathbf X}^2 - \overline{\mathbf X}^3\bigr)}{\bigl(\overline{\mathbf X^2} - \overline{\mathbf X}^2\bigr)^{3/2}} = \frac{\overline{\mathbf X^3} - 3 \overline{\mathbf X^2}\cdot \overline{\mathbf X} + 2 \overline{\mathbf X}^3}{\bigl(\overline{\mathbf X^2} - \overline{\mathbf X}^2\bigr)^{3/2}}.
    \]
    Таким образом,
    \[
    \alpha_3 = \tau\bigl(\overline{\mathbf X}, \overline{\mathbf X^2}, \overline{\mathbf X^3}\bigr),\;\;\text{где}\;\;\tau(x, y, z) = \frac{z - 3yx + 2x^3}{(y - x^2)^{3/2}},
    \]
    и нам остаётся применить дельта-метод к сходимости \eqref{shit_conv} и функции $\tau$. Для этого найдём градиент $\tau$ в точке $a = (\me[] X_1, \me[] X_1^2, \me[] X_1^3) = (0, 1, 0)$:
    \begin{gather*}
        \frac{\partial \tau}{\partial x}(a) = \frac{\partial}{\partial x} \left.\frac{2x^3-3x}{(1-x^2)^{3/2}}\right|_{a} = -3,\;\;\;\frac{\partial \tau}{\partial y}(a) = \frac{\partial}{\partial y} \left.\frac{0}{y^{3/2}}\right|_{a} = 0,\;\;\;\frac{\partial \tau}{\partial z}(a) = \frac{\partial}{\partial z} \left.\frac{z}{(1-0)^{3/2}}\right|_{a} = 1.
    \end{gather*}
    Итого, получаем, что
    \begin{gather*}
        \sqrt{n} \alpha_3 = 
    \sqrt{n}\left(
    \tau\bigl(\overline{\mathbf X}, \overline{\mathbf X^2}, \overline{\mathbf X^3}\bigr)
    -
    \tau(a)
    \right)
    \stackrel{d}{\longrightarrow} \mathcal{N}(0, \sigma^2), \;\;\text{где}\;\;\sigma^2 = \nabla\tau^T \cdot \Sigma \cdot \nabla\tau = \\
    = \begin{pmatrix}
        -3 & 0 & 1
    \end{pmatrix} \cdot 
    \begin{pmatrix}
        1 & 0 & 3\\
        0 & 2 & 0\\
        3 & 0 & 15
    \end{pmatrix}\cdot 
    \begin{pmatrix}
        -3\\
        0\\
        1
    \end{pmatrix} = 6.
    \end{gather*}
\end{example}

\subsection*{Задачи}

\begin{problem}
	Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка из $\expd(\theta)$. Для какой функции от параметра $\theta$ оценка $e^{-\overline{\mathbf X^2}}$ будет состоятельной? Является ли она при этом несмещённой? А асимптотически нормальной?
\end{problem}

\begin{problem}
	Найдите константу $C$ такую, что статистика $C\sum_{i=1}^n |X_{2i-1} - X_{2i}|$ является несмещённой оценкой параметра $\sigma$, где $\mathbf X = (X_1, \ldots, X_{2n})$ --- выборка из $\mathcal{N}(a, \sigma^2)$. Покажите, что она также будет асимптотически нормальной, и найдите её асимптотическую дисперсию.
\end{problem}

\begin{problem}\label{emp_mean_and_var_are_almost_ind}
	Пусть выборка $\mathbf X = (X_1, \ldots, X_n)$ пришла из распределения со средним $\mu$, дисперсией $\sigma^2$ и конечным четвёртым моментом. При каком условии предельное распределение случайного вектора
\[
\sqrt{n}\left(
\begin{pmatrix}
	\overline{\mathbf X}\\
	s^2(\mathbf X)
\end{pmatrix}
-
\begin{pmatrix}
	\mu\\
	\sigma^2
\end{pmatrix}
\right)
\]
имеет независимые компоненты?
\end{problem}

\begin{problem}\label{unbiased-est-for-exp}
	Постройте несмещённую асимптотически нормальную оценку для параметра $e^{-\theta}$ с помощью выборки $\mathbf X = (X_1, \ldots, X_n)$ из распределения $\poisd(\theta)$, где $\theta > 0$, и найдите её асимптотическую дисперсию.
\end{problem}

\begin{problem}
	Оценка $\widehat{\theta}_n(\mathbf X)$ параметра $\theta$ называется $\textit{асимптотически несмещённой}$, если $\forall \theta\colon \me \widehat{\theta}_n \to \theta$. Докажите, что асимптотически несмещённая оценка со стремящейся к нулю дисперсией является состоятельной.
\end{problem}

\begin{problem}
	Предложите какую-нибудь модель и состоятельную оценку параметра в ней, которая не является асимптотически нормальной.

\textit{Замечание.} Считаем константу нормально распределённой с дисперсией~$0$.
\end{problem}

\begin{problem}
	Пусть $\mathbf X = (X_1, \ldots, X_n)$ --- выборка из распределения со средним $\mu \ne 0$ и конечной дисперсией. Известно, что плотность распределения $X_1$ больше некоторого $c > 0$ в окрестности нуля. Покажите, что $\overline{\mathbf X}^{-1}$ является асимптотически нормальной оценкой $1/\mu$, но при этом $\me[\mu] |\overline{\mathbf X}^{-1}| = \infty$.
\end{problem}

\begin{problem}
	Пусть $G(n, p)$ --- случайный граф в модели Эрдеша-Реньи (каждое ребро берётся независимо от других с вероятностью $p$). Найдите какую-нибудь сильно состоятельную оценку параметра $p$ как функцию от числа треугольников в графе. 
\end{problem}

\begin{problem}\label{asympt_var_of_kurtosis}
	Докажите, что выборочный коэффициент эксцесса $\alpha_4$ (см. пример \ref{skewness_and_kurtosis}) для выборки из нормального распределения является асимптотически нормальной оценкой нуля, и найдите его асимптотическую дисперсию.
\end{problem}

% \begin{problem}
%     Пусть $X_1, \ldots, X_n$ -- выборка из распределения с плотностью
%     \[
%         \rho_{\theta}(x) = \frac{1 + \theta x}{2} I(-1 \le x \le 1),\;\;\;\theta \in \Theta = [-1;1].
%     \]
%     Рассмотрим оценку $\widehat{\theta}(X) = 3 \overline{X}$. Является ли она несмещённой, состоятельной, сильно состоятельной, асимптотически нормальной?
% \end{problem}

% \begin{problem}
%     Пусть $X_1, \ldots, X_n \sim F$
% \end{problem}


