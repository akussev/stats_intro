\section{Достаточные статистики}\label{sufficiency}

\subsection{Улучшение оценок}

Рассмотрим подробнее статистики, сохраняющие информацию о неизвестном параметре при редукции данных, которых мы коснулись ранее в разделе \ref{stat_information}.

\begin{definition}
    $\sigma$-алгебра $\mathcal{G} \subset \mathcal{B}(\mathcal{X})$ называется \textit{достаточной} для семейства распределений $\mathcal P_{\theta}$, если существует вариант условного распределения $\mathsf P_{\theta}(X \in B\,|\,\mathcal{G})$, которое не зависит от $\theta$. Статистика $S(\mathbf X)$ называется \textit{достаточной}, если достаточна $\sigma$-алгебра $\sigma(S(\mathbf X))$, или, что эквивалентно, существует вариант условного распределения $\mathsf P_{\theta}(X \in B\,|\,S(\mathbf X))$, которое не зависит от $\theta$.
\end{definition}

Неформально смысл такого определения можно понимать так. Представим себе машину-генератор случайных чисел, который выдаёт нам вектор в виде выборки. С одной стороны, <<розыгрыш>> значения вектора выборки происходит в соответствии совместной функции распределения наблюдения, а с другой --- его можно разделить на два этапа: сначала выбираем значение $t$ для достаточной статистики $S(\mathbf X)$, а после этого -- само значение $\mathbf X$ в соответствии с условным распределением $\mathsf P_{\theta}(\mathbf X \in B|S(\mathbf X) = t)$. Так как оно не зависит от параметра $\theta$, то вся информация о нём хранится в первом этапе. Таким образом, то, какое именно значение $\mathbf X$ доставляет равенство $S(\mathbf X) = t$, нас не особо интересует в отличие от самого $S(\mathbf X)$.

Проверять статистику на достаточность по определению весьма затруднительно, поэтому нам на выручку приходит следующая
\begin{theorem}{критерий факторизации}{}
Пусть $\mathcal P_{\theta}$ --- доминируемое семейство распределений с обобщённой плотностью $\rho_{\theta}$. Тогда $S(\mathbf X)$ --- достаточная статистика тогда и только тогда, когда обобщённая плотность допускает представление
\[
\rho_{\theta}(\mathbf{x}) = g_\theta(S(\mathbf{x})) h(\mathbf{x}),
\]
где для всех $\theta$ функции $g_\theta$ и $h$ -- борелевские и неотрицательные (сравните с выводом в конце раздела \ref{stat_information}).
\end{theorem}

Заметим, что распределения из экспоненциального семейства $\mathcal P_{\theta}$ автоматически имеют достаточные статистики $T(\mathbf X)$, ведь плотность для них имеет вид
\begin{equation}\label{exp}
    \rho_{\theta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(a_0(\theta)+\sum_{i=1}^{k} a_i(\theta) T_i(\mathbf{x})\right),
\end{equation}
и из критерия факторизации получаем требуемое.

\begin{example}\label{suff_for_not_exp}
    Рассмотрим пример достаточной статистики не для экспоненциального семейства. Введём модель с равномерным распределением $\ud(0, \theta)$, где $\theta$ -- неизвестный параметр. Это семейство распределений имеет плотность $\rho_{\theta}(x) = \frac{1}{\theta} I(0 < x < \theta)$, а значит, совместная плотность имеет вид
    \[
    \rho_{\theta}(\mathbf x) = \frac{1}{\theta^n}I(0 < x_i < \theta,\; i = 1, \ldots, n) = I(0 < x_{(1)})\cdot\frac{I(x_{(n)} < \theta)}{\theta^n} = h(\mathbf x) \cdot g(T(\mathbf x), \theta),
    \]
    где $h(\mathbf x) = I(0 < x_{(1)})$, $g(t, \theta) = \frac{I(t < \theta)}{\theta^n}$, $T(\mathbf x) = x_{(n)}$. Таким образом, $X_{(n)}$ является достаточной статистикой по критерию факторизации.
\end{example}

Один из главных плюсов достаточных статистик заключается в том, что они позволяют <<улучшать>> оценки в среднеквадратичном подходе.

\begin{theorem}{Колмогоров, Блэкуэлл, Рао}{}
    Пусть $S(\mathbf X)$ является достаточной статистикой для семейства $\mathcal{P} = \{\pth\colon \theta \in \Theta\}$, и $\theta^*(\mathbf X)$ --- несмещённая оценка $\tau(\theta)$. Тогда $T(\mathbf X) = \me(\theta^*(\mathbf X)\,|\,S(\mathbf X))$ является статистикой и несмещённой оценкой $\tau(\theta)$, причём она <<лучше>> исходной в среднеквадратичном смысле, то есть
    \[
    \me(T - \tau(\theta))^2 \le \me(\theta^* - \tau(\theta))^2,\;\;\;\theta \in \Theta.
    \]
\end{theorem}

\begin{proof}
    Первое заключение теоремы дано не зря: совершенно не очевидно, что УМО будет вообще статистикой --- вдруг её значение зависит от неизвестного параметра? В общем случае это, вообще говоря, вполне возможно (см. задачу \ref{cme_is_not_always_estimator}), но оказывается, для достаточных статистик всё хорошо. Действительно, УМО можно записать как матожидание по условной мере (см. \S7, гл. II, \cite{shiryaev}):
    \[
    \me(\theta^*(\mathbf X)\,|\,S(\mathbf X)) = \int \theta^*(x) \,\mathsf{P}_{\theta}(dx\,|\,S(\mathbf X)).
    \]
    Распределение, по которому берётся матожидание, постоянно и не зависит от параметра, поэтому и УМО от него не зависит и, следовательно, является статистикой.

    Несмещённость $T(\mathbf X)$ очевидна из свойств УМО. Последнее заключение можно доказать с помощью неравенства Йенсена. Напомним его: если $g(x)$ --- выпуклая вниз функция и $\me[]|g(\xi)| < \infty$, то почти наверное выполнено неравенство
    \[
    g\left(\me[](\xi | \eta)\right) \le \me[](g(\xi) | \eta).
    \]
    Возьмём произвольный $\theta \in \Theta$, функцию $g(x) = (x - \tau(\theta))^2$ и $\xi = \theta^*(\mathbf X)$. Тогда по неравенству Йенсена
    \[
    \left(\me(\theta^*\,|\,S(\mathbf X)) - \tau(\theta)\right)^2 \le \mathsf{E}_{\theta}\left[(\theta^* - \tau(\theta))^2\,|\,S(\mathbf X)\right]
    \]
    Беря от обеих частей неравенства матожидание, получаем требуемое.
\end{proof}

\subsection{Оптимальные оценки}

Оказывается, при некоторых дополнительных ограничениях достаточная статистика может дать нам не просто оценку лучше прежней, так ещё и \textit{лучшую} в среднеквадратичном подходе.

\begin{definition}
    Оценка $\widehat{\theta}$ называется \textit{оптимальной}, если она является наилучшей в классе несмещённых оценок в среднеквадратичном подходе.
\end{definition}

Сразу выделим важное свойство таких оценок.

\begin{proposition}
    Если $\widehat{\theta}(\mathbf X)$ и $\theta^*(\mathbf X)$ --- две оптимальные оценки, то они равны $\pth$-п.н. для любого $\theta$.
\end{proposition}

\begin{proof}
    По определению эти оценки несмещённые, а значит, по линейности матожидания оценка $(\widehat{\theta} + \theta^*)/2$ также не смещена. В силу оптимальности $4\me \widehat{\theta}^2 \le \me (\widehat{\theta} + \theta^*)^2$ и $4\me \left(\theta^*\right)^2 \le \me (\widehat{\theta} + \theta^*)^2$, что при сложении даёт $2\me \widehat{\theta}^2 + 2\me \left(\theta^*\right)^2 \le 4\me 
    \widehat{\theta}\theta^*$. При выделении полного квадрата получаем $\me (\widehat{\theta} - \theta^*)^2 \le 0$, что, конечно, означает, что для каждого $\theta$ выражение под знаком матожидания почти наверное равно 0.
\end{proof}

Таким образом, оптимальная оценка не более чем единственна. Существование оной можно показать с помощью следующего понятия.

\begin{definition}
    Статистика $S(\mathbf X)$ называется \textit{полной} для семейства распределений $\mathcal P_{\theta}$, если для любой борелевской функции $f(x)$ выполнено
    \begin{gather*}
        \forall {\theta \in \Theta}\colon \mathsf E_{\theta}f(S(\mathbf X)) = 0 \Longrightarrow
        \forall {\theta \in \Theta}\colon f(S(\mathbf X)) = 0\,\, (\mathsf P_{\theta}\text{-п. н.})
    \end{gather*}
\end{definition}

Определение по сути говорит, что статистика $S(\mathbf X)$ выражает параметр единственным образом, то есть вы не можете двумя разными способами несмещённо оценить функцию от параметра с помощью $S(\mathbf X)$, так как иначе матожидание их разности даст нуль, который по определению полной статистики несмещённо оценивается лишь нулём. Весьма полезной для нахождения полных достаточных статистик оказывается следующая

\begin{theorem}{}{}
Пусть $\theta \in \Theta \subset \R^k$ и для семейства $\mathcal P_{\theta}$ выполняется (\ref{exp}). Пусть кроме того множество значений $(a_1(\theta), \ldots, a_k(\theta))$ для $\theta \in \Theta$ содержит внутреннюю точку. Тогда $T(\mathbf X)$ является полной достаточной статистикой.
\end{theorem}

Ключевой особенностью полных достаточных статистик является тот факт, что они из оценки любой степени паршивости могут сделать конфетку:

\begin{theorem}[label=lehmann_scheffe]{Леман, Шеффе}{}
    Если $S(\mathbf X)$ -- полная достаточная статистика для $\mathcal P_{\theta}$ и $\me \widehat{\theta}(\mathbf X) = \tau(\theta)$, то $\theta^*(\mathbf X) = \me (\widehat{\theta}(\mathbf X) | S(\mathbf X))$ -- оптимальная оценка для $\tau(\theta)$.
\end{theorem}

Как следствие, функция от полной достаточной статистики заведомо является оптимальной оценкой своего матожидания, так как в силу $S$-измеримости её УМО --- она же сама.

\begin{example}\label{suff_norm}
     По выборке из распределения $\mathcal N(a, \sigma^2)$ построим оптимальную оценку для вектора параметров $\theta = (a, \sigma^2)$. Распишем более подробно совместную плотность для нормального распределения:
    \begin{gather*}
    \rho_{\theta}(\mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum \frac{(x_i - a)^2}{2\sigma^2}\right) = \\
    = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac1{2\sigma^2}\sum x_i^2 + \frac{a}{\sigma^2}\sum x_i - \frac{na^2}{2\sigma^2}\right),
    \end{gather*}
    что является функцией от $\left(\sum x_i^2, \sum x_i\right)$ и вектора параметров $\theta = (a, \sigma^2)$. Значит, по критерию факторизации статистика $T(\mathbf X)=\left(\sum X_i^2, \sum X_i\right)$ является достаточной, а следовательно
    \[
    \widehat{a} = \overline{\mathbf X},\,\,\,\widehat{\sigma^2} = \frac{n}{n-1}s^2 = \frac{n}{n-1}(\overline{\mathbf X^2} - \overline{\mathbf X}^2)
    \]
    будут являться оптимальными оценками.
\end{example}

\begin{example}
    По выборке размера $n \ge 2$ из распределения $\expd(\lambda)$ найдём оптимальную оценку для параметра $\lambda$. Так как $\mathsf E_{\theta} \overline{\mathbf X} = \frac{1}{\lambda}$, то логично предположить, что $1 / \overline{\mathbf X}$ даст нам что-то подходящее. Проверим эту догадку. Так как $X_i \sim \expd(\lambda) \sim \Gamma(1, \lambda)$, то $\sum X_i \sim \Gamma(n, \lambda)$ (в силу независимости $X_i$). Это в свою очередь означает, что
    \begin{gather*}
    	\mathsf E_{\theta} \left(\frac{1}{\sum X_i}\right) = \int_0^{+\infty} \frac{1}{x} \frac{\lambda^n x^{n-1}}{\Gamma(n)}e^{-\lambda x}\,dx = \\
    	= \frac{\Gamma(n-1)\lambda}{\Gamma(n)} \underbrace{\int_0^{+\infty} \frac{\lambda^{n-1} x^{n-2}}{\Gamma(n-1)}e^{-\lambda x}\,dx}_{\text{интеграл плотности $\Gamma(n-1,\lambda)$}} = \frac{\Gamma(n-1)\lambda}{\Gamma(n)} = \frac{\lambda}{n-1}.
    \end{gather*}
    Таким образом, из теоремы Лемана-Шеффе получаем, что $\frac{n-1}{\sum X_i}$ является требуемой оптимальной оценкой.

    Возникает логичный вопрос: а зачем условие $n \ge 2$? Что будет, если рассмотреть случай $n=1$? Оказывается, что оптимальной оценки в данном случае просто нет. Действительно, пусть существует оптимальная $T(X_1)$ такая, что $\me[\lambda] T(X_1) = \lambda$. В данной модели выполняются условия регулярности, причём так как $T(X_1)$ --- оптимальна, то у неё существует второй момент, поэтому можно применить свойство \ref{diff_by_theta} регулярности:
    \begin{gather*}
    1 = \frac{\partial}{\partial \lambda} \lambda = \frac{\partial}{\partial \lambda} \me T(X_1) = \me\left( T(X_1) \frac{\partial}{\partial \lambda} \ln{\rho_{\lambda}(X_1)}\right) = \me\left(T(X_1) \left(\frac{1}{\lambda} - X_1\right)\right) =\\
    = \frac{1}{\lambda} \me T(X_1) - \me (X_1 \cdot T(X_1)) = 1 - \me (X_1 \cdot T(X_1)).
    \end{gather*}
    Таким образом, $\me (X_1 \cdot T(X_1)) = 0$. Но так как $T(X_1)$ -- оценка для $\lambda$, то её значения неотрицательны. Если интеграл неотрицательной функции равен нулю, то она почти наверное равна нулю, чего быть не может --- противоречие.
\end{example}

Как можно видеть, теорема не всегда даёт положительный результат, так как несмещённой оценки может просто не быть. Но есть и другая проблема --- для некоторых семейств распределений нет полной достаточной статистики. От такого и вправду никто не застрахован: чтобы статистика была достаточной, она должна быть достаточно <<жирной>> в плане информации, в то время как полная статистика наоборот --- скромной, чтобы вдруг не появилось других способов несмещённо оценить нуль.

\begin{example}
    Рассмотрим семейство распределений $\mathcal N(\theta, \theta^2)$, где $\theta > 0$. Мы уже знаем, что $\left(\sum X_i^2, \sum X_i\right)$ является достаточной статистикой, но теперь параметр лишь один, и есть подозрения, что сейчас эта оценка несёт в себе слишком много информации, что наводит на мысли о неполноте. Надо показать, что, во-первых, она не будет полной, а во-вторых, и это самое главное, любая другая достаточная статистика будет априори \textit{богаче} данной, и из этого мы выведем, что она тем паче не будет полной.

    Неполнота $\left(\sum X_i^2, \sum X_i\right)$ выводится из выражения параметра $\theta$ двумя способами:
    \begin{gather*}
    \left.
    \begin{aligned}
        \me \left(\sum X_i\right)^2 = \va \sum X_i + \left(\me \sum X_i\right)^2 = (n+n^2)\theta^2\\
        \me \sum X_i^2 = n \me X_i^2 = n(\va X_i + (\me X_i)^2) = 2n\theta^2
    \end{aligned}\right\}
    \Longrightarrow \\
    \Longrightarrow \me \left[2\left(\sum X_i\right)^2 - (n+1)\sum X_i^2\right] = 0,
    \end{gather*}
    при этом выражение под знаком матожидания не равно нулю почти наверное.
    
    Пусть нашлась достаточная статистика $T(\mathbf X)$, то есть по критерию факторизации $\rho_{\theta}(\mathbf{x})=g(T(\mathbf{x}), \theta)\cdot h(\mathbf{x})$. Также мы знаем, что
    \[
    \rho_{\theta}(\mathbf x) = \frac{1}{(2\pi\theta^2)^{n/2}}\exp\left(-\frac1{2\theta^2}\sum x_i^2 + \frac{1}{\theta}\sum x_i - \frac{n}{2}\right),
    \]
    Очень хочется показать, что $\sum x_i^2$ и $\sum x_i$ на самом деле выражаются через $T(x)$. Постараемся подобрать $\theta$ так, чтобы и избавиться от ненужной $h(\mathbf x)$, и убрать, например, $\sum x_i$, оставив наедине $T(\mathbf x)$ и $\sum \mathbf x_i^2$:
    \begin{gather*}
    \frac{\rho_{1}(\mathbf x)\rho_{1/3}(\mathbf x)}{\rho_{1/2}(\mathbf x)\rho_{1/2}(\mathbf x)} = \frac{g(T(\mathbf x), 1) \cdot g(T(\mathbf x), 1/3)}{g(T(\mathbf x), 1/2) \cdot g(T(\mathbf x), 1/2)} = C\cdot \exp \left((-1/2-9/2+2+2)\sum x_i^2\right) \Longrightarrow\\
    \sum x_i^2 = -\ln\left(\frac{g(T(\mathbf x), 1) \cdot g(T(\mathbf x), 1/3)}{C g(T(\mathbf x), 1/2) \cdot g(T(\mathbf x), 1/2)}\right)
    \end{gather*}
    В правой части -- функция от $T(\mathbf x)$, что мы и хотели. Аналогично можно показать, что $\sum x_i$ -- функция от $T(\mathbf x)$, то есть $\left(\sum x_i^2, \sum x_i\right) = \phi(T(\mathbf x))$, где $\phi$ -- некая борелевская функция.
    
    {\footnotesize Это и показывает тот факт, что $T(\mathbf X)$ богаче $\left(\sum X_i^2, \sum X_i\right)$, ведь если статистика есть борелевская функция от другой статистики, то $\sigma$-алгебра первой есть подмножество второй. Иными словами, $\left(\sum X_i^2, \sum X_i\right)$ является \textit{минимальной достаточной статистикой}. Занятно, что минимальная достаточная $\sigma$-алгебра существует \textit{всегда}. Правда нам всё же удобнее работать со статистиками, а с отысканием минимальных достаточных статистик всё не так просто, и этому можно посвятить отдельный параграф (например, \S~23 в \cite{borovkov}).}
    
    Почему же из этого следует, что $T(\mathbf X)$ точно не полная? Предположим, что это не так, и $T(\mathbf X)$ всё-таки полная. Но тогда несложно по определению проверить, что полной окажется $\left(\sum X_i^2, \sum X_i\right)$. Действительно, если
    \begin{gather*}
    \me f\left(\sum X_i^2, \sum X_i\right) = 0 \Longrightarrow \me f(\phi(T(\mathbf X))) = 0 \Longrightarrow \text{из полноты $T(\mathbf X)$ для $f \circ \phi$} \\
    f(\phi(T(\mathbf X))) = f\left(\sum X_i^2, \sum X_i\right) = 0\,\,(\mathsf P_{\theta}\text{-п.н.}) \Longrightarrow \left(\sum X_i^2, \sum X_i\right)\text{ -- полная},
    \end{gather*}
    откуда и получаем заветное противоречие.
\end{example}

\subsection{Статистика помогает теории вероятностей}

Занятно, что полученные результаты применимы в задачах по классической теории вероятностей, в которых нет вероятностно-статистической модели и выборочного пространства, но из доказательства фактов для нашей модели будет следовать их справедливость вообще.

Первое приложение заключается в новом способе нахождения УМО. Идея в следующем: пусть нам нужно найти УМО некоторой случайной величины $\xi$ при условии полной достаточной статистики $T(\mathbf X)$. Тогда по теореме Лемана-Шеффе это УМО будет оптимальной оценкой матожидания $\xi$, причём, как известно, УМО есть борелевская функция от условия $\phi(T(\mathbf X))$. Таким образом, если мы подберём $\phi$ так, чтобы $\me[]\phi(T(\mathbf X)) = \me[]\xi$, то из единственности оптимальной оценки будет следовать, что $\phi(T(\mathbf X))$ есть искомое УМО.

\begin{example}
    Пусть $X_1, \ldots, X_n$ --- н.о.р.с.в. из распределения $\poisd(\lambda)$. Найдём $\me[](X_1^2|X_1 + \ldots + X_n)$. Заметим, что никакой статистики тут нет --- вероятностное пространство, на котором заданы величины, произвольно, а параметр $\lambda$ фиксирован, и его не надо оценивать. Но УМО однозначно определяется совместным распределением аргумента и условия, поэтому достаточно найти его в модели масштаба, когда семейство распределений есть $\mathcal{P} = \{\expd(\lambda)\colon \lambda \in \R_+\}$.

    Из сказанного выше легко понять, что $\sum X_i$ является полной достаточной статистикой, а значит, по теореме Лемана-Шеффе УМО будет являться оптимальной оценкой для матожидания $X_1^2$, то есть для $\me[\lambda] X_1^2 = \va[\lambda] X_1 + (\me[\lambda] X_1)^2 = \lambda^2 + \lambda$. Таким образом, нам надо каким-то образом найти такую $\phi$, что $\me[\lambda] \phi\left(\sum X_i\right) = \lambda^2 + \lambda$. Что ж, логично начать с
    \begin{gather*}
        \me[\lambda] \left(\sum X_i\right)^2 = \va[\lambda] \sum X_i + \left(\me[\lambda] \sum X_i\right)^2 = n\va[\lambda] X_1 + (n \me[\lambda] X_1)^2 = n\lambda + n^2\lambda^2.
    \end{gather*}
    Мы почти у цели. Осталось слегка поменять коэффициент у $\lambda$:
    \begin{gather*}
    \me[\lambda] \left[(n-1)\sum X_i + \left(\sum X_i\right)^2 \right] = (n - 1) n\lambda + n\lambda + n^2\lambda^2 = n^2(\lambda + \lambda^2) \Longrightarrow\\
    \me[\lambda] (X_1^2|X_1 + \ldots + X_n) = \frac{n - 1}{n^2} \sum X_i + \frac{1}{n^2} \left(\sum X_i\right)^2.
    \end{gather*}
\end{example}

Но по-настоящему крутой и полезный результат, который будет использован далее, сформулирован в следующем утверждении.

\begin{theorem}{Басу}{}
    Пусть $S(\mathbf X)$ --- полная достаточная статистика, $A(\mathbf X)$ --- статистика, распределение которой одинаково при всех $\theta \in \Theta$ (\textit{англ.} \texttt{ancillary statistic}). Тогда статистики $A(\mathbf X)$ и $S(\mathbf X)$ независимы при любом $\theta \in \Theta$.
\end{theorem}

\begin{proof}
    Проведём доказательство в лоб: покажем независимость событий из $\sigma$-алгебр, порождённых $S$ и $A$. Пусть $C \in \sigma(A)$, то есть $\exists {B \in \mathcal B(\R^n)}\colon A^{-1}(B) = C$. Рассмотрим $I_B \circ A(\mathbf X) = I(\mathbf X \in C)$. Её распределение также не зависит от $\theta$, так как определяется распределением $A(\mathbf X)$. Это значит, что $\me I(\mathbf X \in C)$ является некоторой константой, независящей от $\theta$. То есть $I(\mathbf X \in C)$ является несмещённой оценкой константы $\me I(\mathbf X \in C)$. Возникает вопрос: а какая есть у этой константы оптимальная оценка, то есть с минимальной дисперсией? Так она же сама и является! Её дисперсия попросту равна нулю, куда уж меньше? Следовательно, по теореме Лемана-Шеффе
    \[
    \me (I(\mathbf X \in C) | S(\mathbf X)) = \me I(\mathbf X \in C).
    \]
    По определению УМО это означает, что для любого $D \in \sigma(S)$:
    \[
    \int_D I(\mathbf x \in C)\,\mathsf{P}_{\theta}(d\mathbf x) = \int_D \me I(\mathbf X \in C)\,d\mathsf{P}_{\theta}.
    \]
    Но первый интеграл равен $\int I(\mathbf x \in C \cap D)\,\mathsf{P}_{\theta}(d\mathbf x) = \mathsf{P}_{\theta}(\mathbf X \in C \cap D)$, а второй~--- $\me I(\mathbf X \in C) \cdot \int I(\mathbf x \in D)\,\mathsf{P}_{\theta}(d\mathbf x) = \mathsf{P}_{\theta}(\mathbf X \in C)\cdot\mathsf{P}_{\theta}(\mathbf X \in D)$, что и требовалось.
\end{proof}

Продемонстрируем работу данной теоремы.

\begin{example}\label{ind_of_stat_for_norm}
    Докажем, что статистики $\overline{\mathbf X}$ и $s^2$, построенные по выборке из нормального распределения, независимы. Рассмотрим модель сдвига $\mathcal{N}(a, \sigma^2)$, где $a$ -- параметр, а $\sigma^2$ -- \textit{известная} величина. Распишем плотность, как в примере \ref{suff_norm}:
    \[
    \rho_{\theta}(\mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum \frac{(x_i - a)^2}{2\sigma^2}\right) =
    \]
    \[
    = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac1{2\sigma^2}\sum x_i^2 + \frac{a}{\sigma^2}\sum x_i - \frac{na^2}{2\sigma^2}\right) =
    \]
    \begin{gather*}
    = \underbrace{\frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac1{2\sigma^2}\sum x_i^2\right)}_{h(\mathbf{x})} \cdot \underbrace{\exp\left(\frac{a}{\sigma^2}\sum x_i - \frac{na^2}{2\sigma^2}\right)}_{g(T(\mathbf{x}), a)},
    \end{gather*}
    где $T(\mathbf x)=\sum x_i$. Применяем критерий факторизации и понимаем, что $\overline{\mathbf X}$ -- достаточная статистика. Она же будет полной, так как модель принадлежит экспоненциальному семейству, и функция перед $T((\mathbf X)X)$ в экспоненте, а именно $a/\sigma^2$, подходит под достаточное условие полноты. Осталось только показать, что распределение $s^2$ не зависит от $a$, и дело в шляпе --- применима теорема Басу. Но это несложно показать, избавившись от параметра в формуле выборочной дисперсии: так как $a$ --- матожидание $X_i$, а $\sigma^2$ --- её дисперсия, то $X_i$ можно представить в виде $a + \sigma Y_i$, где $Y_i$ имеет стандартное нормальное распределение. Но тогда
    \[
    s^2 = \sum \left(X_i - \overline{\mathbf X}\right)^2 = \sum \left(a + \sigma Y_i - \overline{a + \sigma\mathbf{Y}}\right)^2 = \sigma^2\sum \left(Y_i - \overline{\mathbf Y}\right)^2.
    \]
    Последнее выражение -- функция от выборки из независимых величин, которые распределены одинаково вне зависимости от $a$, поэтому её распределение также не зависит от $a$.
\end{example}

\begin{example}
    Пусть $X_1, X_2, X_3$ --- н.о.р.с.в. из распределения $\expd(\lambda)$. Докажем, что $$X_1 + X_2 + X_3 \ind \frac{X_1}{X_1+X_2+X_3}.$$ Полнота и достаточность $X_1 + X_2 + X_3$ проверяется аналогично. Так как $X_i \sim Exp(\lambda)$, то $X_i = \xi_i / \lambda$, где $\xi_i \sim Exp(1)$. Значит, для любого $c\in\R$
    \begin{gather*}
    \mathsf P_{\lambda}\left(\frac{X_1}{X_1+X_2+X_3} \le c\right) = \mathsf P_{\lambda}\left(\frac{\xi_1}{\xi_1+\xi_2+\xi_3} \le c\right) = \\
    = \mathsf P_{\lambda}\left(\xi_1 \le c(\xi_1+\xi_2+\xi_3)\right) = F_{(1-c)\xi_1-c\xi_2-c\xi_3}(0),
    \end{gather*}
    что определяется свёрткой независимых случайных величин $\xi_i$, а стало быть определяется целиком и полностью $c$, и от $\lambda$ не зависит.
\end{example}

\subsection*{Задачи}

\begin{problem}
    Покажите, что достаточная статистика из примера \ref{suff_for_not_exp} полна.
\end{problem}

\begin{problem}
    Найдите достаточную статистику в модели сдвига-масштаба для экспоненциального распределения, то есть для семейства
    \[
    \mathcal{P} = \left\{\mathsf{P}_{\alpha, \beta}\colon F_{\mathsf{P}_{\alpha, \beta}}(t) = F_0\left(\frac{t-\alpha}{\beta}\right)\right\},\;\;\;F_0(t) = 1 - e^{-t}.
    \]
    Будет ли эта статистика полной?
\end{problem}

\begin{problem}
    Докажите теорему \ref{lehmann_scheffe}.
\end{problem}

\begin{problem}
	В условиях задачи \ref{unbiased-est-for-exp} при помощи полученной в ней несмещённой оценки найти оптимальную оценку функции $e^{-\theta}$.
\end{problem}

\begin{problem}
    Пусть $X_1, \ldots, X_n$ --- н.о.р.с.в. из распределения $\ud(0, \theta)$. Найдите $\me[](X_{(k)} | X_{(1)})$.
\end{problem}

\begin{problem}[Я.Профи]
    Пусть $\xi_1, \ldots, \xi_{10}$ --- н.о.р.с.в. из распределения $\ud(0, 11)$. Обозначим $X = \min(\xi_1, \ldots, \xi_{10})$, $Y = \max(\xi_1, \ldots, \xi_{10})$. Найдите $\va[](X - 4Y)$.
\end{problem}

\begin{problem}
    Пусть $X_1, \ldots, X_n$ --- выборка из одномерного нормального распределения с неизвестными параметрами сдвига и масштаба. Напомним, что величина
    $$\mu_k = \frac{1}{n}\sum_{i=1}^n\left(X_i - \overline{\mathbf X}\right)^k,\;\;\;k \in \N$$
    называется выборочным центральным моментом. Выборочные асимметрия и эксцесс --- это статистики $\mu_3/s^3$ и $\mu_4 / s^4 - 3$, где $s^2$ --- выборочная дисперсия. Доказать, что каждая из них не зависит от $s^2$.
\end{problem}

\begin{problem}\label{cme_is_not_always_estimator}
    Приведите пример параметрического семейства $\mathcal{P} = \{\pth\colon \theta \in \Theta\}$ и оценок $T(\mathbf X)$ и $\widehat\theta(\mathbf X)$ параметра $\theta$ таких, что $\me\left(\left.\widehat\theta(\mathbf X)\right|T(\mathbf X)\right)$ не является статистикой.
\end{problem}

\begin{problem}
    Докажите, что если $T(\mathbf X)$ --- достаточная статистика, то оценка максимального правдоподобия является функцией от $T(\mathbf X)$.
\end{problem}




