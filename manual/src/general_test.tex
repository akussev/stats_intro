\section{Критерии согласия}\label{simple_goodness_of_fit_tests}

В этом параграфе мы рассмотрим самые популярные критерии, которые проверяют, согласуются ли наблюдения с некоторым конкретным теоретическим распределением. Отсюда пошло название \textit{критериев согласия}. В англоязычной литературе распространён другой термин --- \textsf{goodness of fit tests}, который, вообще говоря, охватывает несколько больший класс критериев. Как можно догадаться из перевода, это критерии, проверяющие качество объяснения данных выбранной статистической моделью. Помимо простых гипотез они проверяют, например, принадлежность выбранному семейству распределений (нормальному, экспоненциальному и т.д.). О них речь пойдёт в следующей главе, а пока остановимся на вышеуказанном частном случае.

\subsection{Критерий Колмогорова}

Поставим на проверку гипотезу о том, что наблюдения поступают нам из какого-то непрерывного распределения $\pth[0]$.
\[
H_0\colon \pth[] = \pth[0] \vs H_1\colon \pth[] \ne \pth[0].
\]
Как мы знаем из теоремы Гливенко-Кантелли, эмпирическая функция распределения $\widehat{F}_n$ равномерно сходится к истинной функции распределения $F$ для почти всех выборок $\mathbf X = (X_1, \ldots, X_n, \ldots)$, то есть
\[
D_n = \sup_{x\in \R} |\widehat{F}_n(x) - F(x)| \stackrel{\text{п.н.}}{\longrightarrow} 0.
\]
Таким образом, судить о выполнимости гипотезы $H_0$ можно исходя из того, насколько близко к нулю значение $D_n$. Оказывается, сходимость этой величины к нулю имеет порядок $1/\sqrt{n}$, притом у $\sqrt{n} D_n$ имеется предельное распределение.

\begin{theorem}[label=kolmogorov_theorem]{Колмогоров}{}
Пусть $F$ -- непрерывная функция распределения. Тогда случайная величина $\sqrt{n} D_n$ распределена одинаково вне зависимости от $F$ и слабо сходится к \textit{распределению Колмогорова} с функцией распределения
\begin{equation}\label{kolmogorov_cdf}
	K(t) = 1 + 2\sum_{k=1}^{\infty} (-1)^k e^{-2k^2t^2},\;\;\;t>0.
\end{equation}
\end{theorem}

\begin{proof}
    Докажем лишь инвариантность распределения $\sqrt{n} D_n$. Статистику $D_n$ можно переписать как
\[
D_n = \sup_{x\in\R} |\widehat{F}_n(x) - F(x)| = \sup_{y\in[0;1]} |\widehat{F}_n(F^{-1}(y)) - y|.
\]
Посмотрим, как распределена $\widehat{F}_n(F^{-1}(y))$:
\[
\widehat{F}_n(F^{-1}(y)) = \sum_{i=1}^n I(F^{-1}(y) \ge X_i) = \sum_{i=1}^n I(y \ge F(X_i)),
\]
а $F(X_i)$, как известно из утверждения \ref{f_to_uniform}, распределено равномерно на $[0;1]$. Таким образом, $D_n$ выражается через независимые величины $F(X_i)$ с одним и тем же распределением, поэтому её распределение определено однозначно.
\end{proof}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pic/kolmogorov_distr/kolmogorov_distr.pdf}
    \caption{CDF и PDF распределения Колмогорова}
\end{figure}

Из теоремы следует, что если при большом $n$ статистика $D_n$ достаточно большая, то это является существенным доводом против $H_0$, то есть критерий для проверки этой гипотезы имеет вид
\[
R = \left\{\sqrt{n} D_n \ge k_{1-\alpha}\right\},
\]
где $k_p$ -- $p$-квантиль распределения Колмогорова. Для его нахождения можно либо воспользоваться таблицами с распространёнными квантилями, либо использовать приближение
\[
k_{1-\alpha} \sim \sqrt{-\frac{1}{2}\ln{\frac{\alpha}{2}}} \text{ при } \alpha \to 0.
\]

Слабая сходимость распределений из теоремы Колмогорова позволяет сказать, что сей критерий имеет асимптотический уровень значимости $\alpha$. Что же насчёт других свойств?

\begin{theorem}{}{}
    Критерий Колмогорова состоятелен против общей альтернативы $H_1$.
\end{theorem}
\begin{proof}
Пусть истинная функция распределения равна $G \ne F$. В таком случае статистику $D_n$ можно оценить следующим образом:
\begin{gather*}
    D_n = \sup_{x \in \R} |\widehat{G}_n(x) - F(x)| \ge 
    \sup_{x \in \R} \left[ |G(x) - F(x)| - |\widehat{G}_n(x) - G(x)| \right] \ge\\ \ge  \sup_{x \in \R} |G(x) - F(x)| - \sup_{x \in \R} |\widehat{G}_n(x) - G(x)| = c - D'_n,
\end{gather*}
причём $c = \sup_{x \in \R} |G(x) - F(x)| \ne 0$, а $D'_n = \sup_{x \in \R} |\widehat{G}_n(x) - G(x)| \stackrel{\text{п.н.}}{\longrightarrow} 0$ по теореме Гливенко-Кантелли, поэтому и $D'_n + k_{1-\alpha} / \sqrt{n} \stackrel{\text{п.н.}}{\longrightarrow} 0$. В таком случае
\begin{gather*}
    \pth[](\sqrt{n} D_n \ge k_{1-\alpha}) \ge \pth[](\sqrt{n} (c - D'_n) \ge k_{1-\alpha}) = \pth[](D'_n + k_{1-\alpha} / \sqrt{n} \le c) \to 1.
\end{gather*}

\end{proof}

Осталось только понять, как на практике находить статистику критерия и проверять основную гипотезу. Ключевое наблюдение заключается в том, что теоретическая функция распределения не убывает и по условию непрерывна, а эмпирическая --- кусочно-постоянна. Из этого следует, что супремум в определении $D_n$ достигается либо в точке разрыва $\widehat{F}_n$, либо в левостороннем пределе к точке разрыва. Поэтому справедлива следующая формула:
\[
D_n = \max_{1 \le i \le n} \left\{ \frac{i}{n} - F(X_{(i)}), F(X_{(i)}) - \frac{i - 1}{n} \right\}.
\]
Ещё проще осуществить проверку можно с помощью функции \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html}{\texttt{scipy.stats.kstest}}, которая выведет значение статистики $D_n$ и соответствующий p-value.

\begin{example}
    Одним из существенных недостатков данного критерия является его низкая мощность, то есть он далеко не всегда хорошо улавливает отличия теоретической и эмпирической функций распределения. Особенно это проявляется для <<хвостов>> распределений, различия которых равномерная метрика почти не чувствует. Проиллюстрируем это следующей программой, которая применяет критерий Колмогорова для распределения $\mathcal{N}(0, 1)$, хотя истинное распределение иное.
    \begin{minted}{python}
import scipy.stats as sps
n_iter = 10000
size = 100
alpha = 0.05
distr_set = [sps.norm(scale=2**0.5).cdf, sps.laplace.cdf, sps.cauchy.cdf]
distr_name = ["N(0, 2)", "Laplace", "Cauchy"]
print(f"Type II error at significance level {alpha} for:")
for i, cdf in enumerate(distr_set):
    rvs = sps.norm.rvs(size=(n_iter, size))
    reject_cnt = 0
    for j in range(n_iter):
        result = sps.kstest(rvs[j], cdf)
        reject_cnt += (result.pvalue < alpha)
    print(f"- {distr_name[i]} distribution = {1 - reject_cnt / n_iter}")
    \end{minted}
\begin{lstlisting}
Type II error at significance level 0.05 for:
- N(0, 2) distribution = 0.5734
- Laplace distribution = 0.9443
- Cauchy distribution = 0.073
\end{lstlisting}
\end{example}

\subsection{Критерий $\omega^2$}\label{omega2_tests}

Попробуем подойти к проблеме иначе, применив другую меру различия двух функций. В критерии Колмогорова используется равномерная метрика, недостатки которой мы уже обсудили. Теперь испробуем $L_2$-метрику, которая рассматривает интеграл квадрата разности функций по некоторой мере.

\begin{definition}
    Пусть нам дана некоторая <<весовая>> функция $\psi(t)$ на $[0; 1]$. \textit{Статистикой омега-квадрат} называют
    \[
    \omega^2(\psi) = \int_{\R} \left(\widehat{F}_n(x) - F(x)\right)^2 \psi(F(x)) \, dF(x).
    \]
\end{definition}

Как можно заметить, интеграл берётся по теоретическому распределению, соответствующему $F(x)$. Сделано это для того, чтобы распределение статистики не зависело от $F(x)$ при верности основной гипотезы. Действительно, осуществим замену переменной $y = F(x)$:
\[
\omega^2(\psi) = \int_0^1 \left(\widehat{F}_n(F^{-1}(y)) - y\right)^2 \psi(y) \, d\mu(y),
\]
где $\mu$ --- классическая мера Лебега на $[0; 1]$. Из доказательства теоремы \ref{kolmogorov_theorem} мы знаем, что $\widehat{F}_n(F^{-1}(y))$ распределена одинаково вне зависимости от $F$, а значит и распределение статистики $\omega^2$ одно и то же, что и требовалось

Среди многообразия весовых функций обычно берут следующие:
\[
\psi_1(t) \equiv 1 \text{\;\;\;и\;\;\;} \psi_2(t) = \frac{1}{t(1-t)}.
\]

Выбор именно таких функций оправдывается их простотой и подходом в обнаружении отклонений. В \cite[гл.~12,~\S~2]{lagutin} даётся такое описание:
\begin{quote}
    Первый из них хорошо улавливает расхождение между $\widehat{F}_n$ и $F$ в области <<типичных значений>> случайной величины с функцией распределения $F$ (часто он оказывается более чувствительным, чем критерий Колмогорова). Второй же, благодаря тому, что $\psi_2(y)$ быстро возрастает при $y \to 0$ и $y \to 1$, способен заметить различие <<на хвостах>> распределения $F$, которому придается дополнительный вес.
\end{quote}

Как и в случае со статистикой критерия Колмогорова, статистика омега-квадрат, только уже домноженная на $n$, имеет некоторый предельный закон.

\begin{theorem}{}{}
    При верности гипотезы $H_0$ статистики $n\omega^2(\psi_1)$ и $n\omega^2(\psi_2)$ слабо сходятся к некоторым фиксированным распределениям $F_1$ и $F_2$ соответственно.
\end{theorem}

У сих распределений также имеется разложение в ряд, но оно весьма ужасное, чтобы приводить его здесь. Если положить $y_p$ и $z_p$ за $p$-квантили $F_1$ и $F_2$ соответственно, то получатся два асимптотических критерия с уровнем значимости $\alpha$:
\begin{gather*}
    R_1 = \left\{n\omega^2(\psi_1) > y_{1-\alpha}\right\} \textit{ --- критерий Крамера — фон Мизеса — Смирнова}\\
    R_2 = \left\{n\omega^2(\psi_2) > z_{1-\alpha}\right\} \textit{ --- критерий Андерсона — Дарлинга}
\end{gather*}
\begin{wrapfigure}{o}{0.6\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
     $\alpha$ & 0.5 & 0.15 & 0.1 & 0.05 & 0.025 & 0.01 & 0.001 \\
     \hline
     $y_{1-\alpha}$ & 0.12 & 0.28 & 0.35 & 0.46 & 0.58 & 0.74 & 1.17 \\
     \hline
     $z_{1-\alpha}$ & 0.77 & 1.62 & 1.94 & 2.49 & 3.08 & 3.88 & 5.97\\
     \hline
\end{tabular}
\end{wrapfigure}

Некоторые квантили этих распределений приведены в таблице. Как на практике вычислять значение статистики омега-квадрат? Как и в случае критерия Колмогорова, в силу кусочно-постоянности $\widehat{F}_n$ можно упростить интеграл выше и получить следующие более приятные формулы:
\begin{gather*}
n\omega^2(\psi_1) = \frac{1}{12n} + \sum_{i=1}^n \left[F(x_{(i)}) - \frac{2i-1}{2n}\right]^2,\\
n\omega^2(\psi_2) = -n - 2 \sum_{i=1}^n \left[ \frac{2i-1}{2n} \ln{F(x_{(i)})} + \left(1 - \frac{2i-1}{2n}\right) \ln{(1 - F(x_{(i)}))} \right].
\end{gather*}

В случае критерия Крамена-фон Мизеса-Смирнова имеется реализация проверки гипотезы \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.cramervonmises.html#reed8bce1e242-1}{\texttt{scipy.stats.cramervonmises}}. К сожалению, для критерия Андерсона-Дарлинга реализации не предусмотрено.

\begin{example}\label{compare_of_tests}
    Сравним рассмотренные ранее критерии на примере выборки из распределения Коши, которую мы будем проверять на нормальность. На рисунке \ref{heavy_tails_kstest} приведены графики теоретической функции распределения $\mathcal{N}(0, 1)$ и эмпирической функция распределения, построенной по сгенерированной выборке.
    
    Чтобы ещё лучше <<обмануть>> критерии, мы взяли распределение Коши с коэффициентом масштаба $0.5$. Даже визуально видно значимое отличие полученных функций, но посмотрим, что скажут критерии на уровне значимости, скажем, 0.05.
    \begin{minted}{python}
n = 100
rvs = sps.cauchy(scale=0.5).rvs(size=n, random_state=73)
cdf = sps.norm.cdf
# Применим критерий Колмогорова
pvalue = sps.kstest(rvs, cdf).pvalue
print(f"Kolmogorov test's pvalue - {pvalue}")
# Применим критерий Крамера-фон Мизеса-Смирнова
pvalue = sps.cramervonmises(rvs, cdf).pvalue
print(f"Cramer-von Mises-Smirnov's pvalue - {pvalue}")
# Применим критерий Андерсона-Дарлинга
ordered = np.sort(rvs)
stat = -n
for i in range(1, n + 1):
    stat -= 2 * (2 * i - 1) * np.log(cdf(ordered[i - 1])) / (2 * n)
    stat -= 2 * (1 - (2 * i - 1) / (2 * n)) * np.log(1 - cdf(ordered[i - 1]))
print(f"Anderson-Darling test's staticstic - {stat}")
    \end{minted}
\end{example}
\begin{lstlisting}
Kolmogorov test's pvalue - 0.18241574592222012
Cramer-von Mises-Smirnov's pvalue - 0.1267607929221075
Anderson-Darling test's staticstic - inf
\end{lstlisting}
\begin{figure}[H]
    \centering
    \includegraphics{pic/test_compare/test_compare.pdf}
    \caption{Распределение из нулевой гипотезы, отличное от настоящего}\label{heavy_tails_kstest}
\end{figure}

Первые два критерия не отвергают основную гипотезу. Критерий Колмогорова не видит, что функции распределения, вообще говоря, отличаются <<много где>>, он лишь видит максимальное отклонение, которое его устраивает. Критерий Крамена-фон Мизеса-Смирнова справляется не лучше: в нём мы берём интеграл по мере, соответствующей нормальной $F(x)$, которая чрезвычайно мала на хвостах, из-за чего различие там имеет малый вес. В то же время критерий Андерсона-Дарлинга с его специальной весовой функцией это различие обнаружил, отчего значение статистики получилось чрезвычайно большим, то есть при данном критерии гипотеза отвергается.

\subsection{Критерий $\chi^2$ Пирсона}\label{pearson_chi2_test}

Рассмотрим наблюдение из \href{https://en.wikipedia.org/wiki/Multinomial_distribution}{мультиномиального распределения} с параметрами $n$, $k$ и $\mathbf{p} = (p_1, \ldots, p_k)$, которое по сути является обобщением биномиального. Проще говоря, у нас есть $k$-гранный кубик, выпадение $i$-ой грани которого происходит с вероятностью $p_i$; мы кидаем этот кубик $n$ раз и записываем в вектор $\mathbf X = (X_1, \ldots, X_k)$, что первая грань выпала $X_1$ раз, вторая --- $X_2$ раз и т. д. Компоненты этого вектора можно записать как
\[
X_i = \sum_{j=1}^n I(B_j = i),
\]
где $B_j$ -- результат $j$-ого броска, причём величины $B_1, \ldots, B_n$ независимы в совокупности.

Пусть мы наблюдаем вектор $(X_1, \ldots, X_k)$ с таким распределением и хотим проверить гипотезу
\[
H_0\colon \mathbf{p} = \mathbf{p}^0 = (p_1^0, \ldots, p_k^0) \vs H_1\colon \mathbf{p} \ne \mathbf{p}^0.
\]
По ЦПТ мы знаем, что при выполнимости гипотезы $H_0$
\begin{equation}\label{chi2_conv}
    \frac{\mathbf X - n \mathbf{p}^0}{\sqrt{n}} \stackrel{d_{\mathbf{p}_0}}{\longrightarrow} \boldzeta \sim \mathcal{N}(0, \Sigma),
\end{equation}
где $\Sigma$ --- матрица ковариаций вектора
\[
\begin{pmatrix}
    I(B_1 = 1)\\
    \ldots\\
    I(B_1 = k)
\end{pmatrix}
\]
Её элементы несложно высчитать: 
\begin{gather*}
    \begin{aligned}
        \Sigma_{ii} = \va[] I(B_1 = i) = p_i^0(1 - p_i^0)\\
    \Sigma_{ij} = \cov{(I(B_1 = i), I(B_1 = j))} = -p_i^0 p_j^0,\;\;i\ne j
    \end{aligned}
    \Longrightarrow
    \Sigma =
\begin{pmatrix}
p_1^0 & 0 & \cdots & 0\\
0 & p_2^0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & p_n^0
\end{pmatrix} - \mathbf{p}^0 {\mathbf{p}^0}^T.
\end{gather*}
Таким образом, компоненты вектора в левой части \eqref{chi2_conv}, то есть
\[
\frac{X_i - n p_i^0}{\sqrt{n}},
\]
распределены почти что нормально. Тогда давайте в качестве меры отклонения от гипотезы $H_0$ возьмём взвешенную сумму квадратов компонент этого вектора:
\begin{definition}
\textit{Статистикой хи-квадрат Пирсона} называется
\begin{equation}\label{chi2_stat}
	\chi^2(\mathbf X) = \sum_{i=1}^k \frac{(X_i - n p_i^0)^2}{n p_i^0}.
\end{equation}
\end{definition}
Логично предположить, что как сумма квадратов почти что нормально распределённых величин эта статистика стремится по распределению к хи-квадрат.

\begin{theorem}{}{}
Если $H_0$ верна, то $\chi^2(\mathbf X) \stackrel{d}{\longrightarrow} \chi^2_{k-1}$.
\end{theorem}

\begin{proof}
    Положим $\xi_i = \zeta_i / \sqrt{p_i^0}$. Применим к сходимости \eqref{chi2_conv} непрерывную функцию $f(x_1, \ldots, x_k) = \sum_{i=1}^k \frac{x_i^2}{p_i^0}$. По теореме о наследовании сходимости получим, что
    \[
    \chi^2(\mathbf X) \stackrel{d_{\mathbf{p}_0}}{\longrightarrow} f(\boldzeta) = \sum_{i=1}^n \xi_i^2.
    \]
    Следовательно, достаточно показать, что если $\boldzeta \sim \mathcal{N}(0, \Sigma)$ (где $\Sigma$ была посчитана ранее), то $f(\boldzeta) \sim \chi^2_{k-1}$.
    
    Несложно посчитать, какова будет ковариационная матрица для случайного вектора $\boldxi$:
\[
\Sigma' = E_n - \mathbf{r} \mathbf{r}^T,
\]
где $r_i = \sqrt{p_i^0}$, $E_n$ -- единичная матрица размера $n$.

Так как вектор $\mathbf r$ имеет единичную длину, то существует ортогональное отображение $S$, переводящее $\mathbf r$ в базисный вектор $\mathbf e_n = (0, \ldots, 0, 1)^T$. Тогда
$$S\Sigma'S^T = SE_nS^T - (S\mathbf r)(S\mathbf r)^T = E_n - \mathbf e_n \mathbf e_n^T = \diag(\underbrace{1, 1, \ldots, 1}_{\text{$n-1$ единиц}}, 0) =: D.$$

Вектор $\boldeta = S \boldxi$ как линейное преобразование над гауссовским вектором имеет распределение $\mathcal{N}(0, S\Sigma'S^T) = \mathcal{N}(0, D)$. Тогда его компоненты не коррелированы, а значит, независимы (по свойству гауссовского вектора), причём одна из координат распределена как нуль из-за одного нуля на диагонали, а остальные --- стандартно нормально. Ортогональное преобразование не меняет норму вектора, поэтому
\[
\sum_{i=1}^n \xi_i^2 = \sum_{i=1}^n \eta^2_i = \sum_{i=1}^{n-1} \eta^2_i \sim \chi^2_{n-1}
\]
как сумма квадратов независимых величин с распределением $\mathcal{N}(0, 1)$.
\end{proof}

Итого, в качестве критерия проверки $H_0$ асимптотического уровня значимости $\alpha$ можно взять
\[
R = \left\{\mathbf{x}\colon \chi^2(\mathbf{x}) > \chi^2_{k-1,1-\alpha}\right\},
\]
где $\chi^2_{k-1, p}$ -- $p$-квантиль распределения $\chi^2_{k-1}$. Следует помнить, что этот критерий --- асимптотический, а значит, его использование при малой выборке не имеет смысла. Обычно критерий $\chi^2$ используют при $n \ge 50$ и $np^0_i \ge 5$ для всех $i=1,\ldots, k$.

\begin{example}[третий закон Менделя]
Согласно наблюдениям, проведённым биологом Г. Менделем, разные признаки наследуются независимо друг от друга. Попробуем убедиться в этом статистически.

Предположим, у семейства гороха имеется два признака: цвет (жёлтый и зелёный) и форма (круглая или морщинистая). Скрещиваются два вида гороха: с доминантными признаками (жёлтые круглые горошины) и рецессивными (зелёные морщинистые горошины). По отдельности в результате селекции признаки распределяются в отношении $3:1$ (по второму закону Менделя), поэтому если третий закон Менделя верен, то распределение двух признаков будет иметь вид $9:3:3:1$.

Проведено $n = 556$ наблюдений. Посмотрим на эту статистику:
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    Тип горошин & Гипотетическая вероятность & Наблюдаемая частота\\
    \hline
    Желтые, круглые & $9/16$ & $315/556$\\
    \hline
    Желтые, морщинистые & $3/16$ & $101/556$\\
    \hline
    Зелёные, круглые & $3/16$ & $108/556$\\
    \hline
    Зелёные, морщинистые & $1/16$ & $32/556$\\
    \hline
    \end{tabular}
\end{center}

В наших обозначениях это значит, что реализация выборки равна $\mathbf x = (315, 101, 108, 32)$, и проверяется гипотеза
\[
H_0\colon \mathbf{p} = \mathbf{p}^0 = (9/16, 3/16, 3/16, 1/16).
\]
Посчитаем статистику Пирсона:
\begin{multline*}
    \chi^2(\mathbf x) = \frac{(315 - 556 \cdot 9/16)^2}{556 \cdot 9/16} + \frac{(101 - 556 \cdot 3/16)^2}{556 \cdot 3/16} + \\
    + \frac{(108 - 556 \cdot 3/16)^2}{556 \cdot 3/16} + \frac{(32 - 556 \cdot 1/16)^2}{556 \cdot 1/16} \approx 0.47.
\end{multline*}
Если в качестве допустимого уровня значимости взять $\alpha = 0.05$, то пороговым значением для критерия Пирсона будет $(1-\alpha)$-квантиль для $\chi^2_3$, что есть примерно $7.815$. Наблюдаемое значение гораздо меньше порогового значения, а значит, причин для отвержения гипотезы $H_0$ нет.
\end{example}

\begin{example}
Среди первых 800 цифр числа $\pi$ цифры 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 встречаются соответственно 74, 92, 83, 79, 80, 73, 77, 75, 76, 91 раз. Проверим при помощи критерия хи-квадрат гипотезу о том, что различные цифры встречаются в числе $\pi$ равновероятно, на уровне значимости $\alpha = 0.05$.

Решим задачу с помощью функции \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html#scipy.stats.chisquare}{\texttt{scipy.stats.chisquare}} на языке Python. В качестве аргументов она принимает массив наблюдаемых частот $(X_1, \ldots, X_k)$ и массив ожидаемых частот $(np_1^0, \ldots, np_k^0)$ (по умолчанию $p_1^0 = \ldots = p_k^0 = 1/n$).

\begin{minted}{python}
obs = np.array([74, 92, 83, 79, 80, 73, 77, 75, 76, 91])
result = sps.chisquare(obs)
alpha = 0.05
threshold = sps.chi2(len(obs)-1).ppf(1 - alpha) # Считаем квантиль chi^2_9
print(f"Chi2 statistic = {result.statistic}")
print(f"pvalue = {result.pvalue}")
print(f"Threshold = {threshold}")
\end{minted}
\begin{lstlisting}
Chi2 statistic = 5.125
pvalue = 0.8232783432788753
Threshold = 16.918977604620448
\end{lstlisting}
Как можно видеть, значение статистики критерия мало по сравнению с критическим значением, да и p-value весьма велико, поэтому гипотеза не отвергается.
\end{example}

\begin{wrapfigure}{l}{0.45\textwidth}
\vspace{-14pt}
    \includegraphics[width=0.45\textwidth]{pic/chi2_test/chi2_test.pdf}
\end{wrapfigure}

Отметим, что критерий $\chi^2$ применяется далеко не только к модели выше. Он также позволяет проверять гипотезы о равенстве истинной функции распределения какой-то данной. Как же это происходит?

Пусть нам выборка $X_1, \ldots, X_n$ из некоторого неизвестного нам распределения $F(x)$. Мы же в свою очередь хотим проверить, не равна ли она чему-то хорошему, то есть проверяем
\[
H_0\colon F(x) = F_0(x).
\]

Разобьём числовую прямую на $k$ дизъюнктных множеств $\Delta_1, \ldots, \Delta_k$ (чаще всего берут полуинтервалы $\Delta_i = (a_i; b_j]$, возможно и бесконечные). В данные интервалы как-то попали наши точки: пусть в $i$-ое множество $\Delta_i$ попало $v_i$ точек. При верности $H_0$ вероятность попасть в $\Delta_i$ равна $p_i^0=\int_{\Delta_i}dF_0(x) = F_0(b_i) - F_0(a_i)$. Это и сводит текущую задачу к задаче выше: для каждого элемента выборки на гранях $k$-гранного кубика написано, в какой полуинтервал оно попадёт, и гипотеза заключается в том, что вероятность выпадания определённой грани равна установленному числу. Получается, критерий имеет вид
\[
R = \left\{\sum_{i=1}^k\frac{(v_i-np_i^0)^2}{np_i^0} > \chi^2_{k-1,1-\alpha}\right\}.
\]

Конечно же, если критерий $\chi^2$ не отверг гипотезу, то нам это ровным счётом ни о чём не говорит. Мы могли разделить прямую как-то не очень удачно, из-за чего истинное распределение может легко мимикрировать под данное, имея одинаковые с ним вероятности промежутков $\Delta_i$. Отсюда представляется логичным брать не слишком мало интервалов, чтобы мы смогли обнаружить различия между распределениями. Но и слишком маленькими их делать не следует, потому что тогда в некоторые интервалы может в теории не попасть ни одна точка, что на корню убивает предположение о нормальности $(v_i - np^0_i)/\sqrt{n}$. Обычно берут $k \approx \log_2{n}$. 

\begin{example}
    Рассмотрим выборку из примера \ref{compare_of_tests}, которая на самом деле имеет распределение $\cauchyd(0, 0.5)$, но мы проверяем гипотезу $H_0\colon F = \Phi$, то есть что данные распределены стандартно нормально. Разобьём носитель распределения на четыре равновероятные (по теоретическому распределению) части, что можно сделать с помощью функции \texttt{scipy.stats.norm.ppf}, находящей квантили. Для простоты будем использовать функцию \href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html}{\texttt{pandas.cut}}, которая сама разделит выборку по <<бинам>>. Как можно видеть, даже немощный $\chi^2$-критерий смог найти отклонения в отличие от критерия Колмогорова.
    \begin{minted}{python}
n = 100
rvs = sps.cauchy(scale=0.5).rvs(size=n, random_state=73)
bins_count = 4
quantiles = sps.norm.ppf(np.linspace(0, 1, num=bins_count+1))
partition = pd.cut(rvs, quantiles).__array__()
intervals, obs = np.unique(partition, return_counts=True)
for interval, count in zip(intervals, obs):
    print(f"There is {count} dots in {interval}")
print("P-value is", sps.chisquare(obs).pvalue)
    \end{minted}
    \begin{lstlisting}
There is 19 dots in (-inf, -0.674]
There is 27 dots in (-0.674, 0.0]
There is 36 dots in (0.0, 0.674]
There is 18 dots in (0.674, inf]
P-value is 0.0384293188578885
    \end{lstlisting}
\end{example}

\subsection*{Задачи}

\begin{problem}
	Согласно \textit{закону Бенфорда}, первая цифра $\xi_1$ случайного числа $\xi = \overline{\xi_1 \ldots \xi_n}$ из достаточно широко диапазона имеет распределение
	\[
	\mathsf{P}(\xi_1 \le d) = \log_{10}{(d+1)}, \;\;\; d \in \{1, \ldots, 9\}.
	\]
	
	Для выборки из стран мира (данные можно взять, например, \href{https://www.kaggle.com/datasets/tanuprabhu/population-by-country-2020/data}{отсюда}) и уровня значимости $0.05$ проверить гипотезу о том, что численность населения подчиняется закону Бенфорда.
\end{problem}

\begin{problem}\label{distance_equivalence}
    Статистика $\chi^2$ по сути своей является некоторой мерой расхождения между теоретическим распределением и эмпирическим, полученным по наблюдаемым частотам. В качестве расстояния между распределениями можно использовать и другие аналоги, которые, впрочем, будут в некотором смысле эквивалентны исходному. Вам предлагается это проверить.
    
    Напомним, что \textit{дивергенцией Кульбака-Лейблера} двух дискретных распределений $\mathsf{P} = (p_1, \ldots, p_k)$ и $\mathsf{Q} = (q_1, \ldots, q_k)$ называется величина
    \[
    D(\mathsf{P} \parallel \mathsf{Q}) = \sum_{i=1}^k p_i \cdot \log\frac{p_i}{q_i}.
    \]

    Пусть $k$-гранный кубик с вероятностями выпадения $i$-ой грани $p_i^0$ подкидывают $n$ раз, $(\nu_1, \ldots, \nu_k)$ --- наблюдаемые частоты, а $p_i = \nu_i / n$. Докажите, что
    \[
    2n \cdot D(\mathsf{P} \parallel \mathsf{P}^0) \stackrel{d}{\rightarrow} \chi^2_{k-1},\;\;\;n\to \infty.
    \]
    Что можно сказать про сходимость статистики $2n \cdot D(\mathsf{P}^0 \parallel \mathsf{P})$? Какую из этих статистик лучше использовать в качестве основы критерия?
\end{problem}

\begin{problem}
    Модифицируйте критерий Колмогорова таким образом, чтобы, во-первых, в качестве основной гипотезы $H_0\colon \mathsf{P} = \mathsf{P}_0$ можно было выбрать произвольное распределение $\mathsf{P}_0$ (необязательно непрерывное), и, во-вторых, критерий остался состоятельным против альтернативы $H_1\colon \mathsf{P} \ne \mathsf{P}_0$.
\end{problem}

