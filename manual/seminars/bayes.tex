\section{Байесовские оценки}

\subsection{Мотивация и определения}

Всё это время мы не воспринимали множество параметров как нечто, имеющее сложную структуру. Максимум мы пользовались какими-то свойствами $\R^n$, откуда чаще всего и приходят параметры, но дополнительными свойствами или информацией множество $\Theta$ мы не наделяли. Это может показаться логичным, весь если мы не знаем истинного значения параметра, то и само множество его теоретических значений скорее всего также покрыто туманом войны. Но в реальной жизни мы не только имеем дело с <<приятными>> множествами допустимых параметров, но также имеем некоторую информацию о них.

Рассмотрим пример с распределением Вейбулла из второй домашки по практикуму. В ней мы убедились, что состояние банковского счёта имеет вышеуказанное распределение, и даже с помощью ОМП находили оценку для параметра этого распределения. Но ведь этот параметр может быть разным для разных счетов, он зависит от человека, который этим счётом владеет. Стало быть, параметр в данной ситуации -- это не что-то фиксированное, что осталось лишь оценить, а в каком-то смысле \textit{случайная величина}, которая к тому же обладает некоторым \textit{распределением}. И если мы знаем это распределение или догадываемся о его природе, исходя из опыта предыдущих наблюдений, то это может помочь в оценке очередного параметра в очередной такой модели.

Это подводит нас к \textit{байесовскому подходу} в оценивании параметра, в котором известную информацию о нём мы заключаем в некотором распределении $\mathsf{Q}$ на множестве $\Theta$ (чаще всего под $\Theta$ будет подразумеваться множество из $\R^n$, поэтому эта мера определяется на борелевских подмножествах из $\mathcal{B}(\Theta)$, если не обговорено иного).

Более формально, теперь мы работаем в новом вероятностном пространстве $$(\Theta\times\mathcal{X}, \mathcal{B}(\Theta) \otimes\mathcal{B}(\mathcal{X}), \widetilde{\pth[]}),$$ где $\mathcal{B}(\Theta) \otimes\mathcal{B}(\mathcal{X})$ -- прямое произведение сигма-алгебр, а мера $\widetilde{\pth[]}$ задаётся обобщённой плотностью $\rho_{\theta, X}(t, \mathbf{x}) = q(t)\cdot \rho_t(\mathbf{x})$. Обобщённая плотность $\rho_t(\mathbf{x})$, как и ранее, отвечает за распределение выборки при фиксированном значении параметра, а новый персонаж -- $q(t)$ -- за распределение на множестве параметров.

\begin{definition}
    Плотность $q(t)$, $t \in \Theta$, называется \textit{априорной}.
\end{definition}

\textit{Априори} означает то, что эта плотность известна нам \textit{до} момента проведения наблюдения, то есть она является чем-то типа прикидки того, каким может быть параметр.

При этом когда наблюдение уже проведено, ясно, что наше мнение о параметре изменилось~ -- выборка подсказывает нам, в какую сторону нужно идти, чтобы оценить параметр. 

\begin{definition}
    Условная плотность параметра $\theta$ при условии выборки $X_1, \ldots, X_n$, которая (напоминаем) может быть высчитана по формуле
    \[
    \rho_{\theta | X}(t | \mathbf{x}) = \frac{\rho_{\theta, X}(t, \mathbf{x})}{\rho_{X}(\mathbf{x})} = \frac{q(t) \rho_{t}(\mathbf{x})}{\int_{\Theta} q(s) \rho_{s}(\mathbf{x}) \,d\mu(s)},
    \]
    называется \textit{апостериорной плотностью} параметра $\theta$.
\end{definition}

Из полученной плотности теперь можно смастерить оценку. Обычно берут среднее значение по плотности, что есть попросту $$\me[](\theta | X) = \int_{\Theta} t\cdot \rho_{\theta|X}(t|X)\,dt.$$ Обратите внимание, что так как УМО по определению является измеримым относительно $X$, то $\me[](\theta | X) = \phi(X)$ для некоторой борелевской $\phi$, то есть она зависит только от элементов выборки.

\begin{definition}
    Оценка $\widehat{\theta} = \me[](\theta | X)$ называется \textit{байесовской оценкой параметра $\theta$}.
\end{definition}

Польза полученной оценки проявляется в свете следующего определения.

\begin{definition}
    Говорят, что оценка $\theta^*$ \textit{лучше оценки} $\widehat{\theta}$ \textit{в байесовском подходе с функцией потерь $g$}, если
    \[
    \int_{\Theta} \me[t] g(\theta^*, t) \,d\mu(t) < \int_{\Theta} \me[t] g(\widehat \theta, t)\, d\mu(t).
    \]
\end{definition}

\begin{theorem*}
    Байесовская оценка является наилучшей в байесовском подходе с квадратичной функции потерь.
\end{theorem*}

\subsection{Выбор априорного распределения}

Звучит просто прекрасно. Но остаётся важный вопрос: а откуда нам брать это априорное распределение параметра? Как было сказано ранее, можно воспользоваться результатами прошлых наблюдений, но так можно сделать не всегда. Хочется иметь некоторый теоретический арсенал, позволяющий даже <<вслепую>> выбирать не очень уж плохие априорные распределения. Вот некоторые способы:
\setcounter{secnumdepth}{2}
\subsubsection{Метод I. Сопряжённые семейства}

Было бы неплохо при переходе от априорного распределения к апостериорному получать не какую-то жесть, а что-то похожее на предыдущее распределение, хоть и с другими параметрами, что, к слову, поможет с дальнейшими вычислениями. Поэтому можно по распределению, которому подчиняется выборка, подобрать априорное распределение так, чтобы оно вместе с апостериорным лежали в одном семействе распределений.
\begin{definition}
    В таком случае семейство распределений, которому принадлежит $\mathsf{Q}$, называют \textit{сопряжённым семейству} $\{\pth\colon \theta \in \Theta\}$.
\end{definition}
\textbf{Пример.} Рассмотрим выборку $X_1, \ldots, X_n$ из распределения $Bern(\theta)$. Её совместная плотность равна
\[
\rho_{\theta}(\mathbf{x}) = \theta^{\sum x_i} (1 - \theta)^{n - \sum x_i}.
\]
Чтобы получить апостериорную плотность, надо домножить $\rho_{t}(\mathbf{x})$ на априорную плотность и потом отнормировать это произведение, деля на некоторый интеграл. Таким образом, $\rho_{\theta | X}(t | \mathbf{x})$ пропорциональна $q(t) \rho_{t}(\mathbf{x})$, а стало быть, надо подобрать семейство для $q(t)$ таким образом, чтобы домножение на $\rho_{t}(\mathbf{x})$ не выкидывало нас за границы этого семейства. Внимательно смотря на табличку известных распределений и находя там что-то со степенями $t$ и $(1 - t)$, можно прийти к выводу, что следует взять в качестве априорного распределения $Beta(\alpha, \beta)$, то есть положить
\[
q(t) = \frac{1}{B(\alpha, \beta)}t^{\alpha-1}(1-t)^{\beta-1} I(0 \le t \le 1).
\]
В таком случае
\[
\rho_{\theta | X}(t | \mathbf{x}) \sim t^{\alpha + \sum x_i - 1} (1-t)^{\beta + n - \sum x_i - 1} I(0 \le t \le 1),
\]
поэтому эта плотность отвечает бета-распределению с параметрами $\alpha + \sum x_i$ и $\beta + n - \sum x_i$.

{\footnotesize Заметьте, что нам не надо находить коэффициент пропорциональности, то есть тот самый интеграл в знаменателе апостериорной плотности, так как при фиксированной выборке это просто какая-то константа, служащая для нормировки (чтобы интеграл от плотности был равен единице), и тем самым определяющаяся однозначно. А мы уже знаем одно распределение, плотность которого с точностью до константы равна правой части -- это и есть бета-распределение, а значит, именно ему равно апостериорное распределение. Ниже мы часто будем писать апостериорную плотность через значок $\sim$, забивая на все множители, которые не зависят от $t$.}

Вспоминаем матожидание бета-распределения и находим байесовскую оценку
\[
\widehat{\theta} = \me[] (\theta | X) = \int_{\Theta} t \rho_{\theta | X}(t | X)\,d\mu(t) = \frac{\alpha + \sum X_i}{\left(\alpha + \sum X_i\right) + \left(\beta + n - \sum X_i\right)} = \frac{\alpha + \sum X_i}{\alpha + \beta + n}.
\]

\begin{problem}\label{conjugate}
    Пусть $X_1, \ldots, X_n$ -- выборка из распределения \textbf{(а)} $U(0, \theta)$, \textbf{(б)} $Pois(\theta)$, \textbf{(в)} $\mathcal{N}(\theta, 1)$, \textbf{(г)} $\mathcal{N}(0, \theta)$. Подберите сопряжённое распределение и найдите байесовскую оценку параметра $\theta$. В качестве точечных оценок возьмите математическое ожидание апостериорных распределений и проверьте их на состоятельность.
\end{problem}
\begin{solution}
    \textbf{(а)} Имеем совместную плотность $\rho_{t}(\mathbf{x}) = t^{-n} I(0 < x_1, \ldots, x_n < t)$. Какое распределение имеет плотность от $t$, которая содержит степени $t$ и индикатор с оценкой $t$ снизу? Конечно же распределение Парето! Положим
\[
q(t) = \frac{ka^k}{t^{k+1}}I(t > a).
\]
В таком случае
\[
\rho_{\theta|X}(t|\mathbf{x}) \sim \frac{1}{t^n}\cdot \frac{ka^k}{t^{k+1}} I(0 < x_1, \ldots, x_n, a < t) \sim \frac{1}{t^{n+k+1}} I(t > \max\{x_{(n)}, a\}).
\]
Следовательно, апостериорным распределением является $Pareto(n+k, \max\{x_{(n)}, a\})$. Тогда искомая байесовская оценка равна
\[
\me[](\theta|X) = \int_{\max\{X_{(n)}, a\}}^{+\infty} \frac{(n+k)\cdot\max\{X_{(n)}, a\}^{n+k}}{t^{n+k}}\,dt = \frac{(n+k)\max\{X_{(n)}, a\}}{n+k-1}.
\]
При $\theta < a$ имеем плачевную ситуацию: элементы выборки не могут быть больше $a$, а значит, оценка не будет вообще зависеть от выборки, поэтому и состоятельности её не видать.

\textbf{(б)} Имеем совместную плотность $$\rho_{t}(\mathbf{x}) = \frac{t^{\sum x_i}e^{-tn}}{\prod x_i!}.$$
Какое распределение имеет плотность от $t$, которая содержит степени $t$ и экспоненту от $-t$? Конечно же гамма-распределение! Положим
\[
q(t) = \frac{\lambda^{\alpha} t^{\alpha - 1}}{\Gamma(\alpha)} e^{-\lambda t} I(t>0).
\]
В таком случае
\[
\rho_{\theta|X}(t|\mathbf{x}) \sim t^{\alpha - 1 + \sum x_i} e^{-t(\lambda+n)}I(t > 0).
\]
Следовательно, апостериорным распределением является $\Gamma\left(\alpha + \sum x_i, \lambda + n\right)$. Тогда искомая байесовская оценка равна
\[
\me[](\theta|X) = \frac{\alpha + \sum X_i}{\lambda + n}.
\]
Какими бы ни были $\alpha$ и $\lambda$, с ростом $n$ их <<влияние>> на оценку падает, и она будет асимптотически эквивалентна $\overline{X}$, что является состоятельной оценкой.

\textbf{(в)} Имеем совместную плотность
$$\rho_{t}(\mathbf{x}) = \frac{1}{(2\pi)^{n/2}} \exp{\left(-\frac{1}{2} \sum (x_i - t)^2\right)} = \frac{1}{(2\pi)^{n/2}} \exp{\left(-\frac{1}{2} \sum x_i^2 + t\sum x_i - \frac{nt^2}{2}\right)}.$$
Какое распределение имеет плотность от $t$, которая содержит экспоненту с $t$ и $t^2$? Конечно же нормальное распределение! Положим
\[
q(t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left(-\frac{1}{2\sigma^2} (t - a)^2\right)} = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left(-\frac{1}{2\sigma^2} t^2 + \frac{a}{\sigma^2}t - \frac{a^2}{2\sigma^2}\right)}.
\]
В таком случае
\[
\rho_{\theta|X}(t|\mathbf{x}) \sim \exp{\left[-\frac{1}{2}t^2\left(n+\frac{1}{\sigma^2}\right) + t\left(\sum x_i + \frac{a}{\sigma^2}\right)\right]}.
\]
Следовательно, апостериорным распределением является $\mathcal{N}\left(\widehat{a}, \widehat{\sigma^2}\right)$. Осталось только понять, чему равны $\widehat{a}$ и $\widehat{\sigma^2}$. Как видно из записи плотности $q(t)$, коэффициент перед $t^2$ в плотности нормального распределения должен быть равен $-1/2\widehat{\sigma^2}$, а перед $t$ ---  $\widehat{a} / \widehat{\sigma^2}$. Это даёт нам следующую систему уравнений:
\[
\left\{
\begin{aligned}
&\frac{\widehat{a}}{\widehat{\sigma^2}} = \sum X_i + \frac{a}{\sigma^2},\\
&\frac{1}{\widehat{\sigma^2}} = n + \frac{1}{\sigma^2}.
\end{aligned}
\right.
\]
Отсюда находим, что
\[
\me[](\theta|X) = \frac{\sum X_i + a/\sigma^2}{n + 1/\sigma^2}.
\]
Как и в пункте \textbf{(б)}, байесовская оценка асимптотически эквивалентна $\overline{X}$, которая состоятельна.

\textbf{(г)} Имеем совместную плотность
$$\rho_{t}(\mathbf{x}) = \frac{1}{(2\pi\theta)^{n/2}} \exp{\left(-\frac{1}{2\theta} \sum x_i^2\right)}.$$
Какое распределение имеет плотность от $t$, которая содержит отрицательные степени $t$ и экспоненту от $1/t$? Конечно же обратное гамма-распределение!.. А, ну да, тут уже не совсем очевидно. Будем говорить, что величина имеет \textit{обратное гамма-распределение с параметрами $\lambda$ и $\alpha$}, если её плотность равна
\[
q(t) = \frac{\lambda^{\alpha} t^{-\alpha - 1}}{\Gamma(\alpha)} e^{-\lambda/t} I(t>0).
\]
Его и возьмём за априорное распределение. В таком случае
\[
\rho_{\theta|X}(t|\mathbf{x}) \sim t^{-\alpha - 1 - n/2} \exp{\left[-\frac{1}{t}\left(\lambda + \frac{1}{2}\sum x_i^2\right)\right]} I(t > 0).
\]
Следовательно, апостериорным распределением является $\text{Inv-Gamma}\left(\alpha + n / 2, \lambda + \frac{1}{2}\sum x_i^2\right)$. Чтобы найти байесовскую оценку, для начала поймём, как выглядит матожидание у $\xi \sim \text{Inv-Gamma}(a, b)$:
\begin{gather*}
    \me[]\xi = \int_0^{+\infty} t \cdot \frac{b^{a} t^{-a - 1}}{\Gamma(a)} e^{-b/t}\,dt = \\
    = \int_0^{+\infty} \frac{b^{a} t^{-a +2} e^{-b/t}}{\Gamma(a)}\cdot \frac{1}{t^2}\,dt = \left[s = \frac{1}{t}\right] = \int_0^{+\infty} \frac{b^{a} s^{a - 2} e^{-bs}}{\Gamma(a)}\,ds =\\
    = \frac{b\Gamma(a-1)}{\Gamma(a)} \cdot \underbrace{\int_0^{+\infty} \frac{b^{a-1} s^{a - 2} e^{-bs}}{\Gamma(a-1)}\,ds}_{\text{интеграл плотности }\Gamma(a-1, b)} = \frac{b\Gamma(a-1)}{\Gamma(a)} = \frac{b}{a - 1}.
\end{gather*}
Значит, для $a = \alpha + n / 2$ и $b = \lambda + \frac{1}{2}\sum x_i^2$ имеем
\[
\me[](\theta|X) = \frac{2\lambda + \sum X_i^2}{2\alpha + n - 2}.
\]
Состоятельность доказывается аналогично предыдущим пунктам.
\end{solution}

\subsubsection{Метод II. Распределение Джеффриса и снова информация Фишера}

Возникает логичное желание задать на $\Theta$ равномерное распределение. Да, с неограниченным носителем так не выйдет (почти, см. задачу \ref{improper}), но для ограниченных $\Theta$ это звучит вполне логично: если мы ничего не знаем о потенциальном параметре, то все возможные варианты равновероятны. Так делают, и это вполне допустимая практика, но этот способ имеет существенный недостаток.

\textbf{Пример.} Рассмотрим выборку из биномиального распределения с параметром $\sqrt{\theta}$: 
\[
\rho_{\theta}(k) = C_n^k \theta^{\frac{k}{2}} (1 - \sqrt{\theta})^{n-k}.
\]
Хоть формально параметром является $\theta$, ясно, что <<главным героем>> здесь выступает именно $\sqrt{\theta}$. И если мы бездумно зададим равномерное априорное распределение, то $\sqrt{\theta}$ будет распределена не равномерно, что уже не является вполне обоснованным. Если в данном игрушечном примере всё <<ясно>>, то как поступать в общем случае (то есть какая именно функция от $\theta$ должна быть распределена равномерно) -- совершенно не понятно.

Это является мотивацией к идее, что априорная плотность должна быть устойчива к замене переменной. Напомним, что если к случайной величине $\xi$ с плотностью $\rho_{\xi}(x)$ применяется диффеоморфизм $\phi$, то плотность пересчитывается как
\[
\rho_{\phi(\xi)}(y) = \frac{1}{\left|\phi'(y)\right|} \cdot \rho_{\xi}(\phi^{-1}(y)).
\]
И тут на сцене появляется информация Фишера. Зададимся вопросом: как поменяется информация Фишера $I_X(\theta)$, если отныне параметром бы будем считать не $\theta$, а некоторую $\phi(\theta)$? Ответ неожиданный и приятный:
\begin{gather*}
    I_{X}(\phi(\theta)) = \me \left(\frac{\partial \ln{\rho_{\theta}(x)}}{\partial \phi(\theta)} \right)^2 = \me \left(\frac{\partial \ln{\rho_{\theta}(x)}}{\partial \theta}\cdot \frac{\partial \theta}{\partial \phi(\theta)} \right)^2 = \left(\frac{\partial \theta}{\partial \phi(\theta)}\right)^2 \me \left(\frac{\partial \ln{\rho_{\theta}(x)}}{\partial \theta}\right)^2=\\
    = \frac{1}{\left(\frac{\partial \phi(\theta)}{\partial \theta}\right)^2} \cdot I_X(\theta) = \frac{1}{\phi'(\theta)^2} \cdot I_X(\theta).
\end{gather*}
Вот те раз! Прямо как в формуле плотности при замене переменной появляется производная в знаменателе, правда на этот раз в квадрате. Поэтому для полного соответствия стоит взять от этого дела корень:
\[
q(t) \sim \sqrt{I_X(t)},
\]
где значок пропорциональности означает, что надо бы ещё нормировать эту штуку. Полученная априорная плотность в силу написанного выше будет инвариантна относительно замены переменной, что мы и хотели.

\begin{definition}
    \textit{Априорным распределением Джеффриса} называется распределение, плотность которого пропорциональна квадратному корню из информации Фишера (или в многомерном случае квадратному корню из определителя информационной матрицы).
\end{definition}

\textbf{Пример.} Рассмотрим всю ту же выборку $X_1, \ldots, X_n$ из распределения $Bern(\theta)$. Посчитаем для неё информацию Фишера:
\begin{gather*}
    \rho_{\theta}(x) = \theta^x (1 - \theta)^{1 - x},\;\;\;
    \ln{\rho_{\theta}(x)} = x \cdot \ln{\theta} + \left(1 - x\right) \cdot \ln{(1 - \theta)},\\
    \frac{\partial}{\partial \theta} \ln{\rho_{\theta}(x)} = \frac{x}{\theta} - \frac{1-x}{1 - \theta} = \frac{x - \theta}{\theta(1 - \theta)},\\
    i(\theta) = \me \left(\frac{X_1 - \theta}{\theta(1 - \theta)}\right)^2 = (1 - \theta) \cdot \left(\frac{0 - \theta}{\theta(1 - \theta)}\right)^2 + \theta \cdot \left(\frac{1 - \theta}{\theta(1 - \theta)}\right)^2 = \frac{1}{\theta(1 - \theta)}.
\end{gather*}
Таким образом, $q(t)$ должна быть пропорциональна $\frac{1}{\sqrt{\theta(1 - \theta)}}$, то есть априорным распределением является $Beta\left(\frac{1}{2}, \frac{1}{2}\right)$, что не может не радовать, так как оно к тому же сопряженно распределению Бернулли.

\begin{remark}
Иногда бывает так, что $I_X(\theta) \notin L_1(\Theta)$, и следовательно не пропорционально никакой плотности. В этом случае априорное распределение Джеффриса будет мерой на $\Theta$ (но не вероятностной), и мы можем лишь надеяться, что апостериорное распределение окажется вероятностным.
\end{remark}
\begin{definition}
    Невероятностные априорные распределения называют \textsf{improper prior}.
\end{definition}
Посмотрим, как ведут себя такие распределения и насколько адекватными получаются из них оценки.

\begin{problem}\label{improper}
    Пусть $X_1, \ldots, X_n$ -- выборка из распределения \textbf{(а)} $Pois(\theta)$, \textbf{(б)} $\mathcal{N}(\theta, 1)$. Возьмите распределение Джеффриса в качестве априорного и найдите байесовскую оценку параметра $\theta$. Сравните результат с первой задачей.
\end{problem}

\begin{solution}
    \textbf{(а)} Найдём информацию Фишера для пуассоновского распределения:
    \begin{gather*}
        \rho_{\theta}(t) = \frac{\theta^te^{-\theta}}{t!};\;\;\;\ln{\rho_{\theta}(t)} = t\ln{\theta} - \theta - \ln{t!};\\
        \frac{\partial}{\partial \theta} \ln{\rho_{\theta}(t)} = \frac{t}{\theta} - 1;\;\;\;i(\theta) = \va \left(\frac{X_1}{\theta} - 1\right) = \frac{1}{\theta^2} \va X_1 = \frac{1}{\theta}.
    \end{gather*}
    Получается, что распределение Джеффриса имеет плотность 
    \[
    q(t) \sim \frac{1}{\sqrt{t}},
    \]
    что не интегрируемо на $(0; +\infty)$, то есть мы получили \textsf{improper prior}. Но при этом
    \[
    \rho_{\theta|X}(t|\mathbf{x}) \sim t^{\sum x_i - 1/2} e^{-tn},
    \]
    то есть апостериорное распределение вполне себе определено, и равно $\Gamma\left(\sum x_i + 1/2, n\right)$, и итого байесовская оценка равна
    \[
    \me[](\theta|X) = \frac{\sum X_i + 1 / 2}{n}.
    \]
    
    \textbf{(б)} Информацию Фишера позаимствуем из задачи \ref{fisher_norm}: $i(\theta) = 1 / \sigma^2 = 1$, то есть распределение Джеффриса будет равномерным \textsf{improper prior} на $\R$ (то есть равномерное распределение на неограниченном $\Theta$ задать можно, хоть и не совсем легально). В таком случае
    \[
    \rho_{\theta|X}(t|\mathbf{x}) \sim \exp{\left(-\frac{n}{2}\theta^2 + \theta \sum x_i\right)},
    \]
    что есть $\mathcal{N}\left(\sum x_i / n, 1/n\right)$, откуда
    \[
    \me[](\theta|X) = \frac{\sum X_i}{n}.
    \]
    Как можно видеть, несмотря на то что затея с \textsf{improper prior} кажется неадекватной, она даёт нам довольно неплохие оценки, которые к тому же имеют не такое конское смещение в отличие от оценок из задачи \ref{conjugate}.
\end{solution}

\begin{problem}
    Аня выиграла в акции от компании <<Random Airlines>> и совершенно бесплатно улетела в случайный город, в котором есть $n$ автобусных маршрутов с номерами $1, 2, \ldots, n$. Выйдя из аэропорта, Аня увидела автобус номер 100. Оцените $n$.
\end{problem}

\begin{solution}
    Как видно, данных в условии не очень много, а значит, вариантов её понимания и решения масса, и однозначного правильного решения тут нет. Эта задача скорее творческая. Вот некоторые способы.
    \begin{enumerate}
        \item Будем считать, что наблюдение в виде номера автобуса имеет равномерное распределение от $1$ до $n$, то есть $\pth[n](X = t) = 1/n$ для $t \in \{1, \ldots, n\}$. В качестве априорного распределения удобно взять сопряжённое, что в нашем случае есть дискретный брат распределения Парето~--- $Zeta(s)$, при котором $\pth(n = t) \sim \frac{1}{t^s \zeta(s)}$, где $\zeta(s)$ -- дзета-функция Римана, или проще говоря какая-то константа для нормировки. Апостериорная плотность имеет вид
        \[
        \pth[](n = t | X) = \frac{\frac{1}{t^{s+1}} I(t \ge X)}{\sum_{p=1}^{\infty}\frac{1}{p^{s+1}} I(p \ge X)}.
        \]
        Отсюда байесовская оценка имеет вид
        \[
        \me[](n | X) = \sum_{t=1}^{\infty} \left(t\cdot \frac{\frac{1}{t^{s+1}} I(t \ge X)}{\sum_{p=1}^{\infty}\frac{1}{p^{s+1}} I(p \ge X)}\right) = \frac{\sum_{t=1}^{\infty} \frac{1}{t^{s}} I(t \ge X)}{\sum_{p=1}^{\infty}\frac{1}{p^{s+1}} I(p \ge X)} = \frac{\sum_{t=X}^{\infty} \frac{1}{t^{s}}}{\sum_{p=X}^{\infty}\frac{1}{p^{s+1}}}.
        \]
        <<Хвосты>> рядов в числителе и знаменателе хорошо приближаются соответствующими интегралами, поэтому 
        \[
        \me[](n | X) \approx \frac{\int_X^{+\infty} t^{-s}\,dt}{\int_X^{+\infty} t^{-s-1}\,dt} = X\cdot \frac{s}{s-1}.
        \]
        \item Хотелось бы получить точную оценку, а не приближённую. Для этого можно считать, что и распределение номера автобуса, и распределение параметра $n$ непрерывны (а почему бы и нет?). Так, будем считать, что номер автобуса $X$ распределён равномерно на отрезке $[0; n]$, отчего его плотность равна $\rho_n(t) = 1/n$. Неплохим вариантом будет снова взять сопряжённое распределение, из задачи \ref{conjugate} мы знаем, что им является $Pareto(k, a)$, и даже в курсе, каковой будет байесовская оценка:
        \[
        \me[](n|X) = \frac{(1+k)\max\{X, a\}}{k} = X\cdot\frac{k+1}{k},
        \]
        если $a$ изначально выбрать достаточно маленьким.
        \item Никто не говорил, что оценка должна быть обязательно построена исходя из байесовского подхода. Её можно построить и обычными методами. Например, если считать, что номер автобуса X распределён равномерно на $\{1, \ldots, n\}$, то по методу моментов
        \[
        \me[n] X = \sum_{k=1}^{n} \frac{k}{n} = \frac{n+1}{2} \Longrightarrow \widehat{n} = 2X - 1.
        \]
        Или можно совсем угореть и рассмотреть ОМП:
        \[
        \rho_n(x) = \frac{1}{n}\cdot I(x \in \{1, \ldots, n\}) \Longrightarrow \argmax_{n\in\N} \rho_n(X) = X,
        \]
        что, очевидно, будет весьма посредственной оценкой.
    \end{enumerate}
\end{solution}
 
