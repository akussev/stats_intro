\section{Коэффициенты корреляции}

Часто на практике представляется весьма полезным проверить, зависимы ли какие-то две характеристики. Представим, что для некоторых $n$ объектов есть признак $X$, их можно записать как вектор $(X_1, \ldots, X_n)$, а также признак $Y$, их можно записать как вектор $(Y_1, \ldots, Y_n)$. Таким образом, $(X_i, Y_i)$ является случайным вектором для всех $i$, который описывает пару характеристик для $i$-ого объекта (выборки с таким свойством ещё называют \textit{связанными}). Нас интересует, зависимы ли они, или, что эквивалентно, мы проверяем гипотезу о независимости:
\[
H_0\colon F_{X, Y}(x, y) = F_X(x)F_Y(y).
\]

Как известно, у независимости и корреляции есть некоторая связь (хотя между этими понятиями имеются и различия), поэтому логичным представляется исследовать корреляцию между элементами выборки, так как идейно и вычислительно это проще, чем проверять равенство выше для всех $x, y$. Для этого рассматривают всякие статистики с областью значений $[-1; 1]$, которые ознаменуют собой коррелированность выборок. Их обычно называют \textit{коэффициентами корреляции}. Рассмотрим некоторые из них.

\subsection{Коэффициент корреляции Пирсона}

Самое простое, что можно придумать, - это взять <<выборочную корреляцию>>.

\begin{definition}
    \textit{Коэффициентом корреляции Пирсона} называют следующую статистику:
    \[
    \widehat{\rho} = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum_{i=1}^n (X_i - \overline{X})^2 \sum_{i=1}^n (Y_i - \overline{Y})^2}}.
    \]
\end{definition}

Выбор такого коэффициента корреляции оправдывается тем, что в силу УЗБЧ и теоремы о наследовании сходимости доказывается, что
\[
    \widehat{\rho} \stackrel{\pth[]}{\longrightarrow} \rho(X_1, Y_1) = \frac{\cov(X_1, Y_1)}{\sqrt{\va[]X_1 \va[]Y_1}} = \mathsf{corr}(X_1, Y_1), \;\;\;n\to \infty.
\]
В контексте проверки гипотезы $H_0$ выше особо важной является следующая
\begin{theorem*}
Если нормально распределённые выборки $X$, $Y$ независимы и $n > 2$, то
    \begin{gather*}
        T := \widehat{\rho}\sqrt{\frac{n-2}{1-\widehat{\rho}^2}} \sim T_{n-2},
    \end{gather*}
    где $T_m$ -- распределение Стьюдента с $m$ степенями свободы.
\end{theorem*}

Проверка гипотезы проводится следующим образом: если коэффициент $\widehat{\rho}$ близок к границам отрезка $[-1; 1]$, то это является поводом отклонить $H_0$. В условиях теоремы выше критерий уровня значимости $\alpha$ проверки гипотезы можно формализовать как
\[
R = [-1; 1] \setminus (t_{\alpha/2}; t_{1-\alpha/2}),
\]
где $t_{p}$ -- $p$-квантиль распределения $T_{n-2}$.

\subsection{Коэффициент корреляции Спирмэна}

Какие минусы у коэффициента выше? Во-первых, конечно, не все рассматриваемые выборки нормальны, хотя такое допущение встречается довольно часто. Самое неприятное -- низкая робастность, то есть неустойчивость статистики к выбросам, что особенно характерно для тех из них, которые основаны на выборочном среднем.

У нас уже встречались статистики, которые таким недостатком обладают в меньшей степени -- это порядковые статистики. В этой связи давайте прибегнем к так называемым \textit{ранговым критериям}, которые основываются на ранге --- номере элементов выборки, расположенных в порядке возрастания.

\begin{definition}
    Пусть $R_i$ и $S_j$ -- место в вариационном ряду для $X_i$ и $Y_j$ соответственно. \textit{Коэффициентом корреляции Спирмэна} называют следующую статистику:
    \[
    \rho_S = \frac{\sum_{i=1}^n (R_i - \overline{R})(S_i - \overline{S})}{\sqrt{\sum_{i=1}^n (R_i - \overline{R})^2 \sum_{i=1}^n (S_i - \overline{S})^2}}.
    \]
\end{definition}

Если считать, что функции распределения $F_X$ и $F_Y$ непрерывны, то всё хорошо, вероятность того, что какие-либо два элемента выборки совпадут, равна нулю, поэтому почти наверное такое упорядочивание однозначно. Если же в выборке встречаются одинаковые значения, то обычно используют средние ранги. Например, если выборка представляет собой набор $2, 5, 5, 7$, то их средние ранги равны соответственно $1, 2.5, 2.5, 4$. Такой подход сохраняет сумму всех рангов, а вот с суммой квадратов будут проблемы, поэтому некоторые вещи ниже для такой модели неприменимы. Чтобы с этим всем не возиться, для простоты будем всё-таки подразумевать, что функции распределения непрерывны.

Оформим все свойства в одном утверждении.

\begin{theorem*}
Имеют место быть следующие свойства:
\begin{enumerate}
    \item Коэффициент корреляции Спирмэна можно переписать в виде
    \[
    \rho_S = 1 - \frac{6}{n^3-n} \sum_{i=1}^n (R_i - S_i)^2.
    \]
    \item При верности $H_0$ имеем $\me[]\rho_S = 0$, $\va[] \rho_S = \frac{1}{n-1}$, а также есть сходимость:
    \[
    \frac{\rho_S}{\sqrt{\va[] \rho_S}} \stackrel{d}{\longrightarrow} \mathcal{N}(0, 1).
    \]
    \item Коэффициент корреляции и в самом деле отражает корреляцию между элементами выборки, то есть $-1 \le \rho_S \le 1$, причём крайние значения достигаются.
\end{enumerate}
\end{theorem*}

Для приличия найдём матожидание и дисперсию сего коэффициента. Во-первых, сделаем витающее в воздухе замечание: $R_1, \ldots, R_n$ есть ничто иное, как перестановка чисел $1, \ldots, n$, поэтому если мы встретим какое-либо симметричное выражение, зависящее от $R_i$, то мы всегда в нём сможем сделать замену. Так, например, $\sum R_i = \frac{n(n+1)}{2}$, а $\sum R^2_i = \frac{n(n+1)(2n+1)}{6}$. Во-вторых, при верности гипотезы $H_0$ величины $R_i$ и $S_j$ независимы при любых $i$, $j$, поэтому матожидание от их произведения раскладывается в произведение матожиданий. В свою очередь так как компоненты выборки независимы, то ранги могут образовывать любую перестановку равновероятно, откуда несложно посчитать $\me[]R_i = \me[]S_j = \frac{n+1}{2}$. С этими новыми знаниями посчитаем матожидание $\rho_S$:
\begin{gather*}
    \me[] \rho_S = \me[] \left(1 - \frac{6}{n^3-n}  \sum_{i=1}^n (R_i - S_i)^2\right) = \me[] \left(1 - \frac{6}{n^3-n}  \sum_{i=1}^n (R_i^2 - 2 R_i S_i + S_i^2)\right) = \\
    1 - \frac{6}{n^3-n} \cdot 2 \cdot \frac{n(n+1)(2n+1)}{6} + \frac{12}{n^3-n} \cdot n\me[]R_1 S_1 = 1 - \frac{4n+2}{n-1} + \frac{12}{n^2-1} \cdot \left(\frac{n+1}{2}\right)^2 = \\
    = \frac{-3n-3}{n-1} + 3 \cdot\frac{n+1}{n-1} = 0.
\end{gather*}

Отлично, теперь посмотрим на дисперсию. Так как прибавление константы на дисперсию не влияет, то оставим в формуле коэффициента только сумму произведений $R_iS_i$. Также нелишним будет посчитать матожидание квадрата ранга:
\[
\me[]R_1^2 = \sum_{i=1}^n i^2\cdot \pth[](R_1 = i) = \frac{1}{n} \sum_{i=1}^n i^2 = \frac{(n+1)(2n+1)}{6},
\]
и матожидание произведения двух разных рангов $R_i$ и $R_j$: различных способов выбрать значения для них теперь равно $n(n-1)$, и они также равновероятны. Поэтому
\begin{gather*}
    \me[]R_iR_j = \sum_{i\ne j} \frac{1}{n(n-1)} \cdot ij = \sum_{i, j=1}^n \frac{1}{n(n-1)} \cdot ij - \sum_{i=1}^n \frac{1}{n(n-1)} \cdot i^2 = \\
    =\frac{1}{n(n-1)}\cdot \frac{n^2(n+1)^2}{4} -  \frac{1}{n(n-1)} \cdot \frac{n(n+1)(2n+1)}{6} = \frac{(n+1)(3n+2}{12}.
\end{gather*}
Теперь можем начинать жёстко считать дисперсию:
\begin{gather*}
    \va[] \rho_S = \frac{144}{(n^3-n)^2} \va[] \sum R_i S_i = \frac{144}{(n^3-n)^2} \left[\me[]\left(\sum R_i S_i\right)^2 - \left(\me[] \sum R_i S_i\right)^2 \right] = \\
    = \frac{144}{(n^3-n)^2} \left[\sum \me[]R_i^2 S_i^2 + \sum_{i\ne j}\me[]R_iS_iR_jS_j - \left(n\cdot\me[] R_1 S_1\right)^2 \right] = \\
    = \frac{144}{(n^3-n)^2} \left[n\cdot\me[]R_1^2 \cdot\me[]S_1^2 + n(n-1)\me[]R_1R_2\cdot \me[]S_1S_2 - \left(n\cdot\me[] R_1 \cdot \me[]S_1\right)^2 \right] =\\
    = \frac{144}{n(n^2-1)^2} \left[\frac{(n+1)^2(2n+1)^2}{36} + (n-1)\cdot \frac{(n+1)^2(3n+2)^2}{144} - n\cdot\frac{(n+1)^4}{16}\right] = \\
    = \frac{1}{n(n-1)^2}\left(4(2n+1)^2+(n-1)(3n+2)^2-9n(n+1)^2\right) = \frac{1}{n-1}.
\end{gather*}


\subsection{Коэффициент корреляции Кендалла}

Схожую по идеологии ранжирования статистику ввёл М. Дж. Кендэлл. Только теперь мы смотрим на количество инверсий, которые образуются во второй выборке, если расположить их в порядке возрастания соответствующих элементов первой. То есть появляется некоторая мера неупорядоченности второй выборки относительно первой, и если выборки независимы, то логично предположить, что инверсий будет примерно столько же, сколько и правильно упорядоченных пар. Более формально:

\begin{definition}
    Будем говорить, что пары $(X_i, Y_i)$ и $(X_j, Y_j)$ \textit{согласованны} (считаем, что $1 \le i < j \le n$), если $X_i < X_j$ и $Y_i < Y_j$ или $X_i > X_j$ и $Y_i > Y_j$ (то есть $sign(X_j - X_i)(Y_j - Y_i) = 1$).
\end{definition}

Пусть для выборок $X$ и $Y$ величина $S$ есть число согласованных пар, а $R$ -- число несогласованных (по всем $1 \le i < j \le n$). При верности гипотезы они должны не слишком сильно отличаться, поэтому логично ввести следующую меру превышения согласованности над несогласованностью:

\[
T = S - R = \sum_{i<j} sign(X_j - X_i)(Y_j - Y_i).
\]

Понятное дело, что величина $T$ может меняться от $-\frac{n(n-1)}{2}$ до $\frac{n(n-1)}{2}$ (второй вариант характерен для выборок с полным согласием порядка, а первый -- наоборот, когда увеличение $X$ означает уменьшение $Y$). Поэтому логично нормировать полученную статистику, чтобы она лежала на отрезке $[-1; 1]$, как и все коэффициенты корреляции.

\begin{definition}
    \textit{Коэффициентом корреляции Кендалла} называют следующую статистику:
    \[
    \tau = \frac{2}{n(n-1)}\cdot T = \frac{2}{n(n-1)} \sum_{i < j} sign(X_i - X_j)\cdot sign(Y_i - Y_j)
    \]
\end{definition}

Отметим следующие свойства:
\begin{enumerate}
    \item При верности $H_0$ имеем $\me[]\tau = 0$, $\va[] \tau = \frac{2(2n+5)}{9n(n-1)}$, и есть сходимость
    \[
    \frac{\tau}{\sqrt{\va[] \tau}} \stackrel{d}{\longrightarrow} \mathcal{N}(0, 1).
    \]
    \item С учётом того, что $S+R=\frac{n(n-1)}{2}$, коэффициент корреляции можно переписать как
    \[
    \tau = 1 - \frac{4}{n(n-1)}R.
    \]
    
\end{enumerate}

