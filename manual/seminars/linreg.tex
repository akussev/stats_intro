\section{Линейная регрессия}

На практике часто встречается ситуация, когда зависимость целевой величины от некоторых <<фичей>> можно приблизить чем-то линейным. В данном случае мы предполагаем, что истинная зависимость линейна и немного искажена каким-то шумом (ошибки измерения, выбросы и прочие вещи), который можно считать \textit{случайным}. Отсюда полезно посмотреть на данную проблему с точки зрения теории вероятности. Давайте же приведём формальную постановку вопроса на языке статистики и установим некоторые приятные результаты.

Предположим, что $i$-ая целевая величина ($i\in\{1,\ldots,n\}$) в своей первозданности есть линейная комбинация <<фичей>> $Z_{ij}$ с некоторыми неизвестными параметрами $\theta_1, \ldots, \theta_k$, то есть $\sum_{j=1}^{k} \theta_j Z_{ij}$. Но при её измерении появляется некоторый шум $\epsilon_i$, поэтому наблюдение за этой величиной $X_i$ можно представить как
\[
X_i = \sum_{j=1}^{k} \theta_j Z_{ij} + \epsilon_i,
\]
или, что эквивалентно,
\[
X = Z \theta + \epsilon,
\]
где $Z=(Z_{ij})$ -- матрица <<фичей>>, $\theta = (\theta_1, \ldots, \theta_k)^T$ -- столбец из неизвестных параметров. Логично допустить, что случайные величины $\epsilon_i$ независимы в совокупности (наблюдения друг на друга не влияют), их матожидание $\me[] \epsilon_i = 0$ (в среднем ошибки нет), а их дисперсия одинакова и равна некоторой неизвестной величине $\va[] \epsilon_i = \sigma^2$, которую мы будем также считать за параметр в модели (то есть ковариационная матрица случайного вектора $\epsilon$ равна $\va[]\epsilon = \sigma^2 E$). Как, надеюсь, стало понятно, наша задача --- по вектору наблюдений $X$ оценить вектор $\theta$ и дисперсию $\sigma^2$. 

\begin{wrapfigure}{o}{200pt}
\begin{asy}
import graph;
size(190,0);

path line=(0,0)--(2,0)--(3,1)--(1,1)--(0,0);
draw(line);
draw((0.7,0.2)--(1.7,1.5),EndArrow);
draw((0.7,0.2)--(1.7,0.5),EndArrow);
draw((1.7,1.5)--(1.7,0.5),dashed);


label("$\mathcal{L}$", (0.2, 0.05), NE);
label("$\Pr_{\mathcal{L}} X$", (1.2, 0.35), SE);
label("$X$", (1.4, 1.1), NW);
\end{asy}
\end{wrapfigure}

Для дальнейшего удобства также будет важно сделать допущение, что столбцы $\mathbf{z}_1, \ldots, \mathbf{z}_k$ матрицы $Z$ \textit{линейно независимы}. Это позволяет интерпретировать задачу с позиций линейной алгебры: истинный вектор $l = Z\theta$ лежит в некотором подпространстве $\mathcal{L} = \langle \mathbf{z}_1, \ldots, \mathbf{z}_k \rangle \subset \R^n$, образованном столбцами матрицы $Z$, в то время как наблюдаемый вектор $X$ может в общем случае и не лежать в $\mathcal{L}$ (см. рис.). Отсюда логично в качестве <<приближения>> вектора $X$ выбрать его проекцию $\Pr_{\mathcal{L}} X$ на это подпространство, так как она доставляет минимум расстояния между $X$ и векторами из $\mathcal{L}$.

Осталось лишь найти оценку вектору параметров $\widehat{\theta}$, отвечающую проекции $\pr_{\mathcal{L}} X = Z \widehat{\theta}$. Так как $\pr_{\mathcal{L}} X$ -- ортогональная проекция, то вектор $\delta = X - \pr_{\mathcal{L}} X$ лежит в $\mathcal{L}^{\bot}$, а значит, он ортогонален любому вектору из $\mathcal{L}$, в частности, векторам $\mathbf{z}_1, \ldots, \mathbf{z}_k$. Следовательно, вектор $Z^T\delta$, состоящий из скалярных произведений $\delta$ с $\mathbf{z}_j$, -- нулевой, то есть
\[
Z^T(X - \pr_{\mathcal{L}} X) = 0 \Longrightarrow Z^T X = (Z^T Z) \widehat{\theta}.
\]
Так как столбцы матрицы $Z$ независимы, то матрица $Z^T Z$ будет невырожденной, поэтому у неё есть обратная, из чего получаем оценку
\[
 \widehat{\theta} = (Z^T Z)^{-1} Z^T X.
\]
\begin{definition}
Полученная оценка называется \textit{оценкой по методу наименьших квадратов} (неожиданно, правда?).
\end{definition}

Сразу выделим полезные свойства полученной оценки.
\begin{problem}\label{cov_mat}
Найдите математическое ожидание и матрицу ковариаций для $\widehat{\theta}$.
\end{problem}

\begin{solution}
Линейность матожидания распространяется на многомерный случай:
\[
\me[] (A \xi B) = A (\me[] \xi) B.
\]
Значит, 
\begin{gather*}
    \me[\theta, \sigma^2] \widehat{\theta} = \me[\theta, \sigma^2] ((Z^T Z)^{-1} Z^T X) = (Z^T Z)^{-1} Z^T \me[\theta, \sigma^2] X = (Z^T Z)^{-1} Z^T \me[\theta, \sigma^2] (Z\theta + \epsilon) = \\
    = (Z^T Z)^{-1} Z^T \cdot Z\theta = \theta.
\end{gather*}
Менее очевидной является формула для ковариационной матрицы, но её легко вывести:
\[
\cov(A\xi, B\eta) = A\cov(\xi, B\eta) = A(\cov(B\eta, \xi))^T = A(B\cov(\eta, \xi))^T = A\cov(\xi, \eta)B^T.
\]
Теперь мы можем получить требуемое:
\begin{gather*}
\va[\theta, \sigma^2] \widehat{\theta} = \va[\theta, \sigma^2] ((Z^T Z)^{-1} Z^T X) = (Z^T Z)^{-1} Z^T \cdot \va[\theta, \sigma^2] X \cdot ((Z^T Z)^{-1} Z^T)^T = \\
= (Z^T Z)^{-1} Z^T \cdot \va[\theta, \sigma^2] (Z\theta + \epsilon) \cdot Z (Z^T Z)^{-1} = (Z^T Z)^{-1} Z^T \cdot \sigma^2 E \cdot Z (Z^T Z)^{-1} = \sigma^2 (Z^T Z)^{-1}.
\end{gather*}
\end{solution}

Таким образом, полученная оценка $\widehat{\theta}$ является несмещённой оценкой вектора $\theta$. Также можно показать, что несмещённой оценкой дисперсии $\sigma^2$ является
\[
\widehat{\sigma^2} = \frac{1}{n - k} \left\|X - Z\widehat \theta\right\|^2
\]
(мы по сути докажем этот факт в случае с нормально распределёнными ошибками в задаче \ref{linreg_ci}).

Потренируемся в \sout{глиномесии} нахождении МНК-оценки на следующей классической задаче.

\begin{problem}
Имеется 2 объекта с весами $a$ и $b$. Мы взвесили с ошибками первый, второй и оба объекта вместе, причём дисперсия ошибки в последнем случае была в 4 раза больше. Сведите задачу к линейной регрессионной модели и найдите оценки для $a$ и $b$.
\end{problem}

\begin{solution}
Пусть наблюдения в первом, втором и третьем случае равнялись $X_a$, $X_b$ и $X_{ab}$ соответственно. Из условия имеем
\[
\begin{pmatrix}
X_a\\
X_b\\
X_{ab}
\end{pmatrix}
=
\begin{pmatrix}
a\\
b\\
a+b
\end{pmatrix}
+
\begin{pmatrix}
\epsilon_1\\
\epsilon_2\\
\epsilon_3
\end{pmatrix} = 
\begin{pmatrix}
1 & 0\\
0 & 1\\
1 & 1
\end{pmatrix}
\begin{pmatrix}
a\\
b
\end{pmatrix}
+
\begin{pmatrix}
\epsilon_1\\
\epsilon_2\\
\epsilon_3
\end{pmatrix}
,
\]
причём $\va[] \epsilon_3 = 4\va[] \epsilon_1 = 4\va[] \epsilon_2 = 4\sigma^2$. Чтобы свести задачу к модели линейной регрессии выше, достаточно поделить на 2 третью строчку в формуле выше: тогда дисперсия ошибки по этой координате уменьшиться в 4 раза (так как $\va[] (\epsilon_3 / 2) = (\va[] \epsilon_3) / 4$), чего бы нам и хотелось. Матрицей признаков и наблюдением тогда будут являться соответственно
\[
Z = \begin{pmatrix}
1 & 0\\
0 & 1\\
1/2 & 1/2
\end{pmatrix},\;\;\; X = \begin{pmatrix}
X_a\\
X_b\\
X_{ab} / 2
\end{pmatrix}.
\]
Теперь у нас есть всё, чтобы посчитать оценку:
\begin{gather*}
    Z^T Z = \begin{pmatrix}
1 & 0 & 1/2 \\
0 & 1 & 1/2
\end{pmatrix} \cdot 
\begin{pmatrix}
1 & 0\\
0 & 1\\
1/2 & 1/2
\end{pmatrix} = 
\begin{pmatrix}
5/4 & 1/4 \\
1/4 & 5/4
\end{pmatrix},\;\;\;
(Z^T Z)^{-1} = 
\begin{pmatrix}
5/6 & -1/6 \\
-1/6 & 5/6
\end{pmatrix}\\
\widehat{\theta} = (Z^T Z)^{-1} Z^T X = 
\begin{pmatrix}
5/6 & -1/6 \\
-1/6 & 5/6
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 1/2 \\
0 & 1 & 1/2
\end{pmatrix}
\begin{pmatrix}
X_a\\
X_b\\
X_{ab} / 2
\end{pmatrix} = 
\begin{pmatrix}
5/6 & -1/6 & 1/3\\
-1/6 & 5/6 & 1/3
\end{pmatrix}
\begin{pmatrix}
X_a\\
X_b\\
X_{ab} / 2
\end{pmatrix} \Rightarrow\\
\widehat{a} = \frac{5X_a}{6} - \frac{X_b}{6} + \frac{X_{ab}}{6},\;\;\;\widehat{b} = -\frac{X_a}{6} + \frac{5X_b}{6} + \frac{X_{ab}}{6}
\end{gather*}
\end{solution}

\subsection{Гауссовская линейная модель}

Всякие физики да химики из своих внутренних побуждений часто полагают ошибки нормальными, поэтому в дальнейшем под $\epsilon$ будем подразумевать гауссовский вектор $\mathcal{N}(0, \sigma^2 E)$. Данная модель называется \textit{гауссовской линейной моделью}. Подобное допущение действительно бывает крайне полезным. Например, теперь можно утверждать следующий факт:

\begin{theorem*}
Статистика $(\widehat{\theta}, \|X - Z \widehat{\theta}\|^2)$ является полной достаточной в гауссовской линейной модели. Как следствие, оценки $\widehat{\theta}$ и $\widehat{\sigma^2}$ являются оптимальными.
\end{theorem*}

\begin{problem}
Взвешивание трёх грузов массами $a$, $b$, $c$ на одних и тех же весах производится следующим образом: $n_1$ раз взвешиваются второй и третий груз вместе, $n_2$ раз взвешиваются первый и третий груз вместе и $n_3$ раз взвешиваются первый и второй груз вместе. В предположении, что все ошибки имеют распределение $\mathcal{N}(0, \sigma^2)$, сведите задачу к модели линейной регрессии и найдите оптимальные оценки для $a$, $b$, $c$.
\end{problem}

\begin{solution}
Если в тупую перевести задачу на язык линейной регрессии, нам придётся иметь дело с матрицей признаков
\[
Z' =
\begin{pmatrix}
0 & \cdots & 0 & 1 & \cdots & 1 & 1 & \cdots & 1\\
1 & \cdots & 1 & 0 & \cdots & 0 & 1 & \cdots & 1\\
1 & \cdots & 1 & 1 & \cdots & 1 & 0 & \cdots & 0
\end{pmatrix}^T,
\]
которая сама по себе выглядит неприятно, а ведь нужно ещё что-то обращать и много чего умножать --- гадость одним словом. Куда проще здесь будет считать именно $\alpha = b+c$, $\beta = a+c$ и $\gamma = a+b$ параметрами модели, а через них потом выразить нужные. В таком случае вектор наблюдений можно выразить как
\[
X = 
\begin{pmatrix}
X^{\alpha}_1\\
\hdotsfor{1}\\
X^{\alpha}_{n_1}\\
X^{\beta}_1\\
\hdotsfor{1}\\
X^{\beta}_{n_2}\\
X^{\gamma}_1\\
\hdotsfor{1}\\
X^{\gamma}_{n_3}
\end{pmatrix}
= Z
\begin{pmatrix}
\alpha\\
\beta\\
\gamma
\end{pmatrix}
+ \epsilon,\;\;\;\text{где  }
Z = 
\begin{pmatrix}
1 & 0 & 0\\
\hdotsfor{3}\\
1 & 0 & 0\\
0 & 1 & 0\\
\hdotsfor{3}\\
0 & 1 & 0\\
0 & 0 & 1\\
\hdotsfor{3}\\
0 & 0 & 1\\
\end{pmatrix}.
\]
Матрица $Z$ намного проще в использовании, потому что её столбцы $\textit{ортогональны}$: в таком случае матрица 
\[
Z^T Z = \begin{pmatrix}
n_1 & 0 & 0\\
0 & n_2 & 0\\
0 & 0 & n_3\\
\end{pmatrix}
\]
будет диагональной, что в разы упрощает дальнейшую работу:
\[
\widehat{\theta} = (Z^T Z)^{-1} Z^T X = 
\begin{pmatrix}
1/n_1 & 0 & 0\\
0 & 1/n_2 & 0\\
0 & 0 & 1/n_3\\
\end{pmatrix}
\begin{pmatrix}
1 & \cdots & 1 & 0 & \cdots & 0 & 0 & \cdots & 0\\
0 & \cdots & 0 & 1 & \cdots & 1 & 0 & \cdots & 0\\
0 & \cdots & 0 & 0 & \cdots & 0 & 1 & \cdots & 1
\end{pmatrix}X =
\begin{pmatrix}
\frac{1}{n_1}\sum X_i^{\alpha}\\
\frac{1}{n_2}\sum X_i^{\beta}\\
\frac{1}{n_3}\sum X_i^{\gamma}
\end{pmatrix}
\]
Далее выражаем $a$, $b$ и $c$ через $\alpha$, $\beta$ и $\gamma$ и дело в шляпе: оптимальность оценок будет следовать из того, что они являются функциями от полных достаточных статистик.
\[
\widehat{a} = \frac{\overline{X^{\beta}}+\overline{X^{\gamma}} - \overline{X^{\alpha}}}{2},\;\;\;
\widehat{b} = \frac{\overline{X^{\gamma}}+\overline{X^{\alpha}} - \overline{X^{\beta}}}{2},\;\;\;
\widehat{c} = \frac{\overline{X^{\alpha}}+\overline{X^{\beta}} - \overline{X^{\gamma}}}{2}
\]
\end{solution}

Рассмотрим внимательнее, как устроены найденные нами оценки в гауссовской линейной модели. В этом нам поможет

\begin{theorem*}[об ортогональном разложении]
Пусть $X \sim \mathcal{N}(a, \sigma^2 E_n)$ -- гауссовский вектор, а $L_1 \oplus \ldots \oplus L_r$ -- ортогональное разложение $\R^n$. Тогда $\pr_{L_1} X, \ldots, \pr_{L_r} X$ независимы и нормально распределены, и для всех $i\in\{1, \ldots, r\}$
\[
\frac{1}{\sigma^2} \|\pr_{L_i} X - \,\me[] \pr_{L_i} X\|^2 \sim \chi^2_{\dim{L_i}}.
\]
\end{theorem*}

\begin{proof}[Идея]
Переводим $X$ в о/н базис, согласованный с разложением $L_1 \oplus \ldots \oplus L_r$. Так как матрица $S$ перехода от одного о/н базиса к другому -- ортогональна, то по формуле пересчёта матожидания и матрицы ковариаций из задачи \ref{cov_mat} в новом базисе вектор $X$ будет иметь распределение $\mathcal{N}(Sa, \sigma^2 E_n)$.
\end{proof}

Применим теорему к нашей модели. По определению МНК-оценки, $\pr_{\mathcal{L}} X = Z\widehat{\theta}$, а значит, $\pr_{\mathcal{L}^{\bot}} X = X - Z\widehat{\theta}$, и по теореме об ортогональном разложении
\begin{equation}\label{chi_nk}
    \frac{1}{\sigma^2} \|X - Z\widehat{\theta}\|^2 \sim \chi^2_{\dim{\mathcal{L}^{\bot}}} = \chi^2_{n - k}.
\end{equation}

Что же касается $\widehat \theta$, то она, как линейное преобразование гауссовского вектора $X$, имеет распределение $\mathcal{N}(\theta, \sigma^2(Z^T Z)^{-1})$ (параметры мы нашли ранее в задаче \ref{cov_mat}). Следовательно, $\widehat{\theta}_i \sim \mathcal{N}(\theta_i, \sigma^2\left[(Z^T Z)^{-1}\right]_{ii})$, или, что эквивалентно,
\[
\frac{\widehat{\theta}_i - \theta_i}{\sigma \sqrt{\left[(Z^T Z)^{-1}\right]_{ii}}} \sim \mathcal{N}(0, 1).
\]

Было бы неплохо избавиться от $\sigma$, чтобы оставить только один неизвестный $\theta_i$. У нас уже есть одна статистика с известным распределением и торчащим $\sigma$ -- это (\ref{chi_nk}). Если поделить одно на корень от другого, то получится от него избавиться, но непонятно, будет ли у полученной случайной величины конкретное распределение, не зависящее от параметров.

Оказывается, будет --- из теоремы об ортогональном разложении следует, что $Z \widehat{\theta}$ и $X - Z \widehat{\theta}$ будут независимыми, а значит, $\widehat{\theta} = (Z^T Z)^{-1} Z^T \cdot Z \widehat{\theta}$ и $\widehat{\sigma^2} = \|X - Z\widehat{\theta}\|^2 / (n - k)$ также независимы. Поэтому распределение
\[
\left. \frac{\widehat{\theta}_i - \theta_i}{\sigma \sqrt{\left[(Z^T Z)^{-1}\right]_{ii}}} \right/ \sqrt{\frac{1}{(n-k)\sigma^2} \|X - Z\widehat{\theta}\|^2} = 
\frac{\widehat{\theta}_i - \theta_i}{\sqrt{\widehat{\sigma^2}\left[(Z^T Z)^{-1}\right]_{ii}}}
\]
однозначно определено свёрткой, и не зависит от неизвестных параметров. Остаётся вопрос: зачем мы поделили на $n-k$? Для красоты? Не только:
\begin{definition}
Если случайные величины $\xi$ и $\eta$ независимы, причём $\xi \sim \mathcal{N}(0, 1)$, а $\eta \sim \chi^2_{m}$, то говорят, что случайная величина
\[
\zeta = \frac{\xi}{\sqrt{\eta / m}}
\]
имеет \textit{распределение Стьюдента с $m$ степенями свободы}. Обозначается как $\zeta \sim T_m$.
\end{definition}

Из всего вышесказанного следует, что
\begin{equation}\label{student}
    \frac{\widehat{\theta}_i - \theta_i}{\sqrt{\widehat{\sigma^2}\left[(Z^T Z)^{-1}\right]_{ii}}} \sim T_{n - k}.
\end{equation}

Полученные распределения приведённых статистик позволяют искать доверительные интервалы для $\theta_i$ и $\sigma^2$. Рассмотрим это на следующем примере.

\begin{problem}\label{linreg_ci}
Пусть $X_1, \ldots, X_n$ -- независимые случайные величины, где $X_i$ распределена по нормальному закону $\mathcal{N}(a+bi, \sigma^2)$. Постройте точные доверительные интервалы для параметров $a$, $b$, $\sigma^2$.
\end{problem}

\begin{solution}
В данной задаче матрица признаков имеет вид
\[
Z = 
\begin{pmatrix}
1 & 1 & \cdots & 1\\
1 & 2 & \cdots & n
\end{pmatrix}^T.
\]
Вспоминаем формулу для суммы квадратов первых $n$ натуральных чисел и считаем:
\begin{gather*}
    Z^T Z = 
    \begin{pmatrix}
    n & \frac{n(n+1)}{2}\\
    \frac{n(n+1)}{2} & \frac{n(n+1)(2n+1)}{6}
    \end{pmatrix},\;\;\;
    ( Z^T Z )^{-1} = 
    \begin{pmatrix}
    \frac{2(2n+1)}{n(n-1)} & -\frac{6}{n(n-1)}\\
    -\frac{6}{n(n-1)} & \frac{12}{n(n^2-1)}
    \end{pmatrix},\\
    \widehat{\theta} = ( Z^T Z )^{-1} Z^T X = ( Z^T Z )^{-1} \cdot
    \begin{pmatrix}
    \sum\limits_{i=1}^n X_i\\
    \sum\limits_{i=1}^n i X_i
    \end{pmatrix} = 
    \begin{pmatrix}
    \frac{2(2n+1)}{n(n-1)}\sum\limits_{i=1}^n X_i - \frac{6}{n(n-1)}\sum\limits_{i=1}^n i X_i\\
    -\frac{6}{n(n-1)}\sum\limits_{i=1}^n X_i + \frac{12}{n(n^2-1)}\sum\limits_{i=1}^n i X_i
    \end{pmatrix} =
    \begin{pmatrix}
    \widehat{a}\\
    \widehat{b}
    \end{pmatrix},\\
    \widehat{\sigma^2} = \frac{1}{n - 2}\|X- Z\widehat{\theta}\|^2 = ... \text{ (лень досчитывать)}
\end{gather*}
Из соотношения (\ref{chi_nk}) имеем доверительный интервал для $\sigma^2$:
\[
\pth[\theta, \sigma^2]\left(\frac{(n-2)\widehat{\sigma^2}}{x_{(1+\gamma)/2}} < \sigma^2 < \frac{(n-2)\widehat{\sigma^2}}{x_{(1-\gamma)/2}} \right) = \pth[\theta, \sigma^2]\left(x_{(1-\gamma)/2} < \frac{(n-2)\widehat{\sigma^2}}{\sigma^2} < x_{(1+\gamma)/2} \right)=\gamma,
\]
где $x_p$ -- $p$-квантиль распределения $\chi^2_{n-2}$. В то же время из (\ref{student}) имеем следующие ДИ для $a$ и $b$:
\begin{gather*}
    \pth[\theta, \sigma^2]\left(\widehat{a} - y_{(1+\gamma)/2}\sqrt{\frac{{2\widehat{\sigma^2}(2n+1)}}{n(n-1)}} < a < \widehat{a} + y_{(1+\gamma)/2}\sqrt{\frac{{2\widehat{\sigma^2}(2n+1)}}{n(n-1)}} \right) =\gamma,\\
    \pth[\theta, \sigma^2]\left(\widehat{b} - y_{(1+\gamma)/2}\sqrt{\frac{{12\widehat{\sigma^2}}}{n(n^2-1)}} < b < \widehat{b} + y_{(1+\gamma)/2}\sqrt{\frac{{12\widehat{\sigma^2}}}{n(n^2-1)}} \right) =\gamma,
\end{gather*}
где $y_{p}$ -- $p$-квантиль распределения $T_{n-2}$. Здесь мы не использовали квантиль $y_{(1-\gamma)/2}$, ибо распределение Стьюдента симметрично относительно нуля.
\end{solution}

Последние две задачи дают теоретическое обоснование методу регуляризации, используемому в машинном обучении.

\begin{problem}\label{ridge}
Применим байесовский подход к оценке параметра $\theta$. Взяв в качестве априорного распределения $\mathcal{N}(0, \gamma^2 E_k)$ -- гауссовский вектор с независимыми компонентами, найдите апостериорную плотность $\rho(\theta|X)$ (с точностью до константы) и байесовскую оценку. Решите задачу оптимизации $\rho(\theta|X) \to max_{\theta}$. Чем данный подход и полученная в нём оценка лучше, чем МНК?
\end{problem}

\begin{solution}
Из условия мы полагаем, что априорное распределение задаётся плотностью
\[
q(t) \sim \exp\left(-\frac{1}{2\gamma^2}\|t\|^2\right) = \exp\left(-\frac{1}{2\gamma^2}t^T t\right).
\]
Значит, апостериорному распределению соответствует
\[
\rho_{\theta, X}(t|\mathbf{x}) \sim 
\exp\left(-\frac{1}{2\gamma^2}t^T t - \frac{1}{2\sigma^2} (\mathbf{x} - Zt)^T (\mathbf{x} - Zt)\right).
\]
Если мы попытаемся максимизировать эту плотность, то получим следующую задачу оптимизации:
\[
\|X - Z \theta\|^2 + \frac{\sigma^2}{\gamma^2} \| \theta\|^2 \to \min_{\theta},
\]
что называется \textsf{Ridge regression}. Её преимущество в том, что мы <<штрафуем>> вектор $\theta$ за излишне большие координаты, что позволяет получать более стабильные решения. Особенно отчётливо это станет видно, когда мы найдём соответствующую байесовскую оценку. Это можно сделать напрямую, решив задачу выше, но мы поступим более интеллектуально, найдя параметры $a$ и $\Sigma$ многомерного нормального распределения, отвечающего $\rho_{\theta, X}(t|\mathbf{x})$.
\begin{gather*}
    \text{С одной стороны, } \rho_{\theta, X}(t|\mathbf{x}) \sim \exp\left[ \frac{1}{\sigma^2} \mathbf{x}^T Zt - \frac{1}{2} t^T \left( \frac{1}{\gamma^2} E + \frac{1}{\sigma^2} Z^T Z\right) t\right].\\
    \text{С другой -- } \rho_{\theta, X}(t|\mathbf{x}) \sim \exp\left(-\frac{1}{2} (t-a)^T\Sigma^{-1}(t-a)\right) \sim 
    \exp\left(a^T \Sigma^{-1} t - \frac{1}{2} t^T\Sigma^{-1} t  \right).\\
    \text{Получается, }
    \left\{
    \begin{aligned}
    \Sigma^{-1} = \frac{1}{\gamma^2} E + \frac{1}{\sigma^2} Z^T Z,\\
    a^T \Sigma^{-1} = \frac{1}{\sigma^2} \mathbf{x}^T Z.
    \end{aligned}
    \right.
\end{gather*}
Транспонируя второе равенство (благо $\Sigma$ симметрична, и на неё это не повлияет), получаем оценку
\[
\me[](\theta|X) = \left(Z^T Z + \frac{\sigma^2}{\gamma^2} E\right)^{-1} Z^T X.
\]
Получили практически решение задачи обычной линейной регрессии, но теперь к матрице $Z^T Z$ добавляется единичная с некоторой константой. Это и позволяет получать более адекватную оценку в случае, если эта матрица близка к вырожденной. Это происходит из-за того, что добавление такой матрицы сдвигает все собственные числа $Z^T Z$ на $\frac{\sigma^2}{\gamma^2}$ вправо, отчего определитель, как произведение собственных чисел, отдаляется от нуля. 
\end{solution}

\begin{problem}
Возьмите следующее априорное распределение: компоненты вектора $\theta$ независимы, $\theta_i \sim Laplace(\lambda)$. Найдите апостериорную плотность $\rho(\theta|X)$ (с точностью до константы). Сформулируйте задачу оптимизации $\rho(\theta|X) \to max_{\theta}$.
\end{problem}

\begin{solution}
Теперь имеем
\[
q(t) \sim \exp\left(-\lambda\sum_{i=1}^k |t_i|\right) = \exp\left(-\lambda \|t\|_1\right).
\]
Следовательно, апостериорная плотность имеет вид
\[
\rho_{\theta, X}(t|\mathbf{x}) \sim 
\exp\left(-\lambda \|t\|_1 - \frac{1}{2\sigma^2} (\mathbf{x} - Zt)^T (\mathbf{x} - Zt)\right).
\]
Соответствующую задачу оптимизации можно сформулировать так:
\[
\|X - Z \theta\|^2 + \frac{\sigma^2}{\lambda} \| \theta\|_1 \to \min_{\theta}
\]
Здесь имеется похожая регуляризация, что и в задаче \ref{ridge}, но теперь мы пытаемся ограничить вектор параметров по $\|\cdot\|_1$-норме (так называемая \textsf{Lasso regression}). Такой подход помимо всего прочего обладает свойством <<отбора признаков>> (подробнее смотрите в курсе Машинного обучения).
\end{solution}


