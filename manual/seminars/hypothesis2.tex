\section{Тысяча и один критерий}

В этом параграфе мы обсудим несколько полезных критериев. Часть из них строится для простых основных гипотез, то есть нам надо проверить, совпадает ли истинное распределение с некоторым данным. Такие критерии ещё называют \textit{критериями согласия}. Также заметим, что некоторые критерии существенно используют тот факт, что исходная выборка достаточно большая, например, их размер может быть довольно большим при маленькой выборке и стремится к чему-то более адекватному при увеличении размера выборки. Поэтому полезным будет следующее

\begin{definition}
Критерий $R$ для проверки гипотезы $H_0\colon \pth[] = \pth[0]$ называется \textit{асимптотическим критерием уровня значимости $\alpha$}, если
\[
\varlimsup_{n \to \infty} \pth[0](R) \le \alpha.
\]
\end{definition}

\subsection{Критерий Колмогорова}

Поставим на проверку гипотезу
\[
H_0\colon \pth[] = \pth[0] \vs H_1\colon \pth[] \ne \pth[0].
\]
Как мы знаем из теоремы Гливенко-Кантелли, эмпирическая функция распределения $\widehat{F}_n$ равномерно сходится к истинной функции распределения $F$ для почти всех выборок $X = (X_1, \ldots, X_n, \ldots)$, то есть
\[
D_n = \sup_{x\in \R} |\widehat{F}_n(x) - F(x)| \stackrel{\text{п.н.}}{\longrightarrow} 0.
\]
Таким образом, судить о выполнимости гипотезы $H_0$ можно исходя из того, насколько похожи $\widehat{F}_n$ и $F$. Оказывается, эта сходимость порядка $1 / \sqrt{n}$, и при этом распределение $\sqrt{n} D_n$ отнюдь не рандомное.

\begin{theorem*}[Колмогоров]
Пусть $F$ -- непрерывная функция распределения. Тогда распределение $\sqrt{n} D_n$ не зависит от $F$ и слабо сходится к \textit{распределению Колмогорова} с функцией распределения
\[
K(t) = 1 + 2\sum_{k=1}^{\infty} (-1)^k e^{-2k^2t^2},\;\;\;t>0.
\]
\end{theorem*}

\begin{wrapfigure}[12]{l}{230pt}
    \begin{asy}
        import graph;
        size(220,0);
        
        real start=-0.15, finish = 2.15;
        
        
        real f(real x) {
            real res = 1;
            for (int k = 1; k <= 7; k += 1) {
                if (k % 2 == 0)
                    res += 2 * exp(-2 * k * k * x * x);
                else
                    res -= 2 * exp(-2 * k * k * x * x);
            }
            return res;
        }
        
        draw((0, 1)--(finish, 1), dashed);
        draw((0, 0)--graph(f, 0.3, finish), red);
        
        yaxis("$y$",EndArrow, ymax=1.25);
        xaxis("$x$",EndArrow, xmin=start, xmax=finish);
        
        label("$0$", (0, 0), SW);
        label("$K(x)$", (1, 0.5), E, red);
        label("$1$", (0, 1), W);
    \end{asy}
\end{wrapfigure}

Из теоремы следует, что если при большом $n$ статистика $D_n$ достаточно большая, то это является существенным доводом против $H_0$, то есть критерий для проверки этой гипотезы имеет вид
\[
R = \left\{\sqrt{n} D_n \ge k_{1-\alpha}\right\},
\]
где $k_p$ -- $p$-квантиль распределения Колмогорова (пощупать его можно \href{https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test}{здесь}).

Слабая сходимость распределений из теоремы Колмогорова позволяет сказать, что сей критерий имеет асимптотический уровень значимости $\alpha$. Что же насчёт других свойств?

\begin{problem}
Выведите состоятельность критерия Колмогорова из теоремы Колмогорова.
\end{problem}

\begin{solution}
Сделаем допущение, что в качестве альтернативы мы берём непрерывные распределения, отличающиеся от взятого $F$ (не непрерывные рассматривать нет смысла). Пусть истинная функция распределения равна $G \ne F$, эмпирическую же обозначим как $\widehat{G}_n$. В таком случае статистику $D_n$ можно оценить следующим образом:
\begin{gather*}
    D_n = \sup_{x \in \R} |\widehat{G}_n(x) - F(x)| \ge 
    \sup_{x \in \R} \left[ |G(x) - F(x)| - |\widehat{G}_n(x) - G(x)| \right] \ge\\ \ge  \sup_{x \in \R} |G(x) - F(x)| - \sup_{x \in \R} |\widehat{G}_n(x) - G(x)| = c - D'_n,
\end{gather*}
причём $c = \sup_{x \in \R} |G(x) - F(x)| \ne 0$, а $D'_n = \sup_{x \in \R} |\widehat{G}_n(x) - G(x)|$ при домножении на $\sqrt{n}$ слабо сходится к распределению Колмогорова по одноимённой теореме. В таком случае
\begin{gather*}
    \pth[](\sqrt{n} D_n \ge k_{1-\alpha}) \ge \pth[](\sqrt{n} (c - D'_n) \ge k_{1-\alpha}) = \pth[](\sqrt{n} D'_n \le c\sqrt{n} - k_{1-\alpha}).
\end{gather*}
Правая часть неравенства в скобках стремится к бесконечности, поэтому для любого $\epsilon$ найдётся $N$, что $\forall n \ge N\colon c\sqrt{n} - k_{1-\alpha} \ge 1/\epsilon$, и для таких $n$:
\[
\pth[](\sqrt{n} D'_n \le c\sqrt{n} - k_{1-\alpha}) \ge \pth[](\sqrt{n} D'_n \le 1/\epsilon) \to K(1/\epsilon).
\]
Беря сколь угодно малый $\epsilon$, мы устремим $K(1/\epsilon)$ к единице, а значит, $\pth[](\sqrt{n} D_n \ge k_{1-\alpha}) \to 1$, и критерий состоятелен.
\end{solution}

\begin{problem}\label{kolm_test}
Дана выборка из неизвестного распределения: 1.63, 1.95, 1.14, 1.8, 0.19, 0.32, 1.3, 1.51, 0.03, 1.64, 1.75, 0.23, 0.36, 0.41, 1.49, 1.13, 1.81, 1.4, 1.45, 1.22. Проверьте гипотезу о том, что распределение, из которого взята выборка, имеет плотность $\rho(x) = \frac{x}{2}I(x\in[0;2])$. Уровень значимости выберете сами.
\end{problem}

\begin{solution}
Для более простого подсчёта есть относительно простая формула для статистики $D_n$, которая следует из кусочно-постоянности $\widehat{F}_n$:
\[
D_n = \max_{1 \le i \le n} \left\{ \frac{i}{n} - F(X_{(i)}), F(X_{(i)}) - \frac{i - 1}{n} \right\}.
\]
С помощью формулы и пары строк на Python'e несложно убедиться, что для данной выборки и $F(x) = x^2 / 4$ ($x \in [0; 2]$) мы имеем $D_n \approx 0.258$, а значит, $\sqrt{n}D_n \approx 1.1537$. Несложно загуглить, что $k_{0.85} \approx 1.138$, $k_{0.9} \approx 1.224$. Таким образом, \href{https://en.wikipedia.org/wiki/P-value}{минимальный уровень значимости, на котором мы должны отвергнуть основную гипотезу} (см. раздел \ref{p-value}), находится между $0.1$ и $0.15$, что весьма много (обычно берут $0.05$ или $0.01$), поэтому основания для отвержения $H_0$ нет.

Полезно оценить верность нашего решения на картинке:
\begin{center}
    \begin{asy}
        import graph;
        size(300,0);
        
        real[] A = {0.03, 0.19, 0.23, 0.32, 0.36, 0.41, 1.13, 1.14, 1.22, 1.3, 1.4, 1.45, 1.49, 1.51, 1.63, 1.64, 1.75, 1.8, 1.81, 1.95};
        real start = -0.15;
        real finish = 2.15;
        int n = 20;
        
        draw((0, 1)--(finish, 1), dashed);
        
        draw((start, 0)--(A[0], 0), blue);
        
        for (int i = 0; i <= 18; i += 1) {
            draw((A[i], i / n)--(A[i], (i+1) / n), dotted+blue);
            draw((A[i], (i+1) / n)--(A[i+1], (i+1) / n), blue);
        }
        draw((A[19], 19 / n)--(A[19], 1), dotted+blue);
        draw((A[19], 1)--(finish, 1), blue);
        
        real f(real x) {return x * x / 4;}
        
        draw((start, 0)--graph(f, 0, 2)--(finish, 1), red);
        
        yaxis("$y$",EndArrow, ymax=1.15);
        xaxis("$x$",EndArrow, xmin=start, xmax=finish);
        
        label("$F(x)$", (1.7, 0.5), red);
        label("$\widehat{F}(x)$", (0.9, 0.5), blue);
        
        draw((0.41, f(0.41))--(0.41, 0.3), EndArrow, BeginArrow);
        label("$D_n$", (0.41, (0.3 + f(0.41)) / 2), E);
        label("$0$", (0, 0), SW);
        label("$1$", (0, 1), W);
    \end{asy}
\end{center}
Как можно видеть, рядом с нулём распределения уж необычно сильно различаются, но критерий Колмогорова это не смутило.
\end{solution}

\begin{problem}
Докажите, что в критерии Колмогорова при справедливости нулевой
гипотезы статистика $D_n$ имеет некоторое фиксированное распределение одинаковое для всех $F(x)$.

\textit{Указание.} Имеет смысл сделать замену $y = F(x)$, $x = F^{-1}(y)$.
\end{problem}

\begin{solution}
Как таковой обратной функции у $F$ может и не быть (она может быть постоянной на некотором промежутке), поэтому определим
\[
F^{-1}(y) = \sup \left\{x\colon F(x) \le y\right\}.
\]
Статистику $D_n$ можно переписать как
\[
D_n = \sup_{x\in\R} |\widehat{F}_n(x) - F(x)| = \sup_{y\in[0;1]} |\widehat{F}_n(F^{-1}(y)) - y|,
\]
так как из отрезка постоянства $y = F(x)$ не может быть элементов выборки, поэтому на нём эмпирическая функция распределения также постоянна.

Посмотрим, как распределена $\widehat{F}_n(F^{-1}(y))$:
\[
\widehat{F}_n(F^{-1}(y)) = \sum_{i=1}^n I(F^{-1}(y) \ge X_i) = \sum_{i=1}^n I(y \ge F(X_i)),
\]
а $F(X_i)$, как известно из задачи \ref{uniform}, распределено равномерно на $[0;1]$. Таким образом, $D_n$ выражается через независимые величины $F(X_i)$ с одним и тем же распределением, поэтому её распределение тоже однозначно определенно.
\end{solution}

\begin{problem}[*]
Убедитесь, что $k_{1-\alpha} \sim \sqrt{-\frac{1}{2}\ln{\frac{\alpha}{2}}}$ при $\alpha \to 0$, где $k_{1-\alpha}$ -- $(1-\alpha)$-квантиль распределения Колмогорова.
\end{problem}

\begin{solution}
To be continued...
\end{solution}

\subsection{Критерий $\chi^2$ Пирсона}

Рассмотрим наблюдение из \href{https://en.wikipedia.org/wiki/Multinomial_distribution}{мультиномиального распределения} с параметрами $n$, $k$ и $\mathbf{p} = (p_1, \ldots, p_k)$, которое по сути является обобщением биномиального: проще говоря, у нас есть $k$-гранный кубик, выпадение $i$-ой грани которого происходит с вероятностью $p_i$; мы кидаем этот кубик $n$ раз и записываем в вектор $X = (X_1, \ldots, X_k)$, что первая грань выпала $X_1$ раз, вторая -- $X_2$ раз и т. д.. Можно также интерпретировать как
\[
X_i = \sum_{j=1}^n I(B_j = i),
\]
где $B_j$ -- результат $j$-ого броска, которые независимы между собой.

Пусть мы наблюдаем вектор $(X_1, \ldots, X_k)$ с таким распределением и хотим проверить гипотезу
\[
H_0\colon \mathbf{p} = \mathbf{p}^0 = (p_1^0, \ldots, p_k^0) \vs H_1\colon \mathbf{p} \ne \mathbf{p}^0.
\]
По ЦПТ мы знаем, что при выполнимости гипотезы $H_0$
\[
\frac{X - n \mathbf{p}^0}{\sqrt{n}} \stackrel{d}{\longrightarrow} \mathcal{N}(0, \Sigma),
\]
где $\Sigma$ -- какая-то там матрица ковариаций (если быть точнее, $\Sigma_{ii} = \va[] I(B_1 = i) = p_i^0(1 - p_i^0)$, $\Sigma_{ij} = \cov{(I(B_1 = i), I(B_1 = j))} = -p_i^0 p_j^0$ для  $i \ne j$). Таким образом, компоненты вектора в левой части, то есть
\[
\frac{X_i - n p_i^0}{\sqrt{n}},
\]
распределены почти что нормально. Тогда давайте в качестве меры отклонения от гипотезы $H_0$ возьмём взвешенную сумму квадратов компонент этого вектора:
\begin{definition}
\textit{Статистикой хи-квадрат Пирсона} называется
\[
\chi^2(X) = \sum_{i=1}^k \frac{(X_i - n p_i^0)^2}{n p_i^0}.
\]
\end{definition}
Логично предположить, что как сумма квадратов почти что нормально распределённых величин эта статистика стремится по распределению к хи-квадрат. Что ж, так оно и есть, хоть и доказывается это не тривиально.

\begin{theorem*}
Если $H_0$ верна, то $\chi^2(X) \stackrel{d}{\longrightarrow} \zeta \sim \chi^2_{k-1}$.
\end{theorem*}

\begin{proof}[Идея]
    Главная задача -- доказать утверждение из задачи \ref{pirson_stat}, а остальное -- дело техники.
\end{proof}

Итого, в качестве критерия проверки $H_0$ асимптотического уровня значимости $\alpha$ можно взять
\[
R = \left\{\mathbf{x}\colon \chi^2(\mathbf{x}) > x_{1-\alpha}\right\},
\]
где $x_{p}$ -- $p$-квантиль распределения $\chi^2_{k-1}$. Следует помнить, что критерий этот -- асимптотический, а значит, пользоваться им на малой выборки имеет мало смысла. Обычно критерий $\chi^2$ используют при $n \ge 50$ и $np^0_i \ge 5$ для всех $i=1,\ldots, k$.

\begin{example*}[ (третий закон Менделя)]
Согласно наблюдениям, проведённым биологом Г. Менделем, разные признаки наследуются независимо друг от друга. Попробуем убедиться в этом статистически.

Предположим, у семейства гороха имеется два признака: цвет (жёлтый и зелёный) и форма (круглая или морщинистая). Скрещиваются два вида гороха: с доминантными признаками (жёлтые круглые горошины) и рецессивными (зелёные морщинистые горошины). По отдельности в результате селекции признаки распределяются в отношении $3:1$ (по второму закону Менделя), поэтому если третий закон Менделя верен, то распределение двух признаков будет иметь вид $9:3:3:1$.

Проведено $n = 556$ наблюдений. Посмотрим на эту статистику:
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    Тип горошин & Гипотетическая вероятность & Наблюдаемая частота\\
    \hline
    Желтые, круглые & $9/16$ & $315/556$\\
    \hline
    Желтые, морщинистые & $3/16$ & $101/556$\\
    \hline
    Зелёные, круглые & $3/16$ & $108/556$\\
    \hline
    Зелёные, морщинистые & $1/16$ & $32/556$\\
    \hline
    \end{tabular}
\end{center}

В наших обозначениях это значит, что вектор наблюдений равен $X = (315, 101, 108, 32)$, и проверяется гипотеза
\[
H_0\colon \mathbf{p} = \mathbf{p}^0 = (9/16, 3/16, 3/16, 1/16).
\]
Посчитаем статистику Пирсона:
\begin{gather*}
    \chi^2(X) = \frac{(315 - 556 \cdot 9/16)^2}{556 \cdot 9/16} + \frac{(101 - 556 \cdot 3/16)^2}{556 \cdot 3/16} + \frac{(108 - 556 \cdot 3/16)^2}{556 \cdot 3/16} + \frac{(32 - 556 \cdot 1/16)^2}{556 \cdot 1/16} \approx 0.47.
\end{gather*}
Если в качестве допустимого уровня значимости взять $\alpha = 0.05$, то пороговым значением для критерия Пирсона будет $(1-\alpha)$-квантиль для $\chi^2_3$, что есть примерно $7.815$. Наблюдаемое значение гораздо меньше порогового значения, а значит, причин для отвержения гипотезы $H_0$ нет.
\end{example*}

\begin{problem}
Среди первых 800 цифр числа $\pi$ цифры 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 встречаются соответственно 74, 92, 83, 79, 80, 73, 77, 75, 76, 91 раз. Проверьте при помощи критерия хи-квадрат гипотезу о том, что различные цифры встречаются в числе $\pi$ равновероятно. Рассмотрите уровни значимости $\alpha = 0.05$, $\alpha = 0.1$.
\end{problem}

\begin{solution}
Python подсказывает, что для данной выборки статистика хи-квадрат равна $\chi^2(X) = 5.125$. Сверимся с квантилями для $\chi^2_9$: $x_{0.95} \approx 16.92$, $x_{0.9} \approx 14.68$. Как видим, значение статистики и близко не подошло к границам критерия.

{
\footnotesize На самом деле несложно с помощью scipy.stats убедиться, что минимальный уровень значимости, на котором мы бы отвергли гипотезу, равен примерно $0.823$ (а если точнее, $1 - F_{\chi^2_9}(5.125)$), то есть при проверке гипотезы мы позволяем себе ошибку I рода в размере $82.3\%$ (!!!), что катастрофически много.
}
\end{solution}

Отметим, что критерий $\chi^2$ применяется далеко не только к модели выше. Он также позволяет проверять гипотезы о равенстве истинной функции распределения какой-то данной. Как же это происходит?

Пусть нам выборка $X_1, \ldots, X_n$ из некоторого неизвестного нам распределения $F(x)$. Мы же в свою очередь хотим проверить, не является ли эта функция чем-то хорошим, то есть проверяем
\[
H_0\colon F(x) = F_0(x).
\]

\begin{wrapfigure}[13]{l}{230pt}
    \begin{asy}
        import graph;
size(220,0);
        
real start=-0.15, finish = 2.15;


real f(real x) {
    real res = 1;
    for (int k = 1; k <= 7; k += 1) {
        if (k % 2 == 0)
            res += 2 * exp(-2 * k * k * x * x);
        else
            res -= 2 * exp(-2 * k * k * x * x);
    }
    return res;
}

draw((0, 1)--(finish, 1), dashed);
draw((0, 0)--graph(f, 0.3, finish), black);

yaxis("$y$",EndArrow, ymax=1.25, ymin=0);
xaxis("$x$",EndArrow, xmin=start, xmax=finish);

draw((0.4, 0.05)--(0.4, -0.05));
draw((0.9, 0.05)--(0.9, -0.05));
draw((1.7, 0.05)--(1.7, -0.05));

draw((0.4, -0.5)--(0.4, 0), dashed);
draw((0.9, -0.5)--(0.9, 0), dashed);
draw((1.7, -0.5)--(1.7, 0), dashed);

draw((start, -0.5)--(0.4, -0.5), EndArrow);
label("$\Delta_1$", (0.1, -0.5), N, red);
draw((0.4, -0.5)--(0.9, -0.5), BeginArrow, EndArrow);
label("$\Delta_2$", (0.65, -0.5), N, red);
draw((0.9, -0.5)--(1.7, -0.5), BeginArrow, EndArrow);
label("$\Delta_3$", (1.3, -0.5), N, red);
draw((1.7, -0.5)--(finish, -0.5), BeginArrow);
label("$\Delta_4$", (1.95, -0.5), N, red);

dot((0.3, 0), filltype=FillDraw(drawpen=blue));
label("\small$X_{(1)}$", (0.3, 0), N, blue);
dot((0.5, 0), filltype=FillDraw(drawpen=blue));
dot((0.67, 0), filltype=FillDraw(drawpen=blue));
dot((0.61, 0), filltype=FillDraw(drawpen=blue));
dot((0.83, 0), filltype=FillDraw(drawpen=blue));
dot((0.94, 0), filltype=FillDraw(drawpen=blue));
dot((0.75, 0), filltype=FillDraw(drawpen=blue));
dot((0.97, 0), filltype=FillDraw(drawpen=blue));
dot((1.18, 0), filltype=FillDraw(drawpen=blue));
dot((1.1, 0), filltype=FillDraw(drawpen=blue));
dot((1.3, 0), filltype=FillDraw(drawpen=blue));
dot((1.67, 0), filltype=FillDraw(drawpen=blue));
label("\small$X_{(n)}$", (1.67, 0), N, blue);


label("$0$", (0, 0), SW);
label("$F(x)$", (1, 0.5), E);
label("$1$", (0, 1), W);
    \end{asy}
\end{wrapfigure}

Разобьём числовую прямую на $k$ дизъюнктных множеств $\Delta_1, \ldots, \Delta_k$ (чаще всего берут полуинтервалы $\Delta_i = (a_i; b_j]$, возможно и бесконечные). В данные интервалы как-то попали наши точки: пусть в $i$-ое множество $\Delta_i$ попало $v_i$ точек. При этом в идеальном мире (и при верности $H_0$) вероятность попасть в $\Delta_i$ равна $p_i^0=\int_{\Delta_i}dF_0(x) = F_0(b_i) - F_0(a_i)$. Это и сводит текущую задачу к задаче выше: для каждого элемента выборки на гранях $k$-гранного кубика написано, в какой полуинтервал оно попадёт, и гипотеза заключается в том, что вероятность выпадания определённой грани равна установленному числу. Получается, критерий имеет вид
\[
R = \left\{\sum_{i=1}^k\frac{(v_i-np_i^0)^2}{np_i^0} > x_{1-\alpha}\right\}.
\]

Конечно же, если критерий $\chi^2$ не отверг гипотезу, то нам это ровным счётом ни о чём не говорит. Мы могли разделить прямую как-то не очень удачно, из-за чего истинное распределение может легко мимикрировать под данное, имея одинаковые с ним вероятности промежутков $\Delta_i$. Отсюда представляется логичным брать не слишком мало интервалов, чтобы мы смогли обнаружить различия между распределениями. Но и слишком маленькими их делать не следует, потому что тогда в некоторые интервалы может в теории не попасть ни одна точка, что на корню убивает предположение о нормальности $(v_i - np^0_i)/\sqrt{n}$. Обычно берут $k \approx \log_2{n}$. 

\begin{example*}
    Решим задачку \ref{kolm_test} с помощью критерия $\chi^2$. Будем делить область значений на $4$ ($\approx \log_2{20}$) части. Встаёт вопрос: как именно разбить отрезок $[0; 2]$, откуда приходят значения выборки? Для начала рассмотрим самый простой вариант: брать равные по длине отрезки. Этот вариант имеет право на существование, однако следует помнить об ограничении $np_i^0 \ge 5$, без которого результаты нельзя назвать точными. У нас это и подавно не выполняется, поэтому продолжим.

    В отрезки $[0; 0.5]$, $[0.5; 1]$, $[1; 1.5]$ и $[1.5; 2]$ попало соответственно 6, 0, 7 и 7 элементов выборки. При верности основной гипотезы их вероятности равны соответственно $1/16$, $3/16$, $5/16$ и $7/16$ (просто считаем интеграл плотности на этих отрезках). Статистика хи-квадрат равна
    \[
    \chi^2(X) = \frac{(6 - 20 \cdot 1/16)^2}{20 \cdot 1/16} + \frac{(0 - 20 \cdot 3/16)^2}{20 \cdot 3/16} + \frac{(7 - 20 \cdot 5/16)^2}{20 \cdot 5/16} + \frac{(7 - 20 \cdot 7/16)^2}{20 \cdot 7/16} = 22.24,
    \]
    в то время как $0.95$-квантиль распределения $\chi^2_3$ равен $\approx 7.81$, а p-value (см. раздел \ref{p-value}) равен $\approx 5.81 \cdot 10^{-5}$, поэтому на уровне значимости $0.05$ гипотеза отвергается.

    Рассмотрим другой способ: разобьём отрезок на равновероятные части. Такими отрезками будут $[0; 1]$, $[1; \sqrt{2}]$, $[\sqrt{2}; \sqrt{3}]$ и $[\sqrt{3}; 2]$ (их концы -- прообразы точек $0.25$, $0.5$, $0.75$ функции распределения $F_0(x) = x^2/4$). В эти отрезки попадёт соответственно $6$, $5$, $5$ и $4$ элементов выборки, и тогда $\chi^2(X) = 0.4$, что уже меньше $0.95$-квантиля, поэтому гипотеза не отвергается. Но смею напомнить, что мощность такого критерия весьма низкая, поэтому этот результат не даёт нам никакой информации.
\end{example*}

Напоследок приведём доказательство факта, из которого следует теорема выше.

\begin{problem}[*]\label{pirson_stat}
Дан вектор $\mathbf{p} = (p_1, \ldots, p_n)^T$, причём $\sum p_i = 1$. Определим матрицу
\[
\Sigma =
\begin{pmatrix}
p_1 & 0 & \cdots & 0\\
0 & p_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & p_n
\end{pmatrix} - \mathbf{p} \mathbf{p}^T.
\]
Рассмотрим гауссовский вектор $\xi \sim \mathcal{N}(0,\Sigma)$. Докажите, что
\[
\sum_{i=1}^n \frac{\xi_i^2}{p_i} \sim \chi^2_{n-1}.
\]
\end{problem}

\begin{solution}
Нормальное решение лучше смотреть в \cite{savelov}, тут приведено решение автора сего конспекта.

Для начала нормируем векторы $\xi_i \mapsto \xi_i / \sqrt{p_i}$, чтобы из $\sum \xi^2_i / p_i$ сделать просто $\sum \xi^2_i$. Поэтому будем считать, что ковариационная матрица имеет вид
\[
\Sigma = E_n - \sqrt{\mathbf{p}} \sqrt{\mathbf{p}}^T,
\]
где $\sqrt{\mathbf{p}}$ означает результат поэлементного взятия корня из $\mathbf{p}$, $E_n$ -- единичная матрица размера $n$.

Как мы знаем из линала, существует ОНБ, в котором квадратичная форма принимает диагональный вид, то есть существует некоторая ортогональная матрица $S$ такая, что $S\Sigma S^T = D$, где $D$ -- диагональная. На диагонали стоят собственные числа $\Sigma$, как их найти? Для это достаточно понять, что многочлен $P(t) = t(t - 1) = t^2 - t$ является аннулирующим для $\Sigma$:
\begin{gather*}
  \Sigma^2 - \Sigma = (E_n - \sqrt{\mathbf{p}} \sqrt{\mathbf{p}}^T)^2 - E_n + \sqrt{\mathbf{p}} \sqrt{\mathbf{p}}^T = - \sqrt{\mathbf{p}} \sqrt{\mathbf{p}}^T + \sqrt{\mathbf{p}} \sqrt{\mathbf{p}}^T \sqrt{\mathbf{p}} \sqrt{\mathbf{p}}^T  = \\
  = \sqrt{\mathbf{p}} (\underbrace{\sqrt{\mathbf{p}}^T \sqrt{\mathbf{p}}}_{=\sum p_i=1} \,-\, E_1)\sqrt{\mathbf{p}}^T = 0.
\end{gather*}

Легко также видеть, что $P(t)$ минимален (куда уж меньше?). Характеристический многочлен делится на минимальный, причём все корни характеристического автоматом являются корнями минимального, поэтому у $\Sigma$ собственными числами будут только 0 и 1. Осталось понять, какой они кратности.

Ранг при домножении на невырожденную матрицу не меняется, поэтому $\rk \Sigma = \rk D = $ кол-во единиц в $D$. Мы знаем, что 0 точно собственное число $\Sigma$, то есть единиц не больше $n - 1$. Оценка снизу берётся из неравенства $\rk(A+B) \le \rk A + \rk B$:
\[
n = \rk E_n \le \rk{\sqrt{\mathbf{p}} \sqrt{\mathbf{p}}^T} + \rk{\Sigma} = 1 + \rk{\Sigma} \Longrightarrow \rk{\Sigma} \ge n - 1.
\]

Таким образом,
\[
D = \begin{pmatrix}
E_{n-1} & 0\\
0 & 0
\end{pmatrix}.
\]

Зачем всё это было надо? А вот зачем: вектор $\eta = S \xi$ как линейное преобразование над гауссовским вектором имеет распределение $\mathcal{N}(0, D)$. Тогда его компоненты не коррелированы, а значит, независимы (по свойству гауссовского вектора), причём одна из координат распределена как нуль из-за одного нуля на диагонали, а остальные -- стандартно нормально. Ну и самое приятное: ортогональное преобразование не меняет норму вектора, поэтому
\[
\sum_{i=1}^n \xi_i^2 = \sum_{i=1}^n \eta^2_i = \sum_{i=1}^{n-1} \eta^2_i \sim \chi^2_{n-1}
\]
как сумма квадратов независимых величин с распределением $\mathcal{N}(0, 1)$.

\end{solution}

\subsection{Линейные гипотезы в линейной регрессии}

Вернёмся к модели гауссовской линейной регрессии:
\[
X = Z\theta + \epsilon,
\]
где $\epsilon \sim \mathcal{N}(0, \sigma^2 E)$, причём $\theta\in\R^k$ и $\sigma^2$ -- неизвестные параметры, $X \in \R^n$ -- наблюдение, $Z \in \R^{n\times k}$ -- матрица признаков. Наша гипотеза будет состоять в предположении, что $\theta$ лежит в некоторой гиперплоскости, то есть 
\[
H_0\colon T\theta = \tau,
\]
где $T\in\R^{m \times k}$, $\tau \in \R^m$ -- известные величины, причём будем допускать, что $\rk{T} = m \le k$. Отсюда собственно и название гипотезы: мы накладываем некоторые линейные ограничения на параметр $\theta$.

Напомним, что в предыдущих параграфах мы получили оценку
\[
\widehat{\theta} = (Z^T Z)^{-1} Z^T X.
\]
Из задачи \ref{cov_mat} мы знаем матожидание и ковариационную матрицу у $\widehat{\theta}$, а значит, можем найти её и у $T\widehat{\theta}$:
\begin{gather*}
    \me[\theta, \sigma^2] T\widehat{\theta} = T\me[\theta, \sigma^2] \widehat{\theta} = T\theta,\\
    \va[\theta, \sigma^2] T\widehat{\theta} = T \left[\va[\theta, \sigma^2] \widehat{\theta}\right] T^T = \sigma^2 \underbrace{T (Z^T Z)^{-1} T^T}_{\phantom{0}=B} = \sigma^2 B.
\end{gather*}
$T\widehat{\theta}$, как линейное преобразование над нормально распределённым $\widehat{\theta}$, само нормально распределено, то есть
\[
T\widehat{\theta} \sim \mathcal{N}(T\theta, \sigma^2 B).
\]
Так как матрица $B$ положительная определена, то у неё существует $\sqrt{B}$, а значит,
\begin{gather*}
    \frac{1}{\sigma} \sqrt{B}^{-1} (T\widehat{\theta} - T\theta) \sim \mathcal{N}(0, E) \Longrightarrow\\
    \left\|\frac{1}{\sigma} \sqrt{B}^{-1} (T\widehat{\theta} - T\theta)\right\|^2 = \frac{1}{\sigma^2}(T\widehat{\theta} - T\theta)^T B^{-1} (T\widehat{\theta} - T\theta) \sim \chi^2_m.
\end{gather*}

При верности гипотезы $T\theta = \tau$, а значит, статистика от $\widehat{\theta}$
\[
\frac{1}{\sigma^2}(T\widehat{\theta} - \tau)^T B^{-1} (T\widehat{\theta} - \tau) \sim \chi^2_m.
\]
Вспомним, что у нас в запасе есть независящая от $\widehat{\theta}$ статистика
\[
\frac{1}{\sigma^2} \|X - Z \widehat{\theta}\|^2 \sim \chi^2_{n-k}.
\]
Поделив одно на другое, мы избавимся от неизвестной $\sigma^2$, да ещё и получим <<хорошее распределение>>:
\begin{definition}
Пусть независимые случайные величины $\xi$ и $\eta$ таковы, что $\xi \sim \chi^2_{a}$, $\eta \sim \chi^2_{b}$, где $a, b\in \N$. Тогда говорят, что случайная величина
\[
\zeta = \frac{\xi / a}{\eta / b}
\]
имеет \textit{распределение Фишера со степенями свободы $a$ и $b$}. Обозначается $\zeta \sim F_{a, b}$
\end{definition}

Тогда при верности $H_0$ имеем
\[
\frac{(T\widehat{\theta} - \tau)^T B^{-1} (T\widehat{\theta} - \tau)}{\|X - Z \widehat{\theta}\|^2}\cdot \frac{n - k}{m} \sim F_{m, n-k}.
\]

Итоговый критерий записывается так:
\[
R = \left\{\frac{(T\widehat{\theta} - \tau)^T B^{-1} (T\widehat{\theta} - \tau)}{\|X - Z \widehat{\theta}\|^2}\cdot \frac{n - k}{m} > f_{1-\alpha}\right\},
\]
где $f_{p}$ -- $p$-квантиль распределения Фишера со степенями свободы $m$ и $n-k$.

\begin{example*}
    Допустим, нам пришли две независимые выборки: $X_1, \ldots, X_n$ и $Y_1, \ldots, Y_m$, элементы которых имеют распределение $\mathcal{N}(a, \sigma^2)$ и $\mathcal{N}(b, \sigma^2)$ соответственно. Хотелось бы проверить гипотезу
    \[
    H_0\colon a = b.
    \]

    Выборки можно рассматривать как общую выборку из модели гауссовской линейной регрессии, а $a$ и $b$ -- как координаты одного вектора параметров $\theta$. Таким образом:
    \[
    \begin{pmatrix}
        X_1\\ \vdots \\ X_n \\ Y_1\\ \vdots \\ Y_m
    \end{pmatrix} = Z\theta + \epsilon = 
    \begin{pmatrix}
        1 & 0\\ \vdots & \vdots \\ 1 & 0 \\ 0 & 1\\ \vdots & \vdots \\ 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        a \\ b
    \end{pmatrix} + \epsilon,
    \]
    где $\epsilon \sim \mathcal{N}(0, \sigma^2 E_{n+m})$. Стало быть, $H_0$ есть линейная гипотеза:
    \[
    H_0\colon T\theta =  \begin{pmatrix}
        1 & -1
    \end{pmatrix}
    \begin{pmatrix}
        a\\ b
    \end{pmatrix}
    = 0 = \tau.
    \]
    Найдём величины, участвующие в критерии выше:
    \begin{gather*}
        Z^T Z = \begin{pmatrix}
            n & 0 \\ 0 & m
        \end{pmatrix},\;\;\;
        \widehat{\theta} = \begin{pmatrix}
            \overline{X}\\ \overline{Y}
        \end{pmatrix},\;\;\;
        B = T(Z^T Z)^{-1} T^T = \frac{1}{n} + \frac{1}{m},\\
        (T\widehat{\theta} - \tau)^T B^{-1} (T\widehat{\theta} - \tau) = \frac{nm(\overline{X} - \overline{Y})^2}{n+m},\;\;\;
        \|X - Z \widehat{\theta}\|^2 = \sum_{i=1}^{n} (X_i - \overline{X})^2 + \sum_{j=1}^{m} (Y_j - \overline{Y})^2.
    \end{gather*}
    Таким образом, критерий для проверки $H_0$ имеет вид
    \[
    R = \left\{(\mathbf{x}, \mathbf{y})\colon \frac{nm(n+m-2)(\overline{\mathbf{x}} - \overline{\mathbf{y}})^2}{(n+m)\left(\sum_{i=1}^{n} (x_i - \overline{\mathbf{x}})^2 + \sum_{j=1}^{m} (y_j - \overline{\mathbf{y}})^2\right)} > f_{1-\alpha}\right\},
    \]
    где $f_{p}$ -- $p$-квантиль распределения Фишера со степенями свободы $1$ и $n+m-2$.
\end{example*}

\renewcommand{\thesubsection}{\thesection.\arabic{subsection}*}

\subsection{Критерий Вальда (z-критерий)}

Следующий материал необязателен, но его коснулись на семинарах. К тому же сей критерий является типичным примером \textit{z-критерия}, то есть такого критерия, статистика которого сходится к чему-то нормальному.

В данном разделе мы рассмотрим, наверное, один из самых простых способов проверки \textit{двусторонних} гипотез, то есть гипотез вида
\[
H_0\colon \theta = \theta_0 \vs H_1\colon \theta \ne \theta_0.
\]

Для построения критерия нам понадобится асимптотически нормальная оценка $\widehat{\theta}$, то есть такая оценка, что
\[
\sqrt{n} \cdot \frac{\widehat{\theta} - \theta}{\sigma(\theta)} \stackrel{d}{\longrightarrow} \mathcal{N}(0, 1),
\]
где $\sigma^2(\theta)$ -- асимптотическая дисперсия оценки $\widehat{\theta}$. Если мы имеем дело с какой-то сложной моделью, то получить точную формулу для $\sigma^2(\theta)$ может быть довольно сложно, поэтому вместо неё будем использовать состоятельную оценку $\widehat{\sigma}$ для $\sigma(\theta)$. В силу состоятельности отношение сих величин сходится по вероятности (а значит, и слабо) к 1, и по лемме Слуцкого:
\[
T_{\theta}(X) = \sqrt{n} \cdot \frac{\widehat{\theta} - \theta}{\widehat{\sigma}} = 
\sqrt{n} \cdot \frac{\widehat{\theta} - \theta}{\sigma(\theta)} \cdot \frac{\sigma(\theta)}{\widehat{\sigma}} \stackrel{d}{\longrightarrow} \mathcal{N}(0, 1).
\]

Вернёмся к проверке гипотезы. При верности $H_0$ имеем $T_{\theta_0}(X) \stackrel{d_{\theta_0}}{\longrightarrow} \mathcal{N}(0, 1)$, а значит, критерий
\[
R = \{\mathbf{x}\colon |T_{\theta_0}(\mathbf{x})| > z_{1 - \alpha/2} \}
\]
будет иметь асимптотический уровень значимости $\alpha$ (здесь $z_p$ -- $p$-квантиль $\mathcal{N}(0, 1)$). Действительно, если за $\Phi$ обозначить функцию распределения для $\mathcal{N}(0, 1)$, то
\begin{gather*}
    \pth[\theta_0](|T_{\theta_0}(X)| > z_{1 - \alpha/2}) =\\
    =\pth[\theta_0](T_{\theta_0}(X) > z_{1 - \alpha/2}) + \pth[\theta_0](T_{\theta_0}(X) < - z_{1 - \alpha/2}) \xrightarrow[\textit{из слаб. сх-ти}]{} (1 - \Phi(z_{1 - \alpha/2})) + \Phi(- z_{1 - \alpha/2}) = \\
    = 1 - (1 - \alpha/2) + (1 - \Phi(z_{1 - \alpha/2})) = \alpha/2 + (1 - (1-\alpha/2)) = \alpha.
\end{gather*}

Теперь изучим критерий на предмет мощности. Предположим, что истинное значение $\theta$ не равно $\theta_0$. Тогда
\begin{gather*}
    \beta(\theta) = \pth(|T_{\theta_0}(X)| > z_{1 - \alpha/2}) =\\
    =\pth\left(\sqrt{n} \cdot \frac{\widehat{\theta} - \theta_0}{\widehat{\sigma}} > z_{1 - \alpha/2}\right) + \pth\left(\sqrt{n} \cdot \frac{\widehat{\theta} - \theta_0}{\widehat{\sigma}} < - z_{1 - \alpha/2}\right) =\\
    = \pth\left(\sqrt{n} \cdot \frac{\widehat{\theta} - \theta}{\widehat{\sigma}} > z_{1 - \alpha/2} + \sqrt{n} \cdot \frac{\theta_0 - \theta}{\widehat{\sigma}}\right) + \pth\left(\sqrt{n} \cdot \frac{\widehat{\theta} - \theta}{\widehat{\sigma}} < - z_{1 - \alpha/2} + \sqrt{n} \cdot \frac{\theta_0 - \theta}{\widehat{\sigma}}\right) \approx\\
    \approx 1 - \Phi\left(z_{1 - \alpha/2} + \sqrt{n} \cdot \frac{\theta_0 - \theta}{\widehat{\sigma}}\right) + \Phi\left(- z_{1 - \alpha/2} + \sqrt{n} \cdot \frac{\theta_0 - \theta}{\widehat{\sigma}}\right).
\end{gather*}

Так как $\theta \ne \theta_0$, то содержимое в скобках стремится к $\pm \infty$, а значит, значения $\Phi$ либо примерно 1, либо примерно 0, отчего мощность близка к единице. Причём из написанного выше видно, что мощность тем больше, чем больше размер выборки и чем дальше от $\theta_0$ находится рассматриваемый параметр из альтернативы.

\begin{problem}\label{small_p_value}
\textbf{(а)} Пусть $X_1, \ldots , X_n$ -- выборка на распределения $Bern(p)$. Предложите $z$-критерий (то есть критерий, распределение статистики которого сходится к нормальному распределению при $n \to \infty$) для проверки гипотезы
\[
H_0 \colon p = p_0 \vs H_1 \colon p \ne p_o.
\]
\textbf{(б)} По данным Интернет-опроса за одного из кандидатов собирались проголосовать 3\% избирателей. По официальным данным за этого кандидата в итоге проголосовали 4661075 из 5818955 избирателей. Нулевая гипотеза заключается в корректности данных Интернет-опроса. На каком уровне значимости можно её принять?
\end{problem}

\begin{solution}
    \textbf{(а)} Критерий Вальда тут подходит идеально. По ЦПТ имеется асимптотически нормальная оценка $\widehat{p} = \overline{X}$, асимптотическую дисперсию которой можно выразить точно и без оценивания: это просто дисперсия одного наблюдения, то есть $\sigma^2(p) = \va[p] X_i = p(1 - p)$. Итого, критерий имеет вид
    \[
    R = \left\{\mathbf{x}\colon \sqrt{n} \cdot \frac{\overline{\mathbf{x}} - p_0}{\sqrt{p_0(1-p_0)}} > z_{1-\alpha/2}\right\}
    \]

    \textbf{(б)} Посчитаем статистику критерия:
    \[
    T(X) = \sqrt{5818955} \cdot \frac{\frac{4661075}{5818955} - 0.03}{\sqrt{0.03 \cdot(1-0.03)}} \approx 10902.83.
    \]

    Кажется, это больше, чем любой адекватный квантиль нормального распределения. Минимальный уровень значимости, на котором мы должны отвергнуть гипотезу, очень близок к нулю (Python при попытке его посчитать выдаёт просто 0), что является невероятно весомым доказательством против гипотезы о корректности Интернет-опроса (ох уж эти опросы!).
\end{solution}

Критерий Вальда подходит и для проверки односторонних гипотез, например:
\[
H_0\colon \theta = \theta_0 \vs H_1\colon \theta > \theta_0.
\]
В таких случаях критерий логично переформулировать так:
\[
R_{+} = \{\mathbf{x} \colon T_{\theta_0}(\mathbf{x}) > z_{1-\alpha}\}.
\]
На асимптотический уровень значимости это не повлияет, зато мы увеличим мощность: теперь мы можем забыть про <<левый хвост>> нормального распределения и больше уделить внимания правому, попадание в который более вероятно для $\theta > \theta_0$. Аналогично, если альтернатива имеет вид $H_1\colon \theta < \theta_0$, то в такой ситуации лучше взять критерий
\[
R_{-} =  \{\mathbf{x} \colon T_{\theta_0}(\mathbf{x}) < -z_{1-\alpha}\}.
\]

\begin{problem}
Посетители ТРЦ Рио ходили по магазинам в среднем $3/4$ часа, стандартное отклонение (a.k.a. корень из дисперсии) было равно $0.1$. Потом на втором этаже появился детский паровозик, а на следующий день оказалось, что по выборке из 35 посетителей среднее время шоппинга составило $4/5$ часа. Требуется проверить на уровне значимости $0.05$ гипотезу о пользе паровозика. Какими будут $H_0$ и $H_1$? Придумайте критерий и проверьте гипотезу.
\end{problem}

\begin{solution}
Выдвинем на проверку
\[
H_0\colon \text{Паровозик не повлиял} \vs H_1\colon \text{Паровозик помог}
\]

Если верна $H_0$, то с появлением паровозика ничего не поменялось, поэтому среднее и отклонение распределения остались прежними, то есть $0.75$ и $0.1$ соответственно. Попробуем применить односторонний критерий Вальда (было бы странно в альтернативу, утверждающую, что паровозик помог, запихивать случай, когда среднее уменьшилось):
\[
T(X) = \sqrt{35}\cdot \frac{0.8 - 0.75}{0.1} \approx 2.958.
\]
Фактический уровень значимости будет примерно равен $1 - \Phi(2.958)$, что равняется $\approx 0.0015$. Это меньше нашего уровня значимости, да и само по себе это весьма маленькое значение, что только больше побуждает отвергнуть $H_0$. С учётом сего факта и того, что средняя продолжительность шоппинга увеличилась, логично сделать вывод, что паровозик принёс пользу.
\end{solution}

\begin{problem}
Пусть $X_1, \ldots , X_n$ -- выборка из распределения $\mathcal{N}(a_1, \sigma^2_1)$, $Y_1, \ldots, Y_m$ -- выборка из распределения $\mathcal{N}(a_2, \sigma^2_2)$, причём выборки независимы. Предложите критерий для проверки гипотезы $H_0\colon \sigma^2_1 = \sigma^2_2$.
\end{problem}

\begin{solution}
Гипотезу можно переформулировать так:
\[
H_0\colon \delta = \sigma^2_1 - \sigma^2_2 = 0.
\]
Таким образом, можно протестировать гипотезу о том, что параметр $\delta$ равен нулю. За оценку сего параметра логично взять $\widehat{\delta} = s^2(X) - s^2(Y)$, то есть разность выборочных дисперсий выборок $X$ и $Y$ (её асимптотическая нормальность следует из из теоремы о наследовании асимптотической нормальности). Осталось найти дисперсию данной оценки. Это сделать довольной просто, если вспомнить, что $\frac{ns^2}{\sigma^2} \sim \chi^2_{n-1}$ для выборки размера $n$. Таким образом, после гуглинга дисперсии распределения хи-квадрат получаем, что
\[
\va[] \widehat{\delta} = \va[] s^2(X) + \va[] s^2(Y) = \frac{\sigma_1^4}{n^2}\cdot 2(n-1) + \frac{\sigma_2^4}{m^2}\cdot 2(m-1).
\]
Сами параметры $\sigma_1^2$ и $\sigma_2^2$ мы не знаем, поэтому логично заменить их на их состоятельные оценки (для приличия возьмём их несмещённые оценки):
\[
\widehat{\va[] \widehat{\delta}} = 2\frac{\left(s^2(X)\right)^2}{n-1} + 2\frac{\left(s^2(Y)\right)^2}{m-1}.
\]
Возьмём от всего этого дела корень, чтобы получить стандартное отклонение, и запишем итоговый критерий:
\[
R = \left\{(X, Y)\colon \frac{s^2(X) - s^2(Y)}{\sqrt{2\frac{\left(s^2(X)\right)^2}{n-1} + 2\frac{\left(s^2(Y)\right)^2}{m-1}}} > z_{1-\alpha/2}\right\}.
\]
\end{solution}

\subsection{Критерий омега-квадрат}

Сего материала на семинарах не было, но он есть в некотором виде в конспектах \cite{savelov}, так что почему бы и нет.

Вновь поставим на проверку гипотезу $H_0$ о том, что истинное распределение равно некоторому непрерывному распределению $F$. У нас уже имеется критерий для проверки такой гипотезы: критерий Колмогорова. Но теперь мы используем другую парадигму: если раньше мы смотрели на отклонение в равномерной метрике, то сейчас мы будем оценивать его через интеграл (чем-то напоминает байесовский подход в сравнении оценок).

\begin{definition}
    Пусть нам дана некоторая <<весовая>> функция $\psi(t)$ на $[0; 1]$. \textit{Статистикой омега-квадрат} называют
    \[
    \omega^2(\psi) = \int_{\R} \left(\widehat{F}_n(x) - F(x)\right)^2 \psi(F(x)) \, dF(x).
    \]
\end{definition}

Среди многообразия весовых функций мы рассмотрим 
\[
\psi_1(t) \equiv 1 \text{\;\;\;и\;\;\;} \psi_2(t) = \frac{1}{t(1-t)}.
\]

Выбор именно таких функций оправдывается их простотой и подходом в обнаружении отклонений. В \cite[гл.~12,~\S~2]{lagutin} даётся такое описание:
\begin{quote}
    Первый из них хорошо улавливает расхождение между $\widehat{F}_n$ и $F$ в области <<типичных значений>> случайной величины с функцией распределения $F$ (часто он оказывается более чувствительным, чем критерий Колмогорова). Второй же, благодаря тому, что $\psi_2(y)$ быстро возрастает при $y \to 0$ и $y \to 1$, способен заметить различие <<на хвостах>> распределения $F$, которому придается дополнительный вес.
\end{quote}

Как и в случае со статистикой критерия Колмогорова, статистика омега-квадрат, только домноженная уже на $n$, имеет некоторый предельный закон.

\begin{theorem*}
    При верности гипотезы $H_0$ статистики $n\omega^2(\psi_1)$ и $n\omega^2(\psi_2)$ слабо сходятся к некоторым фиксированным распределениям $F_1$ и $F_2$ соответственно.
\end{theorem*}

У сих распределений также имеется разложение в ряд, но оно настолько ужасное, что мне не хотелось бы пугать им читателей. Если положить $y_p$ и $z_p$ за $p$-квантили $F_1$ и $F_2$ соответственно, то получатся два асимптотических критерия с уровнем значимости $\alpha$:
\begin{gather*}
    R_1 = \left\{n\omega^2(\psi_1) > y_{1-\alpha}\right\} \textit{ --- критерий Крамера — фон Мизеса — Смирнова}\\
    R_2 = \left\{n\omega^2(\psi_2) > z_{1-\alpha}\right\} \textit{ --- критерий Андерсона — Дарлинга}
\end{gather*}
\begin{wrapfigure}{o}{300pt}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
     $\alpha$ & 0.5 & 0.15 & 0.1 & 0.05 & 0.025 & 0.01 & 0.001 \\
     \hline
     $y_{1-\alpha}$ & 0.12 & 0.28 & 0.35 & 0.46 & 0.58 & 0.74 & 1.17 \\
     \hline
     $z_{1-\alpha}$ & 0.77 & 1.62 & 1.94 & 2.49 & 3.08 & 3.88 & 5.97\\
     \hline
\end{tabular}
\end{wrapfigure}

Приведём некоторые квантили этих распределений, чтобы не ходить далеко искать:

Как на практике вычислять значение статистики омега-квадрат? Можно топорно вычислять интеграл с помощью, например, scipy.integrate.quad, но ответ будет приближённым, а нам хотелось бы точный. Благо в силу кусочно-постоянности $\widehat{F}_n$ можно упростить интеграл выше и получить следующие более приятные формулы:
\begin{gather*}
n\omega^2(\psi_1) = \frac{1}{12n} + \sum_{i=1}^n \left[F(x_{(i)}) - \frac{2i-1}{2n}\right]^2,\\
n\omega^2(\psi_2) = -n - 2 \sum_{i=1}^n \left[ \frac{2i-1}{2n} \ln{F(x_{(i)})} + \left(1 - \frac{2i-1}{2n}\right) \ln{(1 - F(x_{(i)}))} \right].
\end{gather*}

\begin{example*}
    Рассмотрим данные из задачи \ref{kolm_test}.
    
    Статистика из критерия КМС равна $\approx 0.1968$, и судя по таблице, ей соответствует уровень значимости больше $0.15$, что довольно много, и гипотезу мы не отвергаем.

    Статистика для другой весовой функции показывает результат $\approx 2.67$, и соответствующий уровень значимости уже будет меньше $0.05$, что является доводом против $H_0$.
\end{example*}

\subsection{Немного про p-value}\label{p-value}

В задачах выше периодически возникали фразы по типу <<минимальный уровень значимости, на котором мы должны отвергнуть гипотезу>> и <<фактический уровень значимости>>. Даже была оставлена ссылка на Википедию по этому поводу, но по мере составления конспекта стало понятно, что использование таких словечек вскользь скорее вредит, чем приносит пользу, и надо бы ввести некоторые разъяснения, что это, зачем оно надо и каких ложных выводов не стоит делать в этой связи. Опять же, на семинарах сего материала не было, и автор заранее приносит извинения за возможный дальнейший бред, но инструмент этот весьма полезный.

\begin{definition}
    Пусть для проверки гипотезы $H_0\colon \pth[] \in \mathcal{P}_0$ на уровне значимости $\alpha$ имеется критерий $R_{\alpha}$. Назовём \textit{p-value} или \textit{фактическим уровнем значимости} следующую величину:
    \[
    \text{p-value} = \inf \{\alpha\colon X \in R_{\alpha}\}.
    \]
\end{definition}

Поясним, что тут происходит. Для каждого $\alpha$ у нас в рукаве имеется критерий $R_{\alpha}$ с размером $\alpha$ (логично допустить, что при $\alpha < \beta$ имеется вложение $R_{\alpha} \subset R_{\beta}$). Мы смотрим, при каких $\alpha$ реализация выборки $X$ попала в критическое множество $R_{\alpha}$, то есть при каких $\alpha$ нам следовало бы отвергнуть гипотезу, и среди них берём инфимум. То есть p-value -- это \textit{минимальный уровень значимости, на котором мы должны отвергнуть гипотезу}. Таким образом,
\[
H_0\text{ отвергается} \Longleftrightarrow X \in R_{\alpha} \Longleftrightarrow \text{p-value} \le \alpha.
\]

Очень часто критерии имеют вид $R_{\alpha} = \{X\colon T(X) \ge c_{\alpha}\}$, где $\alpha$ -- размер критерия. Предположим, что наблюдаемое значение статистики $T(X)$ равно $t$. Тогда p-value можно переписать так:
\begin{gather*}
    \text{p-value} = \inf \{\alpha\colon t \ge c_{\alpha}\} = \alpha(t),
\end{gather*}
где $c_{\alpha(t)} = t$ (при уменьшении $\alpha$ граница $c_{\alpha}$ лишь увеличивается, поэтому инфимум достигается при $t = c_{\alpha}$). Вспоминая определение размера критерия, получаем, что
\[
\text{p-value} = \alpha(t) = \sup_{\pth[] \in \mathcal{P}} \pth[](T(X) \ge c_{\alpha(t)}) = \sup_{\pth[] \in \mathcal{P}} \pth[](T(X) \ge t).
\]

Если основная гипотеза простая, то супремум берётся по одному элементу -- предполагаемому распределению из $H_0$. В таком случае можно переформулировать
\begin{definition}
    p-value -- это вероятность наблюдать статистику критерия такую же или даже более экстремальную, чем она есть на самом деле, при условии верности $H_0$.
\end{definition}

Это можно проиллюстрировать следующими картинками, на которых изображена плотность статистики $T(X)$:

\begin{multicols}{2}

\begin{asy}
    import graph;
import math;
size(225,0);

real sigma2 = 0.1;
real l = -1.5;
real r = 1.5;
real alpha = 0.5;
real p_value = 0.25;

real rho(real x) {return exp(-x * x / 2 / sigma2) / sqrt(2 * pi * sigma2);}

path rho_graph = graph(rho, l, r, operator..);
path alpha_graph = graph(rho, alpha, r, operator..);
path pvalue_graph = graph(rho, p_value, r, operator..);

path fill_alpha = alpha_graph -- (r,0) -- (alpha,0) -- cycle;
path fill_pvalue = pvalue_graph -- (r,0) -- (p_value,0) -- cycle;
pen alpha_pen = red+opacity(0.5);
pen pvalue_pen = green;

fill(fill_pvalue, pvalue_pen);
fill(fill_alpha, alpha_pen);

label("$\alpha$", (0.65, 0.2), NE, red);
label("$c_{\alpha}$", (alpha, 0), S, red);
label(scale(0.75)*"$T(X)$", (p_value, 0), S, green);
label("p-value", (0.4, 0.5), NE, green);
draw(rho_graph, black);

xaxis(EndArrow, xmin=l, xmax=r);
\end{asy}
\hfill
\begin{asy}
   import graph;
import math;
size(225,0);

real sigma2 = 0.1;
real l = -1.5;
real r = 1.5;
real alpha = 0.5;
real p_value = 0.75;

real rho(real x) {return exp(-x * x / 2 / sigma2) / sqrt(2 * pi * sigma2);}

path rho_graph = graph(rho, l, r, operator..);
path alpha_graph = graph(rho, alpha, r, operator..);
path pvalue_graph = graph(rho, p_value, r, operator..);

path fill_alpha = alpha_graph -- (r,0) -- (alpha,0) -- cycle;
path fill_pvalue = pvalue_graph -- (r,0) -- (p_value,0) -- cycle;
pen alpha_pen = red+opacity(0.5);
pen pvalue_pen = green;

fill(fill_pvalue, pvalue_pen);
fill(fill_alpha, alpha_pen);

label("$\alpha$", (0.65, 0.2), NE, red);
label("$c_{\alpha}$", (alpha, 0), S, red);
label(scale(0.75)*"$T(X)$", (p_value, 0), S, green);
label("p-value", (0.9, 0.05), NE, green);
draw(rho_graph, black);

xaxis(EndArrow, xmin=l, xmax=r);
\end{asy}
\end{multicols}

На левом рисунке значение статистики оказалось достаточно маленьким, и соответствующее p-value (что есть площадь под графиком, выделено зелёным) больше, чем заявленный уровень значимости $\alpha$ (выделен красным). Значит, гипотеза не отвергается. На правом же рисунке статистика $T(X)$ приняла весьма экстремальное значение и попала в <<критическую зону>>. Отсюда делаем вывод о необходимости отвергнуть $H_0$.

Теперь следует сделать некоторые

\begin{remarks}
    1. \textbf{p-value не есть уровень значимости}, это разные вещи. Уровень значимости -- это фиксированное число, величина позволяемой ошибки I рода, которую мы фиксируем до наблюдения. p-value же -- функция от наблюдения, которое уже произошло. Существование p-value не освобождает от постановки уровня значимости в самом начале. У автора есть гипотеза, что первое определение p-value не любят использовать именно из-за подобной путаницы.\\ 
    2. \textbf{p-value не есть вероятность того, что $H_0$ верна}. Гипотеза либо верна, либо нет, это не случайное событие, и байесовским подходом мы тут не занимаемся.\\
    3. p-value можно рассматривать как степень уверенности в отклонении $H_0$. Если оно близко к нулю, то по версии $H_0$ произошло очень маловероятное событие, что и заставляет нас отклонить её. То есть чем меньше p-value, тем более мы спокойны о нашем решении в отвержении $H_0$. Например, в задаче \ref{small_p_value} фактический уровень значимости не просто меньше заявленного уровня значимости, так ещё и чрезвычайно близок к нулю, поэтому сомнений в отвержении гипотезы должно быть минимум.\\
    4. Высокое p-value не свидетельствует о верности $H_0$. Вполне возможно, что на самом деле верна какая-нибудь альтернатива $H_1$, но мощность критерия оставляет желать лучшего, поэтому мы не попадаем в $R_{\alpha}$ и при больших $\alpha$, что и означает большой p-value.
\end{remarks}

\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}



